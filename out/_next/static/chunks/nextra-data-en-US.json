{"/about":{"title":"About","data":{"":"This is the about page! This page is shown on the navbar."}},"/external/ARCHIVED":{"title":"ARCHIVED","data":{"":"[ARCHIVE] As of October 2023, these pages have been archived as we have transitioned from Minerva to Orca.You can find all docs in here which are no longer valid. They are kept for reference purposes only."}},"/external":{"title":"External Documentation","data":{"":"Your one stop shop for more information on exceptions, policies, scanning and more!"}},"/blogs":{"title":"Blogs","data":{"":""}},"/external/compliance_scanning/orca":{"title":"Orca","data":{"":"In depth information about Orca."}},"/external/compliance_scanning":{"title":"Compliance Scanning","data":{"":"In depth information on how compliance scanning works and the architecture itself."}},"/external/compliance_scanning/orca_alicloud_controls":{"title":"Orca AliCloud Control Details","data":{"":"","how-to-use-this-document#How to Use This Document":"Below you will find detailed information for each released Orca Alicloud control. Remediations for the controls are found below using manual steps (console-based) and automated steps (Terraform-based). Follow the manual steps if you do not use Terraform to manage your resources, or add the Terraform configurations to your Terraform modules. Please thoroughly test and adapt the Terraform configurations as needed.","control-severity-details#Control Severity details:":"Following are control severities:\n8.0 represents HIGH/MUST severity controls\n5.0 represents MEDIUM/SHOULD severity controls\n1.0 represents informational controls","note#Note:":"The informational controls will not be reported on. These controls will eventually be release as high or medium in coming releases."}},"/external/compliance_scanning/orca/hsdb_api":{"title":"HSDB API for Orca compliance alerts","data":{"":"This document outlines how to connect and use the HSDB API for Orca enriched\ncompliance alerts.A service account for API can be requested through a SNOW ticket as shown in the\nHow can I get a service account for Hyperscaler API? of the documentation\npageThe endpoint address is https://db.multicloud.int.sap/compliance/orca. Use the\nGET command to retrieve alert data. In order to do a full export of the alerts,\nuse the information in the response metadata:\n\"count\": 1,\n\"next\": \"https://db.multicloud.int.sap/compliance/orca/?page=2\",\n\"previous\": null\nThe count gives the number of alerts returned by the API query, while the next\nand previous give next and previous page. A script can then be created to loop\nover the next value until it's null.If the CSV format is needed, this can be converted from the JSON API\nresponse. All languages support this. An example for Python can be checked at\nhttps://blog.enterprisedna.co/python-convert-json-to-csv/","orca-alert-api-v1-format-sample#Orca alert API v1 format sample":"Below is a sample output of the API:\n{\n\"count\": 1,\n\"next\": null,\n\"previous\": null,\n\"results\": [\n{\n\"alert_id\": \"orca-111111\",\n\"type_string\": \"2_02_azure_subscription_role_overly_permissive\",\n\"subject_type\": \"AzureRoleDefinition\",\n\"rule_id\": \"ur506c2992f9\",\n\"category\": \"Best practices\",\n\"recommendation\": \"\",\n\"alert_labels\": \"[]\",\n\"asset_category\": \"Users and Access\",\n\"account\": {\n\"detail\": \"https://db.multicloud.int.sap/creator/accounts/111111\",\n\"cloud_id\": \"806fda3e-480f-11ee-a857-00155da6aeed\",\n\"type\": \"AZR\",\n\"name\": \"acme\"\n},\n\"asset_name\": \"SF Contributor Custom Role (No NSG, No Delete RG)\",\n\"asset_type\": \"AzureRoleDefinition\",\n\"asset_labels\": \"None\",\n\"description\": \"2_02_azure_subscription_role_overly_permissive\",\n\"details\": \"2.2 - Azure Subscriptions with custom roles should not be overly permissive\\n\\n        SGS Wiki Link: https://wiki.wdf.sap.corp/wiki/x/Ckc3c\",\n\"orca_score\": \"5.00\"\n\"created_at\": \"2023-06-21T11:55:37Z\",\n\"last_seen\": \"2023-06-21T11:55:37Z\",\n\"last_updated\": \"2023-06-21T11:55:37Z\",\n\"status\": \"open\",\n\"severity\": \"imminent compromise\",\n\"risk_level\": \"medium\"\n}\n]\n}\n**NOTE:** This is v1 format of the API resultsFor filters and complex API queries, please visit the documentation\npage","minerva-to-orca-api-data-mapping#Minerva to Orca API data mapping":"Below is a mapping of the Minerva to Orca data models:\nMinerva\tOrca\tNotes\t\talert_id\tIdentify the alert in orca, no equivalent in Minerva\tcontrol_id\ttype_string\tcontrol identifier\t\tsubject_type\trefers to the subject of the alert, which in almost every case is the Asset and therefore should match the asset_type\t\trule_id  e.g. ur506c2992f9\tOrca rule identifier. However, Minerva control_id is not directly mapped into the orca one. Orca rule_id is an internal string\t\tcategory  e.g. Best practices\tOrca alert category\t\trecommendation\tOrca recommendation for alert resolution\t\talert_labels\t\t\tasset_category  e.g. Users and Access\tOrca asset category\taccount\taccount\tHyperscaler account details\t\tasset_name\tName of the asset\tresource_class\tasset_type\tResource type identifier\t\tasset_labels\t\t\tdescription\tIn most cases same as type_string\tcontrol_impact\torca_score\tOrca score maps to a factor of 10 to the minerva one\tcontrol_title\tdetails\tLong control description\tstart_time\t\tMinerva control scan star time\tresource_param\t\tParameters specific to the resource\t\tcreated_at  e.g. 2023-06-21T11:55:37Z\tAlert creation time\t\tlast_seen  e.g. 2023-06-21T11:55:37Z\tLast time of when alert was seen\t\tlast_updated  e.g. 2023-06-21T11:55:37Z\tLast time of when alert was updated\tstatus  e.g. passed, failed, skipped\tstatus e.g. Open, Closed, Dismissed\tThis field is used differently in Minerva and Orca\t\tseverity\tOrca legacy score\t\trisk_level  e.g. High, Medium, Low, Informational\tOrca score indicating how critical the alert is","orca-alert-api-v2-format-sample#Orca alert API v2 format sample":"In v2, the following additional fields have been added:\nsec_attrs with security_officer and environment\ncost_object with CC and l4 to l9 levels\ntech_resp_user\n\n\n{\n\"count\": 1,\n\"next\": null,\n\"previous\": null,\n\"results\": [\n{\n\"alert_id\": \"orca-1064060636\",\n\"type_string\": \"2_02_azure_subscription_role_overly_permissive\",\n\"subject_type\": \"AzureRoleDefinition\",\n\"rule_id\": \"ure85283814b\",\n\"category\": \"Best practices\",\n\"recommendation\": \"\",\n\"alert_labels\": \"[]\",\n\"asset_category\": \"Users and Access\",\n\"account\": {\n\"detail\": \"http://127.0.0.1:8000/creator/accounts/0160c384-e418-4a0c-8c20-4656a2c5dd89/\",\n\"sec_attrs\": {\n\"security_officer\": {\n\"inum\": \"C0000987\",\n\"first_name\": \"Name 0\",\n\"last_name\": \"Surname 0\",\n\"mail\": \"name.surname0@sap.com\"\n},\n\"environment\": \"PROD\"\n},\n\"cost_object\": {\n\"l4_name\": \"l4-name\",\n\"l5_name\": \"l5-name\",\n\"l6_name\": \"l6-name\",\n\"l7_name\": \"l7-name\",\n\"l8_name\": \"l8-name\",\n\"l9_name\": \"l9-name\",\n\"type\": \"CC\",\n\"code\": \"10100000\",\n\"name\": \"co-billing-1\",\n\"verified\": true\n},\n\"cloud_id\": \"sap2-account3-azure4-id1\",\n\"type\": \"AZR\",\n\"name\": \"sap-account-azure\",\n\"tech_resp_user\": {\n\"inum\": \"C0000987\",\n\"first_name\": \"Name 0\",\n\"last_name\": \"Surname 0\",\n\"mail\": \"name.surname0@sap.com\"\n}\n},\n\"asset_name\": \"asset_name\",\n\"asset_type\": \"AzureRoleDefinition\",\n\"asset_labels\": \"None\",\n\"description\": \"2_02_azure_subscription_role_overly_permissive\",\n\"details\": \"2.2 - Azure Subscriptions with custom roles should not be overly permissive\\n\\n        SGS Wiki Link: https://wiki.wdf.sap.corp/wiki/x/Ckc3c\",\n\"orca_score\": \"5.00\",\n\"created_at\": \"2023-07-14T09:12:42Z\",\n\"last_seen\": \"2023-07-14T09:12:42Z\",\n\"last_updated\": \"2023-07-14T09:12:42Z\",\n\"status\": \"open\",\n\"severity\": \"hazardous\",\n\"risk_level\": \"medium\"\n}\n]\n}","orca-alert-api-v3-format-sample#Orca alert API v3 format sample":"In v3, the following additional fields have been added:\nasset_vendor_id: (optional) asset identifier\nasset_tags_info_list: (optional) asset tags\nasset_regions: (optional) asset regions\n\nThese fields are alert attributes from the point of alert notifications from Orca. These are not real-time from hyperscalers and hence may not always reflect the latest values in hyperscalers.\n{\n\"count\": 1,\n\"next\": null,\n\"previous\": null,\n\"results\": [\n{\n\"alert_id\": \"orca-3333333333\",\n\"type_string\": \"5_04_azure_storage_account_object_versioning\",\n\"subject_type\": \"AzureStorageAccount\",\n\"rule_id\": \"ur0fd4638f9b\",\n\"category\": \"Best practices\",\n\"recommendation\": \"\",\n\"alert_labels\": \"[]\",\n\"asset_category\": \"Storage\",\n\"account\": {\n\"detail\": \"http://127.0.0.1:8000/creator/accounts/0160c384-e418-4a0c-8c20-4656a2c12345/\",\n\"sec_attrs\": {\n\"security_officer\": {\n\"inum\": \"C0000987\",\n\"first_name\": \"Name 0\",\n\"last_name\": \"Surname 0\",\n\"mail\": \"name.surname0@sap.com\"\n},\n\"environment\": \"PROD\"\n},\n\"cost_object\": {\n\"l4_name\": \"l4-name\",\n\"l5_name\": \"l5-name\",\n\"l6_name\": \"l6-name\",\n\"l7_name\": \"l7-name\",\n\"l8_name\": \"l8-name\",\n\"l9_name\": \"l9-name\",\n\"type\": \"CC\",\n\"code\": \"10100000\",\n\"name\": \"co-billing-1\",\n\"verified\": true\n},\n\"cloud_id\": \"sap2-account3-azure4-id1\",\n\"type\": \"AZR\",\n\"name\": \"sap-account-azure\",\n\"tech_resp_user\": {\n\"inum\": \"C0000987\",\n\"first_name\": \"Name 0\",\n\"last_name\": \"Surname 0\",\n\"mail\": \"name.surname0@sap.com\"\n}\n},\n\"asset_name\": \"asset_name\",\n\"asset_type\": \"AzureStorageAccount\",\n\"asset_labels\": \"None\",\n\"description\": \"5_04_azure_storage_account_object_versioning\",\n\"details\": \"5.4 - Azure Storage accounts object versioning should be enabled\\n\\n        SGS Wiki Link: https://wiki.wdf.sap.corp/wiki/x/Ckc3c\",\n\"orca_score\": \"5.00\",\n\"created_at\": \"2023-07-14T09:12:42Z\",\n\"last_seen\": \"2023-07-14T09:12:42Z\",\n\"last_updated\": \"2023-07-14T09:12:42Z\",\n\"status\": \"open\",\n\"severity\": \"hazardous\",\n\"risk_level\": \"medium\",\n\"asset_vendor_id\": \"asset1-azure2-id4\",\n\"asset_tags_info_list\": \"['tag1Name|example-tag1', 'tag2Name|example-tag2']\",\n\"asset_regions\": \"['centralus', 'westeurope']\"\n}\n]\n}"}},"/external/compliance_scanning/orca_aws_controls":{"title":"Orca AWS Control Details","data":{"":"","how-to-use-this-document#How to Use This Document":"Below you will find detailed information for each released Orca AWS control. Remediations for the controls are found below using manual steps (console-based) and automated steps (Terraform-based). Follow the manual steps if you do not use Terraform to manage your resources, or add the Terraform configurations to your Terraform modules. Please thoroughly test and adapt the Terraform configurations as needed.","control-severity-details#Control Severity details:":"Following are control severities:\n8.0 represents HIGH/MUST severity controls\n5.0 represents MEDIUM/SHOULD severity controls\n1.0 represents informational controls","note#Note:":"The informational controls will not be reported on. These controls will eventually be release as high or medium in coming releases.Import your JavaScript component:"}},"/external/compliance_scanning/orca_gcp_controls":{"title":"Orca GCP Control Details","data":{"":"","how-to-use-this-document#How to Use This Document":"Below you will find detailed information for each released Orca GCP control. Remediations for the controls are found below using manual steps (console-based) and automated steps (Terraform-based). Follow the manual steps if you do not use Terraform to manage your resources, or add the Terraform configurations to your Terraform modules. Please thoroughly test and adapt the Terraform configurations as needed.","control-severity-details#Control Severity details:":"Following are control severities:\n8.0 represents HIGH/MUST severity controls\n5.0 represents MEDIUM/SHOULD severity controls\n1.0 represents informational controls","note#Note:":"The informational controls will not be reported on. These controls will eventually be release as high or medium in coming releases."}},"/external/compliance_scanning/orca_vs_minerva_controls":{"title":"Orca v/s Minerva controls","data":{"":"","who-target-audience#WHO: (Target Audience)":"The document is intended for all the LoB's using Orca.","what-document-overview#WHAT: (Document Overview)":"This document provides an overview of the key differences between Minerva and Orca scan results. The scope covers controls that are currently implmented in Orca","why-reasoning-for-this-document#WHY: (Reasoning for this document)":"There are many significant differences between how Orca and Minerva scan. The goal of this document is to help all Orca stakeholders understand those differences to take full advantage of the deeper scan results. This will ensure SAP and its customers are protected and enable a greater degree of visibility for LoBs.","how-process-followed#HOW: (Process followed)":"The tool custodians (GCS SRRC yperscalers Security Engineering & Operations team) followed a rigorous methodology to develop full control parity between Minerva and Orca. All new control development was done in Orca for 6 months prior to the transition. Key controls were tested and validated to identify any large discrepancies in data. When gaps were identified, they were investigated, and either the query was updated or the new findings were documented\nThis validation includes comparing the total number of alerts and control implementation\nFollowing steps were performed in order to do the validation:\nGet alerts report from Orca\nGet alerts report from Minerva\nGet list of common accounts and controls between Orca and Minerva. Having list of common accounts in necessary for doing the numbers comparison\nFind delta in the number of alerts for each control. Sort that from largest to smallest. At the same time, it is necessary to have context to the numbers in terms of percentage. The percentage difference in number of alerts is calculated using this formula: abs(orca - minerva)/min(orca,minerva)*100\nThe criteria for controls validation was if it is present in top 10 sorting based on percentage or if the absolute delta in the number of alerts was greater than or equal to 1,000\nIf a certain control is present in top 10 based on percentage however, if the numbers of alerts in the first itself are very low, it won't be impactful when considered over a big scale. In such cases, that control is skipped for validation.","justifcation-for-differences-explanations-for-why-and-where-the-differences-are#Justifcation for differences: (Explanations for why and where the differences are)":"Following are some high level reasoning for top controls where the difference in number of alerts is expected:\nThere are some differences between the implementation in Orca and Minerva.\nThe base model for Orca and Minerva is different hence, it is not possible to achieve same results. For instance, for AWS cloudfront distribution control (8_05_cloudfront_access_log_enabled) has different presentation of results. minerva subject: distribution ID. Orca subject: domain. This was result in far less number of alerts in Orca than Minerva\n\n\nMinerva control producing false negatives due to incorrect implementation\nIn this case, the Orca query was verified for the results and was unchanged since it gave more accurate results\n\n\nFollowing are some top controls where the differences in the number of alerts are expected:\n5.4 - (GCP) Storage log buckets should have object versioning enabled (GCP) -> The minerva control was checking for just the logging buckets but due to updated policy decision from SGS the Orca control is checking for all the buckets.\n6.1.5 - (All hyperscalers) must not allow traffic on Telnet port (23) or RSH port (514) -> The minerva control was checking just for port 23 and restricting traffic just from internet and ingress. Due to updates SGS policy decision, orca control is also checking for RSH port i.e. 514 and retricting traffic from any direction irrepective whether that is from internet or not.\n\n\nAddition of new controls in Orca:\n\n\nCloud Provider\tControl title\tAWS\t3.6 - AWS secrets that have not been accessed for 90 days must be removed from secret manager\tAWS\t3.6 - It must be ensured that in AWS only dedicated users/groups/roles have full kms:* permissions\tAWS\t4.5 - AWS EC2 instances launched using auto scaling group launch configurations must not have public IP addresses assigned by default\tAWS\t4.7 - AWS EC2 instances must have IMDSv2 enabled\tAWS\t5.1 - AWS Glacier lock policy principal permission must not be overly permissive\tAWS\t5.2 - AWS Systems Manager (SSM) should have encryption at rest enabled\tAWS\t5.4 - AWS S3 buckets object versioning should be enabled\tAWS\t6.2 - AWS Subnets must have \"auto-assign public IPv4 address\" disabled\tAWS\t6.5 - AWS Transit Gateways \"AutoAcceptSharedAttachments\" must be turned off, to ensure that only authorized VPC attachment requests are accepted (not automatically accept VPC attachment requests).\tAWS\t8.1 - AWS Elasticache Redis cluster software must be still supported with security patches (no active use of End of Life versions)\tAWS\t8.1 - AWS Elasticache Memcached cluster software must be still supported with security patches (no active use of End of Life versions)\tAWS\t8.2 - AWS underlying cluster software (e.g. elasticsearch) should always be updated to the latest stable release\tAWS\t8.3 - AWS RDS instances or clusters should not use the default VPC\tAWS\t8.3 - AWS DMS replication instances must not be publicly accessible\tAzure\t4.1 - Azure VMs must use encrypted protocols\tAzure\t4.2 - Azure IP Forwarding should not be enabled\tAzure\t4.5 - Azure Virtual Machine Scale Sets must not have public IP addresses assigned by default\tAzure\t5.1 - Azure VM Images must use SAP owned machine images and they must not be made public\tAzure\t5.1 - Azure snapshots must not be publicly accessible\tAzure\t5.4 - Azure Storage accounts object versioning should be enabled\tGCP\t4.2 - GCP IP Forwarding should not be enabled\tGCP\tGCP Machine Images MUST NOT be made publicly available\tGCP\t7.9 - GCP Kubernetes Engine Clusters should have Network Policy enabled\tGCP\t7.13 - GCP “Shielded GKE” nodes should be enabled\tGCP\t8.1 - GCP Caching Software (Memcached) must be still supported with security patches (no active use of End of Life versions)\tAlicloud\t8.3 - Alibaba Cloud ApsaraDB RDS instances should not use the default VPC"}},"/external/hs_account_related/cal-hs":{"title":"CAL Account vs. Hyperscaler Account - \"What is this account, I never heard of it\"","data":{"":"One Question we get asked quite often is, \"I don't know anything about this account, how can I be the owner?\".If you use SAP CAL (Cloud Appliance Library) you most likely have any form of Hyperscaler Account (AWS, GCP, Azure). CAL is a webinterface which shall make it easier for you to deploy and run your Solutions in the cloud.There are two versions of CAL: external CAL, and internal CAL. You most likely use the internal CAL.If you request a CAL account, you will also have any hyperscaler account which CAL manages. So if you get an email with lists you as Owner of a specific account which you never heard of, but you use SAP CAL - thats most likely your Hyperscaler CAL Account.If you want to have a look whats running in your account even if CAL lists nothing active, you can request (web)console access to your account by creating a Ticket on our Servicedesk.You can access your Account:","aws#AWS:":"https:// <your 12 digit account ID from your account>.signin.aws.amazon.com/console\nLike: https://000000000000.signin.aws.amazon.com/console/","azure#Azure:":"azure.microsoft.com - Click on \"Sign in\" on the top right corner. Use your SAP-Credentials (email/pasword) to log in. - On your first login you have to setup an MFA.","gcp#GCP:":"console.cloud.google.com - You will be forwarded to an SAP Authentication page once you entered your SAP email address."}},"/external/hs_account_related":{"title":"Hyperscaler Account Related","data":{"":"Find information and references here regarding cloud accounts and resources."}},"/external/release_info":{"title":"Controls Release Notes and Information","data":{"":"Your go to resource for everything release related!"}},"/external/hs_account_related/firewall1":{"title":"Fixing the overly permissive Firewall rule","data":{"":"### IF THE DEFAULT RULES ARE THERE, THE ALARM GETS TRIGGERED!The hierarchy or other rules which overrule the default rules are not fully considered. So please check your alarms carefully!Alarms about remote access ports open to the internet or whole networks exposed to the internet are some of the most triggered ones.Te remediate this alarm, you need to specify which IPs and IP Ranges are allowed to connect to these 'Admin Ports'.First of all, you need to know which IP Addresses or IP Address Ranges you need to allow. The easiest way here is to lookup which public IP you are using and allow this range.","get-your-sap-public-ip#Get your SAP Public IP":"Getting the IP Range for your SAP VPN Endpoint (Also works on converged Cloud Servers)\nConnect to SAP VPN, use the Endpoint you usually use (Walldorf, Sydney, Tel Aviv, Palo Alto, etc)If you are on Linux or on Mac, open a console and run:curl -k https://ip4only.me/api/ | cut -d \",\" -f 2If you are on Windows:Open the website \"ip4.me\" - This website will tell you which SAP Public IP Address you are using:\n\nLogin to the NIP TOOL, click on \"Networking\" in the top menu and scroll down to \"Public Office Network IP Addresses\"\n\n\n\nIn the overview, scroll to the right until you see \"ext. CIDR\" and enter the first 3 digits of your public IP:","getting-the-ip-for-your-sap-office#Getting the IP for your SAP Office":"You can use the NIP Tool Filters to get the IP address (ranges) for the different offices. Just filter for \"Country\" and \"Location\".","convert-to-cidr-notation#Convert to CIDR notation":"The shown IP Addresses are the ones you need to allow in your Firewall Rules or security Groups, but you need the official CIDR Notation to define these ranges.193.16.224.0/28 and 193.16.224.32/28This can be done by using a subnet calculatorEnter the first IP Address and then switch through the CIDR Netmask option until the CIDR Address Range matches.","fix-firewall-rule-in-gcp#Fix Firewall Rule in GCP":"Login to your GCP Console and switch to \"Networking\" --> VPC network -->Firewalls\n\nOn the next screen click the Firewall route which triggers your alarm. Klick on the actual NAME of this Rule. Maybe it's not visible, but the name is the link you need:\n\nOn the next screen, you can edit the rule:\n\nScroll down to \"Source\" and enter the new IP Range you want to allow:","fix-security-group-in-aws#Fix Security Group in AWS":"Login to your AWS cloud Console and switch to \"Security Groups\" in the EC2 Function:\n\n\n\n\nOn the next screen, search for the rule which triggers your alarm:\n\nClick on the Security Group you want to edit.\nOn the next screen, click on \"Edit inbound rule\"\n\nAt \"Source\" enter the IP Address Range you want to allow access:\n\nSimply press Enter if you need to add another range. Once you added all ranges, remove the \"Everyone from everywhere has access\" range:\n\nHit the Save button."}},"/faq":{"title":"FAQ","data":{"":"Contents have been organized by grouping questions by question category.Contents\nSecurity Solution\nWhat is Orca used for?\nWhat is the Orca deployment scope?\n\n\nSolution Security\nHow is data privacy accomplished?\nHow is data sovereignty (EU-only access) handled?\n\n\nAccess to Orca\nHow does a team onboard to Orca?\nHow long does it take to receive Orca access through CAM permissions?\n\n\nExceptions\nCan certain assets be excluded from a scan?\nHow often are exceptions updated?\n\n\nScanning\nHow often are scans run?\nWhat is the duration of a scan?\nHow does scanning work? (in-place)\nWhat is not scanned by Orca?\nHow can I remove accounts no longer owned by my LoB?\nHow can an LoB reduce false positive alerts?\nWhy are my Minerva results different from Orca results?\nWhat if I have an issue with a control?\nWhy does x policy exist?\nWill there be any differences between the scan results between Minerva and Orca?\nWhere can I find data from previous scans?\nWhere can I find list of custom controls against which the hyperscaler account are getting scanned for?\nHow can I get list of all the alerts associated with the custom controls?\nHow to schedule a report(s) in Orca?\nHow to develop a report(s) showing first/last scanned date by control?\nWhat is Orca Utils Dir?\nWhy should my LoB team contribute?\nHow to contribute to Orca Utils\n\n\nSupport\nWhere can I find remediation steps?\nHow can I get support for Orca?\nWhat if my question isn't answered here?\nHow do I open ticket?\nHow do I add or remove myself from the office hours DL?\n\n\nCost\nWhat costs will the LoB be accountable for?\n\n\nAPI\nHow can I pull my data via an API?\n\n\nReporting\nHow can I access dashboards and downloadable reports with regards to my compliance?\nCan you provide some insights into how access is managed in Hyperdash?\nI found a bug in Hyperdash data. Where can I open an issue to report it?\nWhy do I see additional controls in Orca than in Hyperdash?\n\n\nIntegration & Services\nHow can I integrate Orca alerts in Splunk?\nHow can I use Orca ShiftLeft?\n\n\nAttack Surface Reduction (ASR)","security-solution#Security Solution":"The following questions can be answered in Section 1.1 of our team overview page:","what-is-orca-used-for#What is Orca used for?":"From 1.1 Orca Deployment\nThe immediate use case that drove the initial deployment was to support resource and software Asset Management, specifically on virtual machines and containers. However, the solution also provides vulnerability scans for both. Orca also provides insight into misconfigurations in the landscape that could impact risk severity ratings including Kubernetes clusters, indicators of compromise, known malware, as well as use cases around IP address management and domain name registration.","what-is-the-orca-deployment-scope#What is the Orca deployment scope?":"From 1.1 Orca Deployment\nThe scope for Orca is all public cloud accounts in SAP, similar to the Minerva compliance scans. It does not provide coverage for Converged Cloud.\n\nSAP cloud accounts were onboarded at the org level with scans disabled. Scans were triggered according to deployment rings in the same fashion that preventative controls were rolled out.","solution-security#Solution Security":"The following questions can be answered in Section 1.2 of our team overview page:","how-is-data-privacy-accomplished#How is data privacy accomplished?":"From 1.2 Solution Security\nThe SAP implementation of Orca has been installed in cloud accounts owned by SAP. Since the deployment is entirely within SAP cloud accounts, data never leaves SAP. The deployment, maintenance and management of the accounts is done by Orca. In this way, SAP consumes Orca as an SaaS solution while having all the data and components in SAP EMEA.\n\nOrca's account access read-only. There is an added capability to create, read, and delete its own Orca tagged snapshots. Orca does not have permissions to make any changes to cloud accounts/landscapes. The solution does not have access to encrypted database files. If the scans detect PII data, it flags it for attention, but masks it in the results. Once onboarded, Orca reads account meta data and creates snapshots in the region of the originating instance of endpoints that are then scanned by Orca’s SaaS platform. Once the scan is completed the snapshots are deleted.","how-is-data-sovereignty-eu-only-access-handled#How is data sovereignty (EU-only access) handled?":"From 1.2 Solution Security\nOrca reads account meta data and creates snapshots in the region of the originating instance of endpoints that are then scanned by Orca’s SaaS platform. Once the scan is completed the snapshots are deleted.","access-to-orca#Access to Orca":"The following questions can be answered in Section 6 of our team overview page:","how-does-a-team-onboard-to-orca#How does a team onboard to Orca?":"From 6. Access to Orca\nFor initial access to Orca, you will need to request the following automatically approved CAM profile: Multicloud_Orca_User. This profile does not provide permissions in Orca but enables you to sign into Orca via SSO. You will have to periodically request this profile to have continued access to Orca. Beginning on December 21st, 2022, once a user has signed into Orca, their permissions will be automatically provisioned and updated based on the Hyperscaler Account roles and Group roles. Prior to this point, all users with only the Multicloud_Orca_User CAM profile will have blank permissions and should not be able to view any account information in Orca.","how-long-does-it-take-to-receive-orca-access-through-cam-permissions#How long does it take to receive Orca access through CAM permissions?":"It can take 3-4 hours after auto-approved CAM permission.","exceptions#Exceptions":"The following questions can be answered in Section 10 of our team overview page:","can-certain-assets-be-excluded-from-a-scan#Can certain assets be excluded from a scan?":"From 10. Exceptions\nExceptions are applied at the account level and all assets within the account are excluded for the policy in scope.","how-often-are-exceptions-updated#How often are exceptions updated?":"From 3.2 Orca Compliance Scanning\nScans are performed on a scheduled basis (the default is every twelve hours and reconfigured to run everyday) and also based on detected changes in cloud logs.","scanning#Scanning":"The following questions are largely answered in Section 3 of our team overview page:","how-often-are-scans-run#How often are scans run?":"From 3.1 Hyperscaler Security Compliance Scans and 3.2 Orca Compliance Scanning\n\nOrca scans are daily\nMinerva scans are weekly (replaced by Orca scans)","what-is-the-duration-of-a-scan#What is the duration of a scan?":"From 3.2 Orca Compliance Scanning\nScan duration is largely measured by the duration it takes for Orca to create the snapshot of the target account (approximately 15 minutes).","how-does-scanning-work-in-place#How does scanning work? (in-place)":"From 1.2 Solution Security\nOrcas account access read-only. There is an added capability to create, read, and delete its own Orca tagged snapshots. Orca does not have permissions to make any changes to cloud accounts/landscapes. The solution does not have access to encrypted database files. If the scans detect PII data, it flags it for attention, but masks it in the results. Once onboarded, Orca reads account meta data and creates snapshots in the region of the originating instance of endpoints that are then scanned by Orca’s SaaS platform. Once the scan is completed the snapshots are deleted.","what-is-not-scanned-by-orca#What is not scanned by Orca?":"From 1.2 Solution Security\nThe solution does not have access to encrypted database files. If the scans detect PII data, it flags it for attention, but masks it in the results.","how-can-i-remove-accounts-no-longer-owned-by-my-lob#How can I remove accounts no longer owned by my LoB?":"From 6.4 Account Maintenance\n\nTo have your Hyperscaler Account modified or to have it deleted you can open a Service Now ticket under GCS - Hyperscaler - Manage/Delete Account.\nDeletions can also be processed in HSDB.\nOn the Orca side, automation looks at any account listed as \"DELETED\" in HSDB and will offboard the account 90 days after the initial deletion date. It's worth noting that any associated alerts with the cloud account will have been dismissed as soon as the deletion was processed in HSDB.","how-can-an-lob-reduce-false-positive-alerts#How can an LoB reduce false positive alerts?":"From 7. Control Details\nPlease review remediation steps for each respective control. If you believe you have remediated the alert in scope as mandated by SGS's hardening guidelines a ticket can be opened here 5.3 Hyperscalers Security Engineering & Operations (HSEO) Support","why-are-my-minerva-results-different-from-orca-results#Why are my Minerva results different from Orca results?":"More information can be found in 7. Control Details\nHowever, the short answer is that the controls are written differently due to the change on the back end. There are also some additional controls being scanned with Orca.","what-if-i-have-an-issue-with-a-control#What if I have an issue with a control?":"From 5.3 Hyperscalers Security Engineering & Operations (HSEO) Support\nTickets can be opened for false positives and general control explanation. However if you disagree with the enforcement of a control, SGS would be the ones to contact.","why-does-x-policy-exist#Why does x policy exist?":"Answered in 7.1-7.4\nThis is a question for SGS. We merely enforce the policies they mandated by their hyperscaler hardening guidelines.","will-there-be-any-differences-between-the-scan-results-between-minerva-and-orca#Will there be any differences between the scan results between Minerva and Orca?":"Yes. We have created a document to outline the key differences in the scan results along with a detailed explanation. Minevra v/s Orca controls differences","where-can-i-find-data-from-previous-scans#Where can I find data from previous scans?":"From 3.3 Accessing Previous Scans\nPrevious Minerva scan data can be found in the MC Secure Architecture & Engineering sharepoint.","where-can-i-find-list-of-custom-controls-against-which-the-hyperscaler-account-are-getting-scanned-for#Where can I find list of custom controls against which the hyperscaler account are getting scanned for?":"From 4.1 How to get alerts\nLogin into Orca portal -> Navigate to the menu -> Settings -> Modules -> Alerts Settings -> All the controls under \"Custom Alerts\" tab are SAP specific controls developed. SAP Custom controls","how-can-i-get-list-of-all-the-alerts-associated-with-the-custom-controls#How can I get list of all the alerts associated with the custom controls?":"From 4.1 To get list of all alerts associated with the custom controls","how-to-schedule-a-reports-in-orca#How to schedule a report(s) in Orca?":"In Orca, you have the flexibility to easily create and manage scheduled reports, including sending them to your desired recipients with a specified schedule.\n\nTo set up a scheduled report, follow these steps:\nGo to the \"Alerts\" tab in the Orca.\nIn the top-left corner, click on \"Current View\".\nChoose \"SGS-Custom-Alerts\" from the list of available shared views.\nOn the right, click \"Export\".\nHere, you can either generate a report in your preferred format (CSV or JSON) by selecting \"download\" or you can \"schedule\" an automatic delivery report which you can also share to any specified distribution channels (Slack channel, AWS S3 bucket, Azure Blob, and GCP bucket).\nClick Done.","how-to-develop-a-reports-showing-firstlast-scanned-date-by-control#How to develop a report(s) showing first/last scanned date by control?":"Option 1: To get alert using Sonar Query\n\n\n\nGo to Discovery and click Back to Sonar.\nRun the following Sonar queries:\nFor high alerts:\nAlert with Labels containing \"sap\" and CreatedAt <= 3 months ago and RiskLevel = \"high\"\nAlert with Labels containing \"sap_{hyperscaler}\" and CreatedAt <= 3 months ago and RiskLevel = \"high\"\nFor medium alerts:\nAlert with Labels containing \"sap\" and CreatedAt <= 6 months ago and RiskLevel = \"medium\"\nAlert with Labels containing \"sap_{hyperscaler}\" and CreatedAt <= 6 months ago and RiskLevel = \"medium\"\nIf you are getting timeout error or zero result, most likely the controls were not available in prod until around July 2023.\nLast seen date for all alerts will be presented in the downloaded reports.\n\n\n\n\nClick search.\nTo generate a report, go to Export tab and Download in your preferred format or you can schedule report by using Generate Report option.\n\n\n\nOption 2: From HSDB Orca API endpoint\n\n\n\nPlease refer to API docs below for reference:\nHow to use the HSDB API for Orca compliance alerts\nHSDB Orca API Document\nOrca Alerts by matrix","what-is-orca-utils-dir#What is Orca Utils Dir?":"The Orca Utils Repository is a central hub where our team can share useful scripts, solutions, and resources that can benefit everyone. It serves as a knowledge-sharing platform to foster collaboration and efficiency within the SAP.","why-should-my-lob-team-contribute#Why should my LoB team contribute?":"Knowledge Sharing: Share expertise, improving productivity.\nReusability: Save time by using others' solutions.\nTeamwork: Foster a culture of collaboration.","how-to-contribute-to-orca-utils#How to contribute to Orca Utils?":"Clone the repository, create a new branch, and add your content/commit.\nSubmit a pull request with a clear description & readme file.\nReviews will be managed by HSEO team members and finally, you can merge your PR.","support#Support":"The following questions are largely answered in Section 5.3 of our team overview page:","where-can-i-find-remediation-steps#Where can I find remediation steps?":"From 7. Control Details\n\n7.1 AWS Compliance Scanning Control Details\n7.2 Azure Compliance Scanning Control Details\n7.3 GCP Compliance Scanning Control Details\n7.4 AliCloud Compliance Scanning Control Details","how-can-i-get-support-for-orca#How can I get support for Orca?":"From 5.3 Hyperscalers Security Engineering & Operations (HSEO) Support\nOrca ticket: GCS - Hyperscaler - Orca","what-if-my-question-isnt-answered-here#What if my question isn't answered here?":"From 5.3 Hyperscalers Security Engineering & Operations (HSEO) Support\nHyperscaler Security ticket (formerly GCS-MC-Security): GCS - SRRC - Hyperscaler Security","how-do-i-open-ticket#How do I open ticket?":"From 5.3 Hyperscalers Security Engineering & Operations (HSEO) Support\nHyperscaler Security ticket: GCS - SRRC - Hyperscaler Security","how-do-i-add-or-remove-myself-from-the-office-hours-dl#How do I add or remove myself from the office hours DL?":"From 9.2 Cloud Security Office Hours\nThe Cloud Security Office Hours DL has been made public and the meeting will be refreshed periodically to update the attendee list.\n\nThe link to the DL can be found here: Cloud Security Office Hours Distro\n\nFor your convenience, here's a quick join directly: Join DL Cloud Security Office Hours Distro (External)","cost#Cost":"","what-costs-will-the-lob-be-accountable-for#What costs will the LoB be accountable for?":"From 1.3 Solution Costs\nThere are minimal costs that the LoBs are accountable for. While normally an SaaS solution, Orca for SAP is deployed in SAP-owned dedicated cloud accounts operated by the Hyperscalers Security Engineering & Operations (HSEO). Orca will create snapshots of running compute instances and transfer that to a scanner running in the same cloud region and same provider in the HSEO cloud account. All of the scanning occurs there, with the cloud run costs incurred by the HSEO team.\n\nLicense and operating costs for 2022 are already covered through the SCD2 budget. For 2023, there will be discussions about cross-charging, but the primary targets for those are stakeholder organizations for the resulting data, such as CCIR (SISM), SAM (ALM) and SGS, not the LoBs.","api#API":"","how-can-i-pull-my-data-via-an-api#How can I pull my data via an API?":"From 4.2 API Documentation\nInformation on how to connect and use the HSDB API for Orca can be found here.","reporting#Reporting":"","how-can-i-access-dashboards-and-downloadable-reports-with-regards-to-my-compliance#How can I access dashboards and downloadable reports with regards to my compliance?":"From 9.3 Hyperdash\n\nProject Hyperdash is a single-pane solution that streamlines the management of accounts across all Hyperscalers by consolidating data from multiple sources.\n\n\n\n\nOverview URL: https://pages.github.tools.sap/SAE/hyperdash/\n\n\nOffice Hours URL: https://hyperdash.multicloud.int.sap/office-hours\n\n\nCompliance URL: https://hyperdash.multicloud.int.sap/compliance","can-you-provide-some-insights-into-how-access-is-managed-in-hyperdash#Can you provide some insights into how access is managed in Hyperdash?":"From 9.3.3 Hyperdash Access\nAccess controls in Hyperdash are managed through the HSE Database, which automatically updates permissions based on your roles within accounts and groups on HSDB.","i-found-a-bug-in-hyperdash-data-where-can-i-open-an-issue-to-report-it#I found a bug in Hyperdash data. Where can I open an issue to report it?":"From 9.3.4 Hyperdash Bug Reporting\n\nTo report a bug in Hyperdash, you have two options:\nGitHub Issue: You can open an issue on the Hyperdash GitHub\nrepository. You can do this by visiting the Hyperdash GitHub repository\nhere and creating a\nnew issue with details about the bug you encountered. This is a formal way\nto report issues and allows for structured tracking and resolution.\nTeams Channel: Additionally, you can also post a message in the\ndedicated Teams channel for Hyperdash. You can access this channel\nhere. This can be a more informal way to report issues or discuss them\nwith the Hyperdash team and the community.","why-do-i-see-additional-controls-in-orca-than-in-hyperdash#Why do I see additional controls in Orca than in Hyperdash?":"From 9.3.5 Hyperdash Data Downloads and Backend\nOne of the Hyperdash services specifically reaches out directly to the Orca API endpoint for ‘Custom Rules’ which are the custom SGS policies developed in Orca. However, the Orca default view, currently includes alerts for the controls that are not part of the SGS custom policies but are orca in-built. These additional ones are not being reported on. For more information on how to filter the Orca alerts for the SGS custom controls only, please refer to 4.1 How to get alerts.","integration--services#Integration & Services":"","how-can-i-integrate-orca-alerts-in-splunk#How can I integrate Orca alerts in Splunk?":"Familiarise with the Splunk integration to make sure you know what you need.\n\n\n\n\nOnce you have a clear idea, create a ticket to request Splunk integration.\nAdditional steps will be provided by the Engineer dealing with your ticket.","how-can-i-use-orca-shiftleft#How can I use Orca ShiftLeft?":"Familiarise with the ShiftLeft feature to make sure you know what you need.\n\n\n\n\nOnce you have a clear idea, create a ticket to request ShiftLeft enablement.\nAdditional steps will be provided by the Engineer dealing with your ticket.","vulnerability-management-documentation#Vulnerability Management Documentation":"This link contains all the documentation related to Vulnerability Management."}},"/external/release_info/orca_controls_release_process":{"title":"Orca Controls Release Management","data":{"":"","table-of-contents#Table of Contents":"Introduction\n1.1 WHAT?\n1.2 WHY?\n\n\nOrca Release Process\n2.1 Release Cadence\n2.2 Orca controls overview\n2.3 Release components\n2.4 Release Process\n\n\nImportant links","introduction#Introduction:":"","11-what#1.1 WHAT?":"SAP uses so called \"Detective Controls\" to monitor the security configuration of ALL SAP Hyperscaler Accounts (e.g. Aws, GCP, Microsoft Azure & Alibaba Cloud) towards the SAP Hyperscaler Security Reference Architecture and configuration standards. These security configuration requirements are defined by SAP Global Security and published in the Security Policy Framework / Wiki. These can be found here Alicloud, AWS, Azure, GCPThe detective controls are divided by their ratings (HIGH, MEDIUM), which reflect the SAP Global Security mandatory baseline and additional requirements for critical cloud business:\nHIGH rated control = MUST / Baseline requirement in Security Wiki\nMEDIUM rated control = SHOULD requirement in Security Wiki","12-why#1.2 WHY?":"These set of controls are being updated dynamically as per the security standards. This includes addition of new controls, updating existing ones as per specific use-cases, etc. Considering this, it is important to have a process and track all the changes being made. Following a release process will also enable smooth adoption for the LoB's to all the control changes.","orca-release-process#Orca Release Process":"","21-release-cadence#2.1 Release Cadence":"Release for Orca controls will be done every month\nReleases follow the Semantic Versioning Specification as e.g. release-1.0.x","22-orca-controls-overview#2.2 Orca controls overview":"All the policies defined by SGS are implemented in Orca\nThese policies are converted into code called controls. These controls are developed by GCS SRRC Hyperscaler Security Engineering and Operations (HSEO) team\nOrca controls are written in the form of sonar query using orca built-in query language","23-release-components#2.3 Release components":"The release will mainly consists of two parts:\nMain release -> This will include the following:\nnew controls\ndeletion of controls\nexisiting controls updates\nSHOULD i.e. Medium severity controls bug fixes\nMUST i.e. High severity controls buf fixes if they are not covered in off-cycle release\ncontrol severity changes\n\n\nOff-cycle ad-hoc release -> This will include the following:\nUpdates to existing controls that will include bug fixes or changing the control severity to informational\nIf a controls is changed to informational it is removed from the list and added to the subsequent release\n\n\n\n\nRelease changes are implemented via:\nChange requests by SGS.\nBased on these requested changes, control for that policy is modified accordingly.\n\n\nFeedback on the controls received from LOBs.","24-release-process#2.4 Release Process":"Each release process will be of one month and will include following:\nSGS provides a list of new controls and control updates which will be developed throughout the month\nThe controls implemented and ready to be released, will be initially released as informational for at least 1 month. The controls release as informational will be a continuous process\nControls that were informational over the past at least one month, will be updated as High/Medium\nThe updates to existing controls, deactivation of controls will also be part of the release process","important-links#Important Links":"With completion of release, release notes will be published here: Orca release Notes\nOrca control details can be found here: AWS, Azure, GCP, Alicloud"}},"/":{"title":"Index","data":{"":"","documentation-categories#Documentation Categories":"External Documentation: Resources for non-team members - Explore our offerings and find assistanceInternal Documentation: Resources for team members - Dive into our internal processes and tools","explore-our-services#Explore Our Services":"The GCS Security, Risk, Resiliency & Compliance Team is dedicated to providing top-notch services for securing and configuring workloads on hyperscalers across AWS, Azure, GCP, and AliCloud.","team-services#Team Services":"The GCS Security, Risk, Resiliency & Compliance Team provides services that develop, deploy, and run workloads on hyperscalers to all SAP's Lines of Business. From development and testing to production, we are there every step of the way to enhance their journey to public cloud. It is our mission to engineer, deliver products, and solutions to maximize SAP cloud infrastructure agility, security, and cost efficiency across AWS, Azure, GCP, and AliCloud.","our-team#Our Team":"We are a globally established team of talented SecDevOps Engineers. We work vigilantly and diligently to develop and automate security measures that enforce security policies for SAP’s cloud infrastructure and create and monitor reporting tools that allow us to gain insight into security compliance of the cloud landscape. Our goal: to secure and protect our cross cloud SAP platforms, while improving security posture in SAP’s public cloud landscape - ultimately - protecting customer data and assets and in turn, preserve SAP’s integrity with its’ clients worldwide._","thank-you-to-the-contributors-of-this-library#Thank you to the contributors of this library!":""}},"/internal":{"title":"Internal Documentation","data":{"":"","this-section-provides-documentation-for-the-internal-processes-of-the-hyperscalers-security-operation-team#This section provides documentation for the internal processes of the Hyperscalers Security Operation team":""}},"/internal/ARCHIVED/Dev_Workflow":{"title":"[ARCHIVE] Dev WorkFlow","data":{"":"The links below references the things related to Dev Workflow."}},"/internal/ARCHIVED":{"title":"ARCHIVED","data":{"":"[ARCHIVE] As of October 2023, these pages have been archived as we have transitioned from Minerva to Orca.You can find all docs in here which are no longer valid. They are kept for reference purposes only."}},"/internal/ARCHIVED/Dev_Workflow/application-layer-encryption":{"title":"[ARCHIVE] Application-layer encryption","data":{"":"This feature allows encrypting K8s secrets, providing an additional layer of\nsecurity. Additional details can be found in the GCP\npage.In our implementation, a keyring called k8s-keyring is created. Keys are\ncreated in the format of <cluster-name>-app-key,\ne.g. elastic-cluster-app-key.  As each keyring must be the cluster location,\nthe SG cluster has its own keyring in asia-northeast1 location.Each key is set to be rotated after 1 year. After the first deployment all k8s\nsecrets are encrypted with the primary key version. Once the key is rotated, to\nforce secrets to be re-encrypted with the latest primary key version, a touch\n(i.e. a simple modifications) of the secret should be performed. This can be\naccomplished with an update of each secret annotation, e.g.:\nkubectl get secrets --all-namespaces -o json | kubectl annotate --overwrite -f \\\n- encryption-key-rotation-time=\"TIME\"\nwhere TIME can be time of the next rotation, as e.g. 20230911-120922.Meeting reminders for each cluster in each infrastructure (dev, preprod and\nprod) for the infra admins should be set to perform the above secrets\nannotations command."}},"/internal/ARCHIVED/Dev_Workflow/artifactory-guide":{"title":"[ARCHIVE] Guide to Artifactory","data":{"":"This guide provides how to integrate with SAP artifactory.As per SGS guidelines, artifactory has to be used where possible to push and\npull external toolings, e.g. images, helm charts, etc.The security-cloud-compliance artifacts repository has been set, and currently\nholds the following helm charts:\neck-operator\nfalco\ngatekeeper\n\nThe process is, therefore, to download the public helm charts and push them to a\ndedicated artifactory repository.","pre-requisites#Pre-requisites":"In order to be able to upload or download an artifact, the user needs to be\nadded to the repository. The user can be added only if at least one login to\nartifactory website has been done. Login is SSO based.If a user needs to be added to the repo, a request in the team slack\nmc_devsecops_developer channel should be made.","add-an-artifactory-repo-to-local-helm#Add an artifactory repo to local helm":"In order to add a repo in helm, run the following:helm repo add sap-security-cloud-compliance https://int.repositories.cloud.sap/artifactory/security-cloud-compliance --username <i-number>","pull-a-public-helm-chart#Pull a public helm chart":"To download a public helm chart, run the following:helm pull elastic/eck-operatorNote that the elastic/eck-operator string is a combination of the public\nrepository elastic and the chart eck-operator.After the command is run a local file as e.g. eck-operator-2.4.0.tgz is\ndownloaded.","push-a-chart-to-the-artifactory-helm-repository#Push a chart to the artifactory helm repository":"To upload a chart to the artifactory helm repository, run the following:curl -H \"X-JFrog-Art-Api:<personal artifactory API key>\" -T eck-operator-2.4.0.tgz \"https://int.repositories.cloud.sap/artifactory/security-cloud-compliance/eck-operator-2.4.0.tgz\"where the <personal artifactory API key> can be retrieved (or generated if not\nyet available) from artifactory by editing the personal profile from the up\nright account button with the i-number.","update-local-helm-repository#Update local helm repository":"To update the local helm repository, run the following:helm repo update","test-a-helm-chart#Test a helm chart":"To test a helm chart, check if rake command is available and run the following:rake dry_run_deploy_eck_operatorIf not, use the helm command that rake wraps:helm upgrade --dry-run --install elastic-operator sap-security-cloud-compliance/eck-operator -n elastic-systemand verify that the version is correct."}},"/internal/ARCHIVED/Dev_Workflow/patch-management":{"title":"[ARCHIVE] Patch Management","data":{"":"The security patch management aims to address any identified vulnerabilities in\nthe shortest amount of time. Different elements have to be updated regularly or\nwhen a vulnerability is found.\nInfrastructure: Cloud Managed Kubernetes Clusters, deployments, Operating\nSystems, IAM\nReporting: ELK\nServices: Patching of services themselves\nServices dependencies: Dependencies and Libraries used by any of the\ndeployed services","roles#Roles":"","duty-manager#Duty Manager":"This role is assigned to a dedicated person who is responsible to review any\nsecurity open tickets or create new ones, categorize them and assign them\naccording to their priority. A rota is to be set up by the operations team for\nthis role, so that it is spread across people and it is fully resourced.A Duty manager needs to have a technical understanding of the systems to be\nreviewed and their reported vulnerabilities. He/she doesn't need to work on the\ntask, but can if has the bandwidth. On a major incident it is strongly advised\nthat the Duty manager does not try to resolve the issue, if they do, they\nshould first seek someone to cover the Duty manager role. The Duty manager may\nhave to take on the role of managing a major incident, either initially or\nfinding someone to cover that role.","patch-activities#Patch Activities":"Patches may be required due to different reasons, which require a different\nprocess.\nPatch Requirement\tSummary\tNext step\tRegular maintenance\tA regular meeting item at operations team meeting. A nominated person will have task to check for patches.\tRaise change ticket, then update the system.\tSecurity Incident\tDue to Security Incident an emergency patch might be required. Might be zero-day vulnerability or process failure.\tRaise emergency change as part of the security incident, patch management will be only a component of the Security Incident and managed there.\tInfrastructure: Vulnerability Notification\tEither by a scan, an email, alert by SAP security, cloud provider, 3rd party vendor.\tRaise Incident and if a high risk issue a Security Incident. Then raise change ticket to apply patch.\tReporting\tEither by a scan, an email, alert by SAP security, cloud provider, 3rd party vendor.\tRaise Incident and if a high risk issue a Security Incident. Then raise change ticket to apply patch. These are tools that the Compliance Security system does not require to run and the possibility of shutting down systems is possible to quick patch the issues.\tServices: New Vulnerability, library or Regression\tDeployed code (or dependencies) contains a new vulnerability or a vulnerability that has been previously fixed.\tRaise Incident and if a high risk issues a Security Incident. Raise high priority Bug with software team. Then raise change ticket to apply patch.","operations-vulnerability-review#Operations Vulnerability review":"The SAP VAS team provides a tool to subscribe to different technologies\nvulnerability updates:\nNGT tool\nDocumentation\n\nA SAP Global Cloud Compliance group has been created with subscriptions to:\nCERT.Kubernetes\nCERT.Ubuntu\nCERT.Elastics\n\nTo avoid each operations team member subscribes individually to the group above,\na shared mailbox will be created from\nhere.\nThe shared mailbox can be then configured with the proper filters and folders,\nso that notifications can be easily classified and everyone has the same view.\nAll team admins should then access or setup the shared inbox with their outlook.The NGT tool will have to be configured with the shared mailbox\naccount. However, this is currently not supported in the NGT tool at the\nmoment. Meanwhile admins should subscribe to the group as follow:From the NGT main page:\nclick Join Profile\nin the new page select SAP Global Cloud Compliance group\n\nOutlook filters should be set up for High, Medium and Low, e.g.:\n- Name: Received from CERT - High\n- From contains: listserv.sap.corp\n- Subject Contains: --High\n- Move to Folder: Cert/High","additional-sources#Additional sources":"The US Cert national advisory:\nhttps://www.kb.cert.org/vuls/\n\n\nCloud vendors:\nhttps://cloud.google.com/compute/docs/security-bulletins\nhttps://www.microsoft.com/en-us/msrc/technical-security-notifications?rtc=1\nhttps://aws.amazon.com/security/security-bulletins/\nhttps://www.alibabacloud.com/help/product/35474.htm\n\n\nSAP software:\nhttps://wiki.scn.sap.com/wiki/display/PSR/The+Official+SAP+Product+Security+Response+Space","maintenance-patchesupgrades#Maintenance Patches/Upgrades":"OS / Cloud suppliers and Software vendors will regularly release maintenance\npatches, these should be reviewed and planned. These are not usually required\nimmediately unless they are the only source of a fix. At bare minimum this\nshould be done one a year under a change process. Enabling automatic security\nupdates should be implemented.","weekly-vulnerability-check#Weekly Vulnerability check":"At the weekly meeting a patch review should be done. The Duty manager (see\nroles above) for that week is responsible for ensuring that necessary\nvulnerabilities check is performed:\nreview releases of vulnerable patches and upcoming maintenance requirements\nscan all sources for vulnerabilities\nEmails from VAS subscriptions\nEmails from GKE clusters notification or in GCP console\nSlack #mc_devsecops_cicd_notifications channel notifications from\nTrivy scans or in Trivy console\nSort patches into high, medium low and N/A in outlook (using filters as\nthe example above)\nuse outlook category colours to mark out:\nRed for Required\nGreen for N/A\nYellow for follow-up\n\n\n\n\nDependencies notifications in github\n\n\nTake a screenshot for evidence purposes and add it to the Teams patch\nevidence folder(link TBA)\nUse table above to create relevant zenhub tasks\nCheck if base images update in gcr are needed in all environments vs the artifactory one:\n\n\nArtifactory\tgcr\tPython\teu.gcr.io/sap-mcsec-inspec-prod/securityapprovedbaseimages.int.repositories.cloud.sap/pythoneu.gcr.io/sap-mcsec-inspec-preprod/securityapprovedbaseimages.int.repositories.cloud.sap/pythoneu.gcr.io/sap-mcsec-compliance-dev/securityapprovedbaseimages.int.repositories.cloud.sap/python\n\nIAM Review of accounts on Production for ECK and GCP. Review IAM on ECK and GCP Production services on a weekly basis, and create tickets for ECK/GCP UAM admins to review roles and accounts as necessary.\nOnce a month check if keys rotation is needed. Any items that require changes to be addressed are reported at next weekly meeting.","implementation#Implementation":"Using Zenhub ticketing system, there are the following levels of priority that\ndictate the time frame in which the tickets should be implemented.\nPriotity\tResolution time\tLow/lowest\tTo be done once bandwidth permits\tMedium\tAim to be done within a month\tHigh\tAim to be done within two weeks\tVery High\tAim to be done within 5 days","very-high-patches#Very High Patches":"On the event of receiving an email with a very high patch rating, members of the\ndevsecops team will be notified immediately. A rule has to be configured on the\nshared inbox that checks if the new email contains a very high topic, if it does\nthen the email has to be sent to each member of the ops team immediately. When\nthe email is received the ops team should convene as soon as possible and\ninvestigate whether or not the patch is applicable. If the patch is applicable\nthen the highest priority ticket should be created and should be addressed\nwithin 5 days.","monthly-keys-rotation#Monthly Keys rotation":"As part of the HS security controls GCP Keys need to be rotated once a month. The\nduty manager should check for any preventative controls notifications email and\ncreate a ticket to rotate the keys in preprod, prod and cicd deployments.","base-image-update#Base image update":"As currently trying to build directly from the approved artifactory image is not\nworking, in order to update the python base image the team member assigned to\nthe task should use the workaround below:\nGet base image\nwget --no-parent -r\nhttps://int.repositories.cloud.sap/artifactory/securityapprovedbaseimages/python/3.10.0-slim-bullseye/\n\n\nCreate tgz file\ntar czf 3.10.0-slim-bullseye/ 3.10.0-slim-bullseye.tgz\n\n\nImport locally\ndocker import ./3.10.0-slim-bullseye.tgz\nsecurityapprovedbaseimages.int.repositories.cloud.sap/python/3.10.0-slim-bullseye\n\n\nLogout from artifactory\ndocker logout securityapprovedbaseimages.int.repositories.cloud.sap\n\n\nBuild, push image and test. This needs to be done for all environments prod,\npreprod and dev","rollback-plan#Rollback plan":"Before a patch is to be executed, a rollback plan needs to be in defined. The\nresponsible team member in charge of working on the patch needs to create such a\nplan in advance and add in the comment of the ticket."}},"/internal/ARCHIVED/Dev_Workflow/version-management":{"title":"[ARCHIVE] Version Management","data":{"":"In a software system, all dependencies go over release cycles to provide better\nsecurity, performance, functionalities. A system should always keep up with the\ndependencies versions, at minimum for security updates. Leaving versions\nunchanged for too long may have bad effects in the long run.  Also, the\nproduction system needs to have pinned versions for reproducibility as well as\nfor easier debugging when issues arise as it would avoid confusion in case of\nnewer and untested versions were inadvertently deployed.\nAll dependencies versions need to be pinned in production\nIn development, versions should be always the latest, unless they break a\nfunctionality in which case they have to be pinned. In such a case a zenhub task\nshould be created to modify the component/artifact to use the higher version\n\nVersion files  in the main repo folder, i.e. prod-requirements.yaml and\ndev-requirements.yaml with the following format:\npython:\ntype: language\nversion: 3.8.0\npackages:\n- google-cloud-pubsub:\nversion: latest/1.7.0\nlocations:\n- services/inspec-dispatcher-alicloud/requirements.txt\n- services/inspec-elastic-data-transformer/requirements.txt\n- …\nruby:\ntype: language\nversion: ...\npackages:\n- bundle:\nversion: latest/…\nlocations:\n- profiles/sap-aws/Gemfile\n- profiles/sap-gcp/test/Gemfile\n- ...\nrake:\ntype: tool\nversion: ...\nterraform:\ntype: tool\nversion: ...\nubuntu:\ntype: OS\nversion: ...\npackages:\n- python3-pip:\nversion: latest/…\nlocations:\n- profiles/sap-aws/Dockerfile.pubsub\n- profiles/sap-alicloud/Dockerfile.pubsub\n- ...\n\nProduction version file prod-requirements.yaml holds pinned versions for all\ndependencies\nDevelopment version file dev-requirements.yaml maintains the list of all\ndependencies that don't break any functionality with no versions specified,\nand pinned versions of the known working ones where the higher version is\nknown to break\nCreate zenhub task for known breaking version\n\n\nWhen performing a new release, it should be safe to bump-up the version from\ndev-requirements.yaml to prod-requirements.yaml as it should be implicitly\ntested"}},"/internal/ARCHIVED/Minerva_docs":{"title":"[ARCHIVE] Internal Minerva Docs","data":{"":"The links below references the things related to Minerva documentations."}},"/internal/ARCHIVED/Minerva_docs/ci_cd":{"title":"[ARCHIVE] CI/CD","data":{"":"","contacts#Contacts":"Name\tRole\tContact\tTimezone\tJustin Nikles\tTopic Lead\tjustin.nikles@sap.com\tEST\tCarmelo Ragusa\tTopic Deputy\tcarmelo.ragusa@sap.com\tGMT\tJames Yan\tTopic Deputy\tjames.yan01@sap.com\tSGT\tRohit Prasad Joshi\tTopic Deputy\trohit.prasad.joshi@sap.com\tPST"}},"/internal/ARCHIVED/Minerva_docs/ci_cd/applying_pipeline_configurations":{"title":"[ARCHIVE] Applying Pipeline Configurations","data":{"":"In addition to defining a pipeline in a yaml file, Azure Pipelines requires a pipeline build definition to be configured. In the past, we've configured these build definitions in the Azure Pipelines UI. There are a few downsides to using the UI to do this:\nBuild definitions are given many default configurations by the platform for which the user is not prompted for\nCreating build definitions in the UI requires users to have elevated permissions in some cases\nBuild definitions that are created and changed in the UI are not traceable\nIt's cumbersome work when you have to create multiple build definitions for a single piece of work.\nConfiguring build definitions in the UI is separate from our regular git workflow, and isn't subject to any review\n\nBecause of this, we've made use of the Azure DevOps terraform provider combined with a pipeline to track and automatically apply these changes through code. This documentation will cover any background required, as well as the process to add/change/remove a build definition.","implementation#Implementation":"In each GitHub repository, you will find the directory cicd/build-definitions containing the following files:","maintf#main.tf":"This file defines the required version of the azuredevops provider, the connection to the terraform state bucket, and the instantiation of the azuredevops_build_definition resource. The azuredevops_build_definition resource is created for each pipeline object in the pipeline variable in the tfvars file listed below.","variablestf#variables.tf":"Defines the required variables and variable structure in use by main.tf","repository-nametfvars#<repository-name>.tfvars":"This file contains the variable assignments of the variables defined in variables.tf. The pipelines variable is a list of all pipelines within the relevant repository. Here is an example pipeline configuration:\n{\npipeline_name = \"GCP_Controls_sap-mcsec-compliance-dev_inspec-cluster\"\nyml_path = \"cicd/sap-mcsec-compliance-dev/gcp_controls_cd_inspec-cluster.yml\"\naz_devops_folder_path = \"\\\\cloud-compliance\\\\deployment-pipelines\\\\sap-mcsec-compliance-dev\"\nsource_branch = \"develop\"\ndevelopment = true\nci_trigger = {\nuse_yaml = true\n}\npull_request_trigger = {\ninitial_branch = \"develop\"\nuse_yaml       = true\nforks = {\nenabled       = true\nshare_secrets = false\n}\n}\n}\nMost blocks and variables within the object can be easily mapped to their counterpart in the build definition terraform resource, which is thoroughly documented here. There are a few variables that are not part of the resource, and their documentation and explanation can be found here.source_branch - this variable is used to determine the branch which the build definition should look for the pipeline's yaml file. Often, this variable is the develop branch, but situations exist where this is not the case, such as main for a product build\naz_devops_folder_path - this is directly related to the path variable of the build definition resource, but we should make specific mention of it here. This variable is a great way to keep things organized in Azure Pipelines, as it allows us to view build definitions in a structured way. Put your build definition in a folder that makes sense, and if one doesn't exist, it should be created.\ndevelopment - this variable is a boolean - set to either true or false. This should not be used to indicate the target environment, but rather the lifecycle state of the pipeline you are working on. When this option is set to true, '_dev' will be appended to your build definition name. If you are making changes to a pipeline that already exists in our Azure Pipelines setup, you should make a copy build definition which is marked with development = true. You can also point your copied build definition to your working branch, so that you can test your changes as they are made. Here is an example:Existing Pipeline:\n{\npipeline_name = \"GCP_Controls_sap-mcsec-compliance-dev_inspec-cluster\"\nyml_path = \"cicd/sap-mcsec-compliance-dev/gcp_controls_cd_inspec-cluster.yml\"\naz_devops_folder_path = \"\\\\cloud-compliance\\\\deployment-pipelines\\\\sap-mcsec-compliance-dev\"\nsource_branch = \"develop\"\nci_trigger = {\nuse_yaml = true\n}\npull_request_trigger = {\ninitial_branch = \"develop\"\nuse_yaml       = true\nforks = {\nenabled       = true\nshare_secrets = false\n}\n}\n}\nPipeline Used for Testing and Development:\n{\npipeline_name = \"GCP_Controls_sap-mcsec-compliance-dev_inspec-cluster\"\nyml_path = \"cicd/sap-mcsec-compliance-dev/gcp_controls_cd_inspec-cluster.yml\"\naz_devops_folder_path = \"\\\\cloud-compliance\\\\deployment-pipelines\\\\sap-mcsec-compliance-dev\"\nsource_branch = \"MY_WORKING_BRANCH\"\ndevelopment = true\nci_trigger = {\nuse_yaml = true\n}\npull_request_trigger = {\ninitial_branch = \"MY_WORKING_BRANCH\"\nuse_yaml       = true\nforks = {\nenabled       = true\nshare_secrets = false\n}\n}\n}\nNotice, the name of the build definition in the object remains that same, but by adding development = true this will automatically append _dev to the build definition in Azure DevOps. Additionally, MY_WORKING_BRANCH was added as the source_branch and the intial branch of the PR trigger. This is just an example of how you can carry out testing on your feature branch without affecting the existing build definition.Once you are done testing, open a PR to have your changes merged, and remove the object defining your testing pipeline.","apply-build-definitionsyml#apply-build-definitions.yml":"This is a pipeline that automatically applies changes to the build definition list upon merge to develop. A service account called azdevops-automation was created through Thycotic to apply these changes. The personal access token for this user and azure devops organization url are set in the AzurePipelinesConfigurationAPI variable group.","development-flow#Development Flow":"Now that this process is integrated into our git workflow, you can simply raise a PR to add or remove a build definition from Azure DevOps. Here is an example of how one might go about developing a new pipeline:\nA feature branch is created to develop the new pipeline, called adding-new-az-pipeline\nAn initial commit is made to the pipeline to create the pipeline definition yaml file - called new-pipeline.yml\nA PR is opened, wherein a new build definition is added to the list in cloud-compliance.tfvars. This build definition points to the new-pipeline.yml file in the adding-new-az-pipeline branch, and is marked as a development pipeline with development = true\nThe PR is approved and merged, and the build definition is automatically created in Azure Pipelines. Testing and development can begin.\nOnce testing and development is completed, the developer creates a PR to merge their pipeline to the develop branch, removes development = true from their build definition, and points the build definition to the develop branch instead of adding-new-az-pipeline\nOnce the PR is merged, the new build definition will be automatically updated, and the pipeline is live.","service-account-applying-pipeline-changes#Service Account Applying Pipeline Changes":"A service account called azdevops-automation@global.corp.sap was created through Thycotic to apply changes to the build pipelines. This is because we would not want to use an regular user's personal access token to programmatically apply changes in a pipeline. The personal access token for the service account user and azure devops organization url are set in the AzurePipelinesConfigurationAPI variable group.","managing-the-pat#Managing the PAT":"To create a new personal access token, a user who is dedicated as an account owner in Thycotic will have to login as the user. To do this, you should use an incognito/InPrivate session in your browser, and access the service account's credentials in Thycotic. You can then create a PAT for the user, or disable old PATs. This should be the only type of work done when logged in with this user through the UI.\nNote: some operating systems may still try to sign you in via certificate in an incognito browser session. You'll have to click the \"Sign in with other options\" button before the certificate is used to authenticate your personal user."}},"/internal/ARCHIVED/Minerva_docs/ci_cd/azure_devops_naming_conventions":{"title":"[ARCHIVE] Azure Devops Naming Conventions","data":{"":"All information on Azure DevOps naming conventions for our team can be found here.","pipeline-naming#Pipeline Naming":"Each pipeline configured in our Azure Pipelines should be given a name which allows a user to quickly identify its scope. For this reason, the following attributes, separated by dashes wrapped in spaces, should be included in the pipeline name when applicable in the following order:\nRelevant Hyperscaler in Scope - for instance, if the pipeline is specific to the Azure dispatcher, the Hyperscaler in scope would be Azure\nRelevant topic area - a current list of relevant topics can be seen belowControlsDispatcher\nGCP project to deploy to\nCluster to deploy to\n\nHere is an example pipeline name for Azure controls being deployed to the inspec-cluster on the sap-devsecops GCP account\nAzure - Controls - sap-devsecops - inspec-cluster","yml-file-naming#yml File Naming":"YML files should be included in a folder under cicd which makes sense. For example, all CD pipelines for a specific GCP project should be included under a folder named after that project - all templates should be included under templates - if a pipeline needs a new category, create a new folder under cicd. YML files should be named in all lowercase and include the following, separated by underscores, in the specified order:\nRelevant Hyperscaler in Scope\nRelevant topic area\nci or cd\nCluster to deploy to\n\nHere is an example yml file name for deployment of azure controls to the inspec-cluster on the sap-devsecops projectazure_controls_cd_sap-devsecops_inspec-cluster.ymlWhen possible, create a template pipeline in the cicd/templates folder with the following, separated by dashes, in the specified order:\nRelevant topic area\nci or cd\n\nFor the above example, it would make sense to create a template pipeline which is agnostic to hyperscalers, GCP projects, and clusters. This would allow for hyperscaler specific configurations to be configured in files likeazure_controls_cd_sap-devsecops_inspec-cluster.ymlaws_controls_cd_sap-devsecops_inspec-cluster.ymlgcp_controls_cd_sap-devsecops_inspec-cluster.ymlalicloud_controls_cd_sap-devsecops_inspec-cluster-sg.ymlazure-cn_controls_cd_sap-devsecops_inspec-cluster-sg.ymlaws-cn_controls_cd_sap-devsecops_inspec-cluster-sg.ymlWhile the majority of the deployment definition is specified in the template controls_cd.yml","variable-group-naming#Variable Group Naming":"Variable groups are a convenient way to specify environment-specific variables in a single group. Variable group names should include the following elements in the specified order when relevant, separated by an underscore\nRelevant Hyperscaler in Scope\nRelevant topic area\nRelevant GCP Project\nRelevant Cluster\nRelevant Namespace\n\nHere is an example for the above described scenario\nazure_controls_sap-devsecops_inspec-clusterIf the variable group is relevant for an entire project or entire cluster, omit the other elements of the name, and you will have sap-devsecops or sap-devsecops_inspec-cluster","kubernetes-service-connections#Kubernetes Service Connections":"When creating a kubernetes service connection in Azure DevOps, please include the following in the specified order:\nRelevant GCP Project\nRelevant Cluser\nRelevant Namespace"}},"/internal/ARCHIVED/Minerva_docs/ci_cd/pipeline_documentation":{"title":"[ARCHIVE] Pipeline Documentation","data":{"":"This page provides an overview of all pipelines in our landscape. All existing and new pipelines should be documented here with their background, purpose, and any caveats.\nIf a single pipeline is used, please provide any relevant details, including, but not limited to:\nrequired variables\npre-requisites for onboarding environments (if applicable)\nonboarding new environments.\n\nIf a template is used, please provide any relevant details, including, but not limited to:\nan overview of what the template does\nrequired variables\npre-requisites for onboarding environments (if applicable)\nonboarding new environments\na list of pipelines which call the template with a brief description of the calling pipeline.","cicdtemplatescontrols_cdyml#cicd/templates/controls_cd.yml":"","background#Background":"Throughout 2021, our team put major efforts into developing a release process for Minerva which would allow for automated deployments to dev, pre-prod, and prod projects. This pipeline automates the build and deployment of the pubsub image to any onboarded kubernetes cluster.","implementation#Implementation":"Since each profile follows the same deployment process, a template was written which takes in variable groups as parameters. Any profile-specific configurations can be added in these variable groups.","pipelines-in-use#Pipelines in use:":"sap-devsecops/aws_controls_cd_inspec-cluster.yml - deploys aws profile to sap-devsecops project\nsap-devsecops/azure_controls_cd_inspec-cluster.yml - deploys azure profile to sap-devsecops project\nsap-devsecops/gcp_controls_cd_inspec-cluster.yml - deploys gcp profile to sap-devsecops project\nsap-mcsec-compliance-dev/aws_concur_controls_cd_inspec-cluster.yml - deploys concur-specific aws profile to sap-mcsec-compliance-dev project\nsap-mcsec-compliance-dev/aws_controls_cd_inspec-cluster.yml - deploys aws profile to sap-mcsec-compliance-dev project\nsap-mcsec-compliance-dev/azure_controls_cd_inspec-cluster.yml - deploys azure profile to sap-mcsec-compliance-dev project\nsap-mcsec-compliance-dev/gcp_controls_cd_inspec-cluster.yml - deploys gcp profile to sap-mcsec-compliance-dev project","pre-requisites-for-onboarding-environments#Pre-requisites for Onboarding Environments":"GCR connection is created for each project\nKubernetes connection is created","onboarding-new-environments#Onboarding New Environments":"The following inputs are required to onboard an environment to this pipeline:\nprofile-group - the name of a variable group which provides inspec profile-specific variables\nnaming convention: PROVIDER_controls_PROJECT_CLUSTER\nexample: aws-concur_controls_sap-mcsec-compliance-dev_inspec-cluster\n\n\nvariables to provide:\napp-version: the app version you are building (alicloudAppVersion, awsAppVersion, azureAppVersion, gcpAppVersion)\ndeploy-adhoc: true/false - whether to deploy the adhoc image or not (this isn't used in the pipeline yet)\ndeploy-pubsub: true/false - whether to deploy the pubsub image or not\nprofile: the inspec profile that you are building (sap-aws, sap-gcp, sap-alicloud, sap-azure)\nprovider: the provider that you are building for (aws, aws-concur, aws-cn, gcp, alicloud, azure, azure-cn)\n\n\n\n\ncluster-group - the name of a variable group which provides kubernetes cluster-specific variables\nnaming convention: PROJECT_CLUSTER_NAMESPACE\nexample: sap-devsecops_inspec-cluster_inspec-system\n\n\nvariables to provide:\ncontainer-registry: the name of the container registry service connection to use for building and deploying\ndeploy-env: the variables file deployment name (dev, dev2, preprod, prod)\ndeploy-ns: the namespace to deploy to (typically inspec-system)\nimage-name: the path to build the image with. Should usually be $(project-name)/inspec/$(profile)\nkubernetes-connection: the name of the kubernetes service connection to deploy to (example: sap-devsecops_inspec-cluster_inspec-system)\nrelease-name: the release name for the deployment. Should usually be sap-$(provider)-$(deploy-env)\nvalues-file: the name of the values file to use. Should usually be values.$(deploy-env).yaml\n\n\n\n\nproject-group - the name of a variable group which provides project-specific variables\nnaming convention: PROJECT\nexample: sap-mcsec-compliance-dev\n\n\nvariables to provide:\nproject-name: the name of the GCP project to deploy to","cicdtemplatesdispatcher_cdyml#cicd/templates/dispatcher_cd.yml":"","background-1#Background":"This pipeline automates the build and deployment of the dispatcher service Docker image to any onboarded kubernetes cluster.","implementation-1#Implementation":"Since each provider follows the same deployment process, a template was written which takes in variable groups as parameters. Any provider-specific configurations can be added in these variable groups.","pipelines-in-use-1#Pipelines in use:":"sap-mcsec-compliance-dev/alicloud_dispatcher_cd_inspec-cluster.yml - deploys alicloud dispatcher to sap-mcsec-compliance-dev project\nsap-mcsec-compliance-dev/aws_cn_dispatcher_cd_inspec-cluster.yml - deploys aws-cn dispatcher to sap-mcsec-compliance-dev project\nsap-mcsec-compliance-dev/aws_concur_dispatcher_cd_inspec-cluster.yml - deploys concur-specific aws dispatcher to sap-mcsec-compliance-dev project\nsap-mcsec-compliance-dev/aws_dispatcher_cd_inspec-cluster.yml - deploys aws dispatcher to sap-mcsec-compliance-dev project\nsap-mcsec-compliance-dev/azure_cn_dispatcher_cd_inspec-cluster.yml - deploys azure-cn dispatcher to sap-mcsec-compliance-dev project\nsap-mcsec-compliance-dev/azure_dispatcher_cd_inspec-cluster.yml - deploys azure dispatcher to sap-mcsec-compliance-dev project\nsap-mcsec-compliance-dev/azure-accenture_dispatcher_cd_inspec-cluster.yml - deploys accenture-specific azure dispatcher to sap-mcsec-compliance-dev project\nsap-mcsec-compliance-dev/gcp_dispatcher_cd_inspec-cluster.yml - deploys gcp dispatcher to sap-mcsec-compliance-dev project","pre-requisites-for-onboarding-environments-1#Pre-requisites for Onboarding Environments":"GCR connection is created for each project\nKubernetes connection is created","onboarding-new-environments-1#Onboarding New Environments":"For the cluster-group and project-group variable groups, many of the variables are shared between\nthe dispatcher and consumer cd template. In the cluster-group variable group, the image-name and\nrelease-name are changed to be dispatcher-image-name and dispatcher-release-name.\nThe following inputs are required to onboard an environment to this pipeline:\nprofile-group - the name of a variable group which provides provider-specific variables\nnaming convention: PROVIDER_dispatcher_PROJECT_CLUSTER\nexample: aws_concur_dispatcher_sap-mcsec-compliance-dev_inspec-cluster\n\n\nvariables to provide:\napp-version: the app version you are building (alicloudAppVersion, awsAppVersion, azureAppVersion, gcpAppVersion)\nprofile: the dispatcher profile that you are building (sap-aws, sap-gcp, sap-alicloud, sap-azure)\nprovider: the provider that you are building for (aws, aws-concur, aws-cn, gcp, alicloud, azure, azure-cn, azure-accenture)\n\n\n\n\ncluster-group - the name of a variable group which provides kubernetes cluster-specific variables\nnaming convention: PROJECT_CLUSTER_NAMESPACE\nexample: sap-devsecops_inspec-cluster_inspec-system\n\n\nvariables to provide:\ncontainer-registry: the name of the container registry service connection to use for building and deploying\ndeploy-env: the variables file deployment name (dev, dev2, preprod, prod)\ndeploy-ns: the namespace to deploy to (typically inspec-system)\ndispatcher-image-name: the path to build the image specific to devsecops-dispatcher with. Should usually be $(project-name)/dispatchers/sap-$(hyperscaler)-dispatcher-image\nkubernetes-connection: the name of the kubernetes service connection to deploy to (example: sap-devsecops_inspec-cluster_inspec-system)\ndispatcher-release-name: the release name specific for the devsecops-dispatcher deployment. Should usually be sap-$(provider)-dispatcher-$(deploy-env)\nvalues-file: the name of the values file to use. Should usually be values.$(deploy-env).yaml\n\n\n\n\nproject-group - the name of a variable group which provides project-specific variables\nnaming convention: PROJECT\nexample: sap-mcsec-compliance-dev\n\n\nvariables to provide:\nproject-name: the name of the GCP project to deploy to"}},"/internal/ARCHIVED/Minerva_docs/ci_cd/service_connections":{"title":"[ARCHIVE] Service Connections","data":{"":"Information on Azure DevOps service connections can be found here.Please do not give all pipelines access to a service connection unless a risk assessment has been done along with the topic delegates","kubernetes-service-connection#Kubernetes Service Connection":"When creating a kubernetes service connection on Azure DevOps, please do not create connections with kubernetes service accounts that have admin or cluster admin privileges. Service connections must be namespace-specific and make use of least-privilege principles.After creating a service account with least privilege, you can get the secret for that service account by setting the environment variables SERVICE_ACCOUNT_NAMESPACE and SERVICE_ACCOUNT_NAME running the following command:\nkubectl get secret $(kubectl get serviceaccounts $SERVICE_ACCOUNT_NAME -o custom-columns=\":secrets[0].name\" -n $SERVICE_ACCOUNT_NAMESPACE ) -o yaml -n $SERVICE_ACCOUNT_NAMESPACE"}},"/internal/ARCHIVED/Minerva_docs/compliance_scanning":{"title":"[ARCHIVE] Minerva - Compliance Scanning","data":{"":"","this-section-provides-documentation-for-compliance-scanning-and-reporting#This section provides documentation for Compliance Scanning and Reporting":""}},"/internal/ARCHIVED/Minerva_docs/compliance_scanning/elastic_data_backup_and_recovery":{"title":"[ARCHIVE] Elastic Data Backup and Recovery","data":{"":"","this-section-provides-documentation-for-elastic-data-backup-and-recovery#This section provides documentation for Elastic data backup and recovery.":""}},"/internal/ARCHIVED/Minerva_docs/compliance_scanning/elastic_data_backup_and_recovery/elastic_data_backup_and_recovery_automation":{"title":"[ARCHIVE] Elastic Data Backup and Recovery Automation","data":{"":"There are two mechanisms in place that backup data for Minerva. The first one is\nbased on the elastic native Snapshot and Restore feature. The second is a\ncustom data archiving service which is implemented to create LoBs reports.","elastic-snapshot-and-restore#Elastic Snapshot and restore":"The snapshot and\nrestore\nfeature from elastic has been setup to create snapshots of data and\nconfigurations in elastic on a daily basis. The scheduling policy was done in\nthe Stack Management --> Snapshot and Restore --> Policies page in kibana. A\nrepository was setup as a GCP bucket to hold the snapshots in Stack Management\n--> Snapshot and Restore --> Repositories. In order to get access to the\nbucket, a plugin was needed in elastic. This has been setup in the elastic helm\nchart in the file /elastic/templates/elastic.yaml as:\n- name: plugin\ncommand: ['sh', '-c', 'bin/elasticsearch-plugin install --batch repository-gcs']\nFinally, elastic needs to have read and write permissions to the bucket to\ncreate the snapshots and restore them if needed. This has been done through a\ngcp secret that holds a service account key called elastic-bkp-secret\nmanaged through Terraform, which is then mapped into a K8s secret called\ngcs-credentials. A secureSettings configuration for elastic was then\ncreated in the elastic helm chart in values files as:\nsecureSettings:\n- secretName: \"gcs-credentials\"\nThe restore process can be started from each snapshot and can be monitored in\nthe Stack Management --> Snapshot and Restore --> Restore Status page.","custom-data-archiving#Custom data archiving":"Minerva data in Elastic is archived in a GCP storage bucket for backup\npurposes. The backup process is performed by the data archiver service which\nruns in prod and preprod environments. The archiving frequency can be controlled\nby configurations in the cloud scheduler which determines when the data archiver\nwould run. It is best to configure the data archiver to run only when the scans\nare completed and fully ingested into Elastic. Usually, no additional effort is\nrequired to make the archiver service run as it is handled by the cloud\nscheduler.The data archiver is programmed to archive data from the previous day whenever\nit runs. For example, if the archiver runs on a Tuesday (16.08.2022), it\nautomatically formulates the index to archive based on the previous day's\ndate. So in this case, it would try to archive data from (15.08.2022). Of\ncourse, these are logics embedded in the archiver code and can be adapted to\nsuite any current requirements. The backup files produced are categorized\naccording to severity (highs and mediums), and the mediums are categorized based\non important board areas (PE, TI, others). All backup files are in json format\nand can be re-imported into Elastic through the data recovery process. The data\narchiver needs adequate access and permissions to the Elastic instance and GCP\nbucket. It is important to note that aside from performing backup operations,\nthe data archiver also produces xlsx report outputs used for executive\nreporting.","custom-data-recovery#Custom data recovery":"Backed up data in GCP bucket can be restored to Elastic using the data recovery\nservice. The data recovery service is an adhoc service that should only be\ndeployed for data recovery use cases. Same project or cross project data\nrecovery can be performed by the data recovery service. For same project\nrecovery, the GCP storage bucket containing the backup data and the Elastic\ninstance where data is to be recovered are in the same project. Cross project\nrecovery refers to recovering data to an Elastic instance in a project different\nfrom the project of the GCP bucket containing the backup data. The correct\nvariables should be provided in the values file of the helm chart before\ndeploying. Recovery time is largely dependent on how much backup data is\navailable. Currently (16.08.2022), about 8 months worth of data was recovered\nduring test in about 15 hours. The recovery operation requires data recovery\nservice access to the Elastic instance and GCP bucket.","terminology#Terminology":"The user needs to know the below terminologies, that are used by the service to\nrun recovery steps.\nproject:\nproject where the elastic service is running that needs to be recovered.\nproject_with_bucket:\nproject with bucket where elastic data is archived actively.\nbucket_name:\nbucket name where elastic data resides.","deploying-recovery-service#Deploying Recovery Service":"To deploy the recovery service, use the Rake file in the deploy folder in\nData Recovery\nService.\nFor testing in DEV, to use data from preprod then values.dev.yaml file needs\nto contain project: sap-mcsec-compliance-dev , project_with_bucket:\nsap-mcsec-inspec-preprod, bucket_name:\nelastic-data-archive-bucket-preprod\nFor recovery in PreProd and Prod, the user needs to check that the values\nproject,project_with_bucket, and bucket_name are set properly in\nvalues.{ENV}.yaml. In this case, both project and project_with_bucket must be\nthe same.\n\nOnce the above values are added, then deploy using the rake file.","steps-to-trigger-data-recovery#Steps to Trigger Data Recovery":"To run the data recovery service, push a message {} to the Topic where\nthe subscription is attached, data-recovery-trigger.\nOnce the message is picked from the queue, the recovery service will pick\ndata from project_with_bucket and inject it to elk in that same project."}},"/internal/ARCHIVED/Minerva_docs/compliance_scanning/email_templates":{"title":"[ARCHIVE] Email Templates","data":{"":"","this-section-provides-documentation-and-references-for-email-templates#This section provides documentation and references for Email Templates.":""}},"/internal/ARCHIVED/Minerva_docs/compliance_scanning/email_templates/api_request_template":{"title":"[ARCHIVE] API Request Template","data":{"":"Please answer all questions below.Please have a legitimate business reason for requesting API access for Minerva/Chef Inspec Data. Share why this access is needed and what it will be used for:**___****___**Please consider AdHoc Scanning for your accounts/resources, Multicloud Compliance Mailer or attending the Hyperscaler Security Office Hours:Adhoc Scanning DocumentationMulticloud Compliance MailerHyperscaler Security Office HoursConfirm that you've reviewed the Adhoc Scanning option, Hyperscaler Security Office hours option and the Compliance Mailer option:**___****___**If you can not use the Adhoc Scanning option, please provide a reason:**___****___**Estimated number of accounts (and resources) that will be accessed via API? Please share an estimate:**___****___**Has approval been given from the TRU/BISO for API access? Please attach the approval:**___****___**How often will you be using the API? Explain the frequency you expect to be using the API:**___****___**What's the estimated time frame would you like access? Please share the an estimated time range:**___****___**"}},"/internal/ARCHIVED/Minerva_docs/compliance_scanning/minerva_dash_requirements":{"title":"[ARCHIVE] Minerva Dashboarding Requirements","data":{"":"![Minerva logo]({{ site.baseurl }}/assets/docs-images/minerva_dash_requirements/screenshots/Minerva_logo_white_small.png)This document covers requirements for security compliance analytics as part of Minerva Dashboarding in HS Portal. This is a deliverable for Secure Cloud Delivery 2.0, included in WS2: Security Analytics, WS6: Networking Security, WS7: Host Security and WS8: Kubernetes Security  with key milestones:\nQ1/22: Minerva Dashboarding MVP\nQ2/22: Minerva Dashboarding v1 release\nQ3/22: Compliance Drift reporting\nQ3/22: Golden Image verification reporting\nQ4/22: Network Security compliance reporting and enforcement\nQ4/22: Kubernetes compliance reporting\n\nThese requirements largely cover Q1 and Q2 deliverables, as it is not yet clear enough and too early to tell what exactly will be covered in the Q3 and Q4. These will likely be separate additions to the v1 release, to be defined during upcoming SCD2 Quarterly Planning Workshops, and driven by dependencies such as Cerberus (Workstream 3) or the Kubernetes compliance scanning (Workstream 8). To ensure this flexibility, ensuring a good Continous Delivery process is critical - as well as allow us to respond quickly to changing needs and priorities in compliance reporting.","general-purpose-and-objectives#General Purpose and Objectives":"Tools don't fix security problems, people do\nAs we have seen over the last few years, it is not enough to have compliance scanning in place. We had this in 2019, and the number of alerts simply grew as the size of the landscape grew. Giving LoB teams access to compliance data also proved to be insufficient, as long as there was no great pressure on teams to follow up on alerts. Only when compliance alerts were made visible to the organization, remediation got the support of SGS leadership and the various executive board members, and board area COOs started to keep track of progress did we start to make significant progress. This progress in the various business units came on the back of weekly Prisma data exports and the status reporting driven from it, shared with executive leadership, board area delegates and our Office Hours community on the weekly calls.Key in this has been the data enrichment we already do with Prisma and Minerva scan alerts, to associate cloud accounts with their organizational hierarchy, security attributes and account ownership and roles. This is not something that comes with security tools and yet has proven critical to get organizations to move, based on the pivot tables included in the weekly data exportsThe purpose of our compliance scans with Minerva ultimately, then, is to bring as much as possible of the landscape in compliance with SGS policies (or covered by mitigating controls and granted exceptions). In order to do so, we have to:\nMake alert scans accessible to the teams who can remediate them\nMake the data accessible to security officers and other LoB representative so they can ensure teams remediate them\nMake the compliance posture visible and transparent to executive leadership (whether SGS or within the board areas)\nEmpower those involved in enforcement of policies with easy access to compliance scan data\n\nSo far, the tendency has been to share more liberally, as long as colleagues have a legitimate requirement for the data. The data exports and weekly reporting is shared with the Office Hours invite and email distribution list. That doesn't mean it should be available to everyone, though, so there is a need to verify access to Minerva Dashboards.Beyond that core purpose, though, it is critical we allow our stakeholders to get a grip on the \"shape\" of the landscape. Whether within SGS, within Multi Cloud, let alone elsewhere in the organization, it has been difficult to visualize the variety in workloads, resource sizes and cloud accounts, or the state of their relative compliance posture within their board area or across the company. So far, remediation efforts have been on \"company-wide\" level, whether it comes to top-line high alert numbers, or specific focus areas like S3 buckets. We have not yet been able to easily visualize whether particular policy categories are more or less of a problem in different segments of the company. Only recently, even, we've been able to highlight outliers through alerts/cloud account ratio reporting.","minerva-dashboarding-as-the-face-of-entire-minerva-solution#Minerva Dashboarding as the \"face\" of entire Minerva solution":"Compliance scans run from our Kubernetes cluster are quite abstract for our users. Beyond an architecture diagram there isn't anything to \"touch\", so it remains largely an idea. Especially for our existing Prisma users (in the tool itself), the decommissioning of that CSPM tool will feel like something has been taken away from them. Many mostly interacted already with Prisma alerts through the weekly exports, but many also insisted on Prisma access directly, despite its flaws.Minerva Dashboards will for most of our users and stakeholders be the most tangible and visible part of the entire solution - and especially at executive level. Mailers and data exports will satisfy those \"below\" following up on specific alerts in specific cloud accounts, but many of our \"middle\" and \"top\" users (and hopefully also Account-specific Viewers) will consume Minerva scans primarily through Minerva Dashboards. It is therefore inevitably the \"face\" of the entire solution, and therefore most likely judged primarily on how well it serves our various stakeholders.Even our MVP will provide insights that have as of now not yet been possible, so I doubt we will disappoint. But it does highlight the need for the analytics to be:\nvisually appealing\neasy to use and understand, and\nprovide actionable insights\n\nDespite the \"should\" language, by necessity, in documented requirements, there is (hopefully) something inherently enjoyable  about making \"pretty things\" that show off all the hard work the team has put and is putting into less visible backend engineering. Let's build something extraordinary that no security tool on the market can do!\nNote: I apologize in advance for the \"wall-of-text\" and volume of these requirements. The requirements contain a lot of background information, basic principles and vision statements, even analytics and dashboard theory. Hopefully these are helpful as background for the more specific and detailed \"should\" sections.\n\nNote two: Throughout the requirements, some are speculative or aspirational and up for debate. These are marked with a ? in their section header. These are not MVP or v1 requirements - they may never see the light of day if not practical.","detailed-requirements-by-topic#Detailed Requirements by Topic":"The list below describes different topic areas for the Minerva Dashboards. These generally cover MVP and release 1 requirements, but also look forward to what might lie ahead - as it is good to know what we might ultimately be able to accomplish as part of a future vision. Speculative/aspirational post-MVP/v1 releases are marked with a ? in the requirements in the links below:\nRoles and Personas\nRBAC and Data Access\nData Sets\nHS Portal Integration\nContinuous Delivery: Templating and Continuous Improvement\nDashboards and Flow","background#Background":"The Minerva dashboarding has some legacy. We have been reporting weekly since Aug 2019 with slides like these. Note that these are deliberately from before some of the data visualizations in D3/React replaced elements in it. Slides like these are used both for tracking high and medium alerts (the latter since April/May 2021).The weekly alert slides open with a time-based view:![status 1]({{ site.baseurl }}/assets/docs-images/minerva_dash_requirements/screenshots/analytics-1.png)Followed by a current view of the top 10 alerts and a delta view.![status 2]({{ site.baseurl }}/assets/docs-images/minerva_dash_requirements/screenshots/analytics-3.png)Separately, the weekly status deck reports on number of resources in the landscape (as per Prisma), as well as alerts/(1000) cloud resources and alerts/cloud accounts overall. More recently also the ratio heatmaps have been included on company-wide level.These slides are included in the weekly status deck that is shared with executive leadership on fridays, board area delegates on monday morning, and the wider Office Hours community on Tuesday's calls.This provides context for the direction of the visualizations in the Dashboard Views, and ties back to the \"Show & Tell\" structure. We don't want to take things away, but make them better, more accessible, updated more frequently... and provide a lot more depth than before. The base elements are already there:\nTime-based views (year-to-date progress to targets, delta charts)\nCurrent state views by board area and environment\nAdditional commentary, updated weekly\n\nWhile the weekly status reporting will continue in some way - if only to make it more easily digestible to our executive stakeholders - the same principles should also be baked into Minerva Dashboarding.It is also assumed that Minerva Dashboarding will replace any manual work currently involved in producing weekly slides - short of commentary, and will be based on Minerva Dashboard screenshots. This further speaks to the need to be visually appealing, while at the same time ensuring that everybody through the company is always working from the same dat, and teams know what their leadership sees.Finally, the wish is that Minerva Dashboards may even replace current status reporting on Office Hours with live views - in order to focus specifically on any notable changes over the past week, beyond high level repeating status slides."}},"/internal/ARCHIVED/Minerva_docs/compliance_scanning/minerva_dash_requirements/continuousdelivery":{"title":"[ARCHIVE] Continuous Delivery","data":{"":"The Mode of Operations of the HS SecDevOps team is one of Continuous Improvement and Continuous Delivery, as evidenced by our release process and CI/CD pipeline, our development and operational practices and even how we do documentation (including these requirements), soon extending even into blogs.In the same spirit, a template, development and release process should be established to allow for continuous delivery of new data visualizations, updates, as well as decommissioning of visualizations that are no longer relevant. This continuous delivery of Minerva Dashboards should allow us to respond rapidly  to new demands from exexcutives, changes in focus whether due to policy changes or particular problem areas, and leverage new data sets, new insights and share them with our stakeholder community as soon as we can - of course while maintaining quality standards.","data-visualization-development-short-term#Data Visualization Development (Short Term)":"Development of the data visualizations based on React and D3 as included in Jay's personal company github repository, largely takes place off-line based on a retrieved dataset. That allows for the data visualizations to drive requirements, as well as lighten operational load for weekly status reporting - as is already the case today (Q4/2021).This allows for the screenshots in Dashboard Views, data samples after (hopefully unnecessary) data cleansing and exception handling within the data access component making data requirements much clearer, but also of course the code base itself, and thereby help clarify the necessary backend requirements to enable these databoards integrated into HS Portal, including any required API work, and code-level integration with HS Portal and HS API.","templating-and-continuous-improvement#Templating and Continuous Improvement":"This may be feasible as we work towards the MVP in Q1, but would likely slow our development process down if each off-line demo individually needs to be updated and prepared to get ready for integration with the portal. Therefore, as soon as a practical back-end data access and HS Portal integration method is established, a template should be established that can serve as the basis for future data visualization development.(Ideally) this template should:\nAllow for easy, if not seamless, integration into Minerva Dashboards in HS Portal as additional visualizations\nAllow for data access directly to Minerva alert scan data or data aggregates held in HSDB via HS API\nAllow for data access directly to Minerva alert scan data held in Minerva ELK, either by agreeing on a process to pull data for offline development from ELK directly, or as live query to ELK intermediated via HS API, whichever is more practical\nAllow for data access directly to HSDB data via HS API\nAllow for data access directly to HS Asset data via HS API\nAllow for data access to other future data sources as they emerge\n\nThe first of these is to ensure a quick integration and deployment path from development to production, so we can quickly respond to new requests or needs, as well as new visualizations providing new insight.The rest on the list is to both facilitate data access during development, as well as ease integration for rapid deployment - with the possibility of course to optimize the data query in that process.","encouraging-those-in-the-team-interested-in-d3react-data-viz#Encouraging those in the team interested in D3/React data viz":"This may also be a good point to encourage those in the team with interesting in developing data visualizations, as well as understand how the existing ones work. Only basic knowledge of React (so far) has been required, and while D3 in pure form can be daunting, the combination of React and D3.js makes for a productive pipeline - especially when combining multiple views in a single visualization with a common (filtered) dataset.I can strongly recommend this 17-hours (in two parts) D3 React Tutorial on YouTube for those who want to get started. As the data visualizations are essentially based on the format defined in this course, it would be an excellent introduction directly relevant to the Minerva Dashboarding work."}},"/internal/ARCHIVED/Minerva_docs/compliance_scanning/minerva_dash_requirements/dashboardsflow":{"title":"[ARCHIVE] Dashboards and Flow","data":{"":"I (Jay) spent much of my earlier career in tech in Analytics - in fact for so long that I saw its name change from Decision Support Systems, to Business Intelligence, to Analytics and Big Data, and Metrics, KPIs and OKRs.After that time in analytics, much of it in Business Objects and SAP Consulting where every direct colleague was a BI/Analytics specialist, it is easy to take assumption for granted that those with much less experience in analytics may not share. Moreover, analytics has gone through evolutions as technology has become more readily available, but also information demands and datasets have changed. Whereas analytics and dashboard in the 90s-early 2010s often focused on counting things (sales transactions and value perhaps the most typical example), with the advent of big data analytics, the boundary between data science, data visualization and analytics & dashboarding is increasingly fuzzy.This section therefore includes extensive background in analytics and data visualization that","metric-selection#Metric Selection":"It is easy to just throw a bunch of arbitrary metrics onto a dashboard canvas, and call it a day. Unfortunately, this is often what is done with dashboarding, rather than give it some real thought. Dashboards are often limited by the capabilities of a reporting tool or graphing package, and too often dashboards are produced that may look nice, but don't provide actual insight, exploration or actionable results.The most extreme example I have seen is an SAP internal results deck from a team that had counted the number of meetings and collective number of minutes spent in meetings - as if that by itself was an achievement worthy of communication. So... you spent time together on Teams? How is that relevant in relationship to the success of your program?The success of Project Evolve, Secure Cloud Delivery (1.0), One Strike Hyperscaler Security, Secure Cloud Delivery 2.0 and whatever comes after should be measured by how effective it has been towards improving  driving security improvements in the hyperscaler landscape. The purpose of the Minerva Dashboard is to measure that effectiveness, as well as move the organization to action and facilitate enforcemnent.As indicated in the diagram below, actionable insight should be the minimum standard. The main question should be what is the situation, how does that compare to any targets set, and how does it drive towards action and continuous improvement.![Actionable Analytics]({{ site.baseurl }}/assets/docs-images/minerva_dash_requirements/screenshots/actionable-analytics.png)Analytics and dashboards that provide no insight or only leads to a mild satisfaction of curiosity neither keeps anyone engaged, nor does it drive help us toward remediation efforts and therefore waste a lot of effort and undermines what we're trying to achieve.We don't want to keep the organization in a constant state of shock, though, and the more insightful and effective the Minerva Dashboards, the more we enable our internal stakeholders to get their compliance under control and stay on top of things. We would want to keep them engaged and return frequently, to keep driving towards continuous improvement.It is important therefore that we always keep the following questions in mind for our audience.\nNote: to avoid confusion with the ? marker for aspirational requirements, these deliberately miss one","question-who-is-the-audience#Question: Who is the audience":"We have to keep our intended audience in mind, many of whom are not security experts. Analytics should be clearly explained in jargon-free language and their intention obvious. It must be clear what is good and what is bad, and what desired state is expected.","question-are-we-measuring-the-right-things#Question: Are we measuring the right things":"Given the difference in team size, variety of workloads, number of cloud accounts in use, number of resources deployed, and even cloud providers used, it is very easy to give a distored view. For instance, a simplistic count of outstanding alerts may be the most obvious metric, but that doesn't take into account that different board areas and teams operate vastly different number of cloud accounts. Only counting outstanding alerts is likely to only focus attention on teams operating large numbers of cloud accounts and resources - simply by the size of their landscape.As we have seen in the past two years, though, this often hides the state of policy compliance of smaller teams operating fewer cloud accounts, where their total number of alerts are easily lost among the larger numbers among LoBs operating thousands of cloud accounts like BTP Core, CX, or ECS. For this reason we have already included reporting in the weekly status deck based on ratios of alerts per 1,000 resources or alerts per cloud account. Especially among high alerts we often see an inverse correlation between the size of the public cloud landscape of an LoB (and their associated team size) and the ratio of alerts/cloud acount. If we only counted alerts, this correlation would not be visible at all.","question-are-we-encouraging-the-right-behavior#Question: Are we encouraging the right behavior":"Since the point of analytics is to drive action, it is important we are encouraging the right behavior. That is directly related to picking the right metrics, again, because tracking particular metrics inevitably means that teams start working towards those metrics... and potentially missing out on other actions that may even have higher priority.This also comes into play when we pick \"focus issues\", as we have done with S3 storage buckets in H2/2021 and will do again with unencrypted EBS Volumes after Q1/22. Think of this encouraging of right behavior as campaigns to bring attention to particular topics. This can even be just mentioning and tracking specific alerts or alert types, like the AWS ELB ruleset.","question-are-we-effective#Question: Are we effective":"Beyond whether the dashboards themselves are effective in communicating their message (see diagram above), we have to be able to measure how effective we are in our overall goal to bring compliance alerts down as low as possible. This may seem obvious, but again, it is easy to measure the wrong thing, and therefore fool yourself into a sense of progress - or even by a desire to show success.If we are doing well, we should be able to show that and celebrate it. But if we fall behind, that shouldn't be hidden behind \"green\" stoplights, but highlighted so corrective action is taken. Lack of progress, or falling behind goals, must be communicated just as strongly as success.","question-are-we-enabling-continuous-improvement#Question: Are we enabling continuous improvement":"We don't just want to get teams to go through a one-time exercise to fix their current outstanding alerts, and then think it is done. We want them to return again and again and continuously keep monitoring their landscape and keep improving. We want them to remain up-to-date and informed about changes in policies and how that may (have) effect(ed) their own alert numbers.We keep teams engaged through providing easy access to deep insight into the composition and security compliance posture of cloud accounts under their management. By also allowing them to compare and rate themselves to other teams we further encourage them to return to the dashboard and do better. I am not a big fan of gamification, but a little competition among teams is healthy and encouraging.","story-telling#Story telling":"Good analytics tell a story. But a story has a stucture, with a beginning, a middle and end, as well as a cadence. The first question we all ask is how are we doing. That puts time-based charts first, with progress on the year to targets to begin, and then showing more details how things changed over a certain time period.This flows into current state charts that shows in far greater detail how the current alerts are distributed by organizational hierarchy, environment, provider, etc. The ratio heatmaps, then, show better how business units compare across the organization, now taking their number of cloud accounts into account.Then, if you still want to know more how that breaks down by policy category, across the company and within your (or other) board area, we have the clustering charts.","navigation-tab-structure#Navigation: Tab structure?":"That brings up how navigation between dashboards should be done. As the ? indicates, this is open, but - especially given the assumption for additional visualizations in the future - it may make sense to add sub-menus under main Tabs:\nTime-based Charts\tCurrent State Charts\tFurther Analysis\tYTD Progress\tAlert Status Multi View\tClustering\tDelta Charts\tAlerts/Cloud Accounts Heatmap\tClustering (Board Area)\nDepending on the dashboard page structure chosen, room may need to be made within the menus for medium alerts views.It may also be useful to have at the bottom of each dashboard page a -> next link or button to move to the next visualization.\nNote: We will also need to find a way for Account-specific Viewers to get their unique views. Perhaps a similar submenu structure can be set up for a \"My Accounts\" section? (vs a \"Company-wide\" section for everyone).","general-structure---show--tell#General Structure - Show & Tell":"It would be unfair to our users to give them a data visualization such as this and not tell them what it actually is, and how they should look at it.![dendrogram heatmap]({{ site.baseurl }}/assets/docs-images/minerva_dash_requirements/screenshots/dendro.png)There should be a space to provide more explanation about the chart type and what it shows, and explain potentially how any interactive features work. This description likely updates infrequently.Such a chart description is not likely to change frequently, but that itself is not enough - it only described how the chart works and how it should be consumed. Similarly to the current weekly status deck slides, the dashboards should allow for a frequently (weekly, for instance) commentary space, where our team (Jay, most likely) can comment on notable changes - for instance through policy updates, exceptions done active, or a general positive or negative trend. This commentary is likely to update frequently.","dashboard-page-structure#Dashboard Page Structure?":"Beyond the backend and code challenge potentially to accommodate the description and commentary text areas, we should consider page structure. Many of the visualizations have a lot going on, and that makes text areas to the right or left of them problematic without sizing them down considerably. It may therefore make more sense to structure them vertically, and allow the user to scroll.\nHS Portal Banner\tNavigation Menu?\tText: Title and Chart Description (slow-changing)\tSVG: Data Visualization - High Alerts\tText: Chart Commentary  (frequently changing)\tSVG: Data Visualization - Medium Alerts\tText: Chart Commentary (frequently changing)\nThe thinking behind this would be that most would be interested in the High Alerts, and can scroll down to the Mediums should they wish - and will have loaded by the time the user is done with the High alert visualization.Alternatively, it may be more effective to use submenus under the tab structure with separate entries to high and medium pages\nHS Portal Banner\tNavigation Menu?\tText: Title and Chart Description (slow-changing)\tSVG: Data Visualization - High or Medium Alerts\tText: Chart Commentary  (frequently changing)\nWith full screen windows quite wide, text should be divided into two columns, at least. The page may also require a margin on each side, however reduced screen width will leave less space for these visualizations, especially when showing multiple views.","look-and-feel#Look and Feel":"To ensure coherence of data visualizations in Minerva Dashboarding, they should all look and feel similar as part of the whole. This primarily manifests itself in font, and especially color palette.","accommodating-various-screensizes#Accommodating various screensizes":"All the SVGs resize to screensize - with the subtle exception of the clustering chart which updates its height depending on the number of business units to display. I would not suggest to access these visualizations on a phone, but otherwise they may well accommodate tablets to 2K, 4K and 8K screens.Some adjustments may be required for different screensizes, especially for fonts, despite using \"rem\" units for all text sizes, which are relative to the user's browser default font size. Users should be able to use the zoom in their browsers to adjust font size. This is not ideal, though, and may not be immediately obvious to users.SVG margins may need to be converted to a % value, if necessary.","fonts#Fonts":"SAP's standard font is Arial. The index.css also includes Roboto and Oxygen as sans serif alternatives, a. o., should somehow Arial not be available.","color-scales#Color Scales":"Colors can be tricky in data visualizations. You don't want them to be too bright, they should be distinguishable between categories and should be appropriate for the message the visualization should convey (for instance in good/bad situations). Luckily, there are some good resources online for this, most notably the Sunlight Foundation Data Visualization Style Guidelines (SFSG).The style guide is not followed to the letter, but is especially useful for the color palettes in it, and the colors in the Minerva concept visualizations follow this palette. Note that not all the RGB hex color codes in the guide are correct! So verify in case the color seems off, or not even to be in the same hue. Verify with the separate RGB values.\nNote: these colors have not been tested or verified for color-blindness or other accessibility issues. We could consider an alternative set of color scales that does.","board-area-color-scale#Board Area Color Scale":"The colors below are used for the different board areas. Many of these have come about historically as part of previous status reporting, and are maintained for continuity - with slide shade adjustments to bring them in line with the color sty;es in the SFSG. Consolidation, GF&A and People & Operations all share a purple hue for their colors. This is mostly historic as previously they were reported jointly as 'Other'. Unmapped is a red color, as no accounts and alerts ideally are unmapped. (Note: at the moment these are essentially all Concur)\nBoard Area\tColor\tSample\tConsolidation\t#8E6C8A\t\tCustomer Success\t#B0CBDB\t\tGlobal Finance & Administration\t#B396AD\t\tMarketing & Solutions\t#E6842A\t\tOffice of the CEO\t#5F7186\t\tPeople & Operations\t#684664\t\tSAP Product Engineering\t#BD8F22\t\tTechnology & Innovation\t#FFC000\t\tUnmapped\t#BD2B28\nNote that in drillable charts into board areas, these base colors are used and then varied by luminosity depending on the number of business units at that particular level. That way each board area has a certain \"tone\".","delta-chart-color-scale#Delta Chart Color Scale":"Delta\tColor\tSample\tNegative value (Good)\t#A0B700\t\tPositive value (Bad)\t#BD2D28","environments-color-scale#Environments Color Scale":"Similarly, specific colors are chosen for environment types. Again, these colors match the SFSG. DEV through SANDBOX colors have an element of risk rating in them, going from light yellow (LAB) to red (PROD). Policy violations in a Lab environment still need to be followed up on, but they carry less risk then when seen in QA or PROD landscapes. DEMO and TRAINING landscapes tend to be short-lived, and at least are not connected to anything.\nEnvironment Type\tColor\tSample\tDEMO\t#5F7186\t\tDEV\t#FFC000\t\tLAB\t#F2DA57\t\tPROD\t#BD2B28\t\tQA\t#E6842A\t\tSANDBOX\t#F6B656\t\tTRAINING\t#B0CBDB\t\tUnmapped\t#E5E2E0","provider-color-scale#Provider Color Scale":"At the moment only colors have been chosen for the global three of AWS, Azure and GCP. We'll have to pick a separate color for AliCloud (?). For AWS and Azure China I propose a similar hue but with lighter luminosity (i.e. lighter).\nProvider\tColor\tSample\tAWS\t#E6842A\t\tAzure\t#33B6D0\t\tGCP\t#A0B700","ratio-log-scale#Ratio Log Scale":"The colors for the ratio log scales are:\nFixed\tVariable\tColor\tSample\t100\tmax\t#BD2B28\t\t1\t1\t#FFF000\t\t0.01\tmin\t#5C8100\n\nNote: the yellow midpoint is brighter than other yellows used, but avoids the scale from \"washing out\" entirely and becoming harder to read - as both the max and min colors are more tempered.","font-color#Font Color":"Default font color for titles and axis labels is #363636 (). Hover color is #888888 ().","dark-mode#Dark Mode?":"While the visualizations have been designed against white background, the colors above likely work against either light or dark backgrounds. It may require only some CSS updates for text (titles, axis ticks and labels, links). Such a choice could be implemented through a possible User Preferences (see Usability section in Dashboard Views).","dashboard-views#Dashboard Views":"To avoid this page from getting very long - the detailed dashboard view requirements are in a separate page linked below. It is assumed these are contained within the Dashboard Page Structure?, unless an alternative framing is agreed.Dashboard Views","future-state#Future State?":"The requirements above (as well as the detailed dashboard views) cover the immediate horizon of MVP and version 1 release, or even much of 2022. So what follows is more aspirational and thinking of where we might go. This is certainly subject to change and review as circumstances change and data becomes available. But it is good to present a general idea of where Minerva dashboarding might evolve to.Topics in here and elsewhere that have a ? are such aspirational ideas.","global-drill-through#Global drill through?":"Imagine this use case for security experts in LoBs, security incident response, and operational monitoring.The screen opens up to a view of the world, with locations where resources are deployed highlighted and clickable. To the side are key metrics or event that require alerts, as well as a search box for specific cloud accounts, and navigation to filter by board area and business units.This global view can be zoomed in on, and for each location gives a general indication of its security and compliance state, as well as any markers for specific cloud accounts or issues that require more immediate attention based on their risk level.We can drill into a particular cloud region, and see more detail for that level. If some event requires our immediate attention, or we are looking up a particular cloud account, the Account View opens.","account-view#Account View?":"Lookup for a particular cloud account opens up the Account View. This gives a graphical representation of the resources the deployed in the account, and any other cloud accounts or DCs it is connected to. The cloud account view shows the VPCs/NSGs that separate the networks, and represents the connections between them. Within the VPCs, other resources deployed are shown, such as compute and storage, or any managed services.Each of the resources have indicators associated with them, from compliance scans, vulnerability scans, netflow information, or threat detection events to give both the cloud account as a whole as well as the resources within it a risk rating.Such a view would provide a comprehensive insight into a particular cloud account, whether to follow-up and remediate based on highest risk priority, or for context during initial triage of an active security incident."}},"/internal/ARCHIVED/Minerva_docs/compliance_scanning/minerva_dash_requirements/dashboardviews":{"title":"[ARCHIVE] Dashboard Views","data":{"":"Until the repository for them changes, the visualizations refered to in this page are in the https://github.tools.sap/I818358/d3-react repository. The screenshots included in here come from off-line concepts in that repo.Datasets for these visualizations at the moment is Prisma data exports, and a manually maintained weekly Excel for the year-to-date charts, as well as HSDB data exports where appropriate. The assumption is that this should be relative easy to replace with Minerva scan data, as most of the data fields will match... and in any case the useData.js file tends to transform the data. This is where therefore any necessary data field remapping is likely to take place.Beyond the data requirements to make the visualizations work, hopefully little modification will be required. Should the repo for the visualization have a data folder, it will contain a data sample after processing by useData.js. It is simple to add this for other visualizations by console.log'ing the return value and saving the object. For more complex visualizations, also check the README.mdAs can be seen from the screenshots included in this, these visualizations can be quite busy, and will require some descriptions as well as commentary for notable changes or trends. All visualizations apart from the dendrogram heatmaps also have interactivity allowing the users to drill further into the data. Rather than let users discover that by themselves, it is important there is a space to explain. (See also Dashboard and Flow)","time-based-views#Time-based views":"Time-based views show progress against a timeline to targets - whether successful or not. It indicates whether we are effective and can help identify trends.","targets-intro#Targets (Intro)":"Since Project Evolve, we have set targets. This is no different for 2022. Apart from successful execution on SCD2 deliverables, We have set targets for ourselves as well as the rest of the company when it comes to High and Medium alerts. Nikki and I promised to the Executive Board in early 2020 that we would get the Highs under 20,000 ... when it was still well above 80,000 at the time. Nikki initially didn't agree, as it wasn't really under our control: it required the LoBs to remediate all the alerts we couldn't somehow handle centrally. But what convinced her was the argument that there is no point to a security program if it is not effective. And we surely get funding to fix the problem, right? Not just to put some new tools in place.We ended that year on 5,400. A reduction of 96% of the peak of 135,000 in January.For 2021 the policy set was sharpened and put us just under 10K. We chose to set the target to keep below that. We ended the year 48% below that target at just over 5,200, despite adding nearly 7,000 unencrypted S3 storage bucket alerts in July when the alert was lifted from medium to high severity. And all that while the landscape grew by 2.35x in number of resources deployed.So setting targets is good - especially if you overachieve on them. 😁The targets for 2022 are:\nKeep High severity policy violations below 10,000 (repeat of last year)\nBring Medium alerts down to 25,000 from the current ~230K (a reduction of ~90%)\n\nThe highs don't seem that ambitious given we are at 5,200 before the year starts, but there are 30,000-ish unencrypted EBS Volumes that will become high alerts end of Q1. Given that all of these need to be migrated, that is a significant challenge to the LoBs.The Medium alerts target is ambitious, but here we will also be assisted by the growing role of Cerberus. It is always good to set ambitious targets: you may actually achieve them (as we have in past years).\nNote: these target numbers are based on current Prisma alert counts. Once we can reliably map the current state of Prisma alerts to Minerva alerts we will remap these targets accordingly. Given the increase in controls, the numbers are bound to rise.\nThe targets are set company/landscape-wide (excluding China - although my thinking is to simply add them in, and stick to current targets), and we don't set targets for the board areas invididually. However, as we have seen in 2020 and 2021, board area members compete with each other (especially SAP Product Engineering and Technology & Innovation) on these numbers, with their COO team tracking the numbers closely. The board areas and business units within them may well set their own targets - as we have seen ourselves with pressure from Gary Slater as well as Tom Lee to make alerts go away - not because they care about security, but because they don't want to have to explain why there are so many alerts in their unit.The most important dashboard views are therefore the time-based views, and in particular the progress on yearly targets. That is what account owners, security experts, BISOs, board area delegates and even ourselves get judged on. It can be argued that SCD2 would never even have been possible had we not had the track record in progress towards these targets in prior years.","progress-on-yearly-targets#Progress on Yearly targets":"For the reasons stated in the intro, the year-to-date progress charts to targets are the most important dashboards, and should open by default. The screenshot below is top level, but is intended to also allow drill into the board areas, similar to the Alerts Status Multi View. The visualization below does not show the progress table that has been part of weekly status reporting and still debating whether that should just remain an executive deck item, or should be included. After all, there are separately detailed delta-charts.![Year to date progress chart]({{ site.baseurl }}/assets/docs-images/minerva_dash_requirements/screenshots/ytd-chart.png)The visualization drills two layers down the hierarchy. While the company-wide view will show the targets for the year, there are no board area or LoB specific targets. The benchmark line for those are set at the first entry of the data set for that LoB.\nNote: this chart is the first one that is done on the basis of an ELK KQL query, which is included in the gitrepo. There are some data issues in the set, as well as some days are missing, some days are \"doubled\" and needed some adjustment. Hopefully with Minerva and exception database this will go away.","delta-views-difference-across-time-period#Delta Views (difference across time period)":"Delta Views help teams to understand what has changed in their landscape over a time period. Currently manually via Excel a company wide delta chart is produced (see Background section in index.md) to keep track of changes in the top 10 high alerts over a weekly period.We can do better than that - as with other views in here from the concept, we can segment the data in multiple ways much easier than has ever been possible. So, this doesn't just track the top 10 high alerts across the landscape, it also tracks by policy category and cloud provider, as well as environment type (top 4 by current high alerts) and board area/business unit (top 8 by current high alerts). The number of alerts per segment is included in the titles.![delta chart high alerts]({{ site.baseurl }}/assets/docs-images/minerva_dash_requirements/screenshots/delta-chart.png)The chart is drillable one layer deeper into the organization. More levels can be added.![delta chart high alerts T&I]({{ site.baseurl }}/assets/docs-images/minerva_dash_requirements/screenshots/delta-ti.png)","current-state-views#Current State Views":"Current state views give a deeper view of the current state of the landscape - whether on company-wide level or within levels down in the organization. This allows users at different levels of the organization to get a better understanding of how policy violations are distributed among business units, types of environment, policy category and cloud provider, for instance. This should assist teams and those watching them in targeting who to notify, and/or what policy or policy type to focus on next in the remediation or continuous compliance effort.","alerts-status-multi-view#Alerts Status Multi View":"This visualization starts on company-wide level, segmented by board area, but allow for two drills into the organizational hierarchy and single drills into type of environment and cloud provider, allowing the filters to be combined, by clicking on the chart legends. Through the five different views (two by top 10 alerts, three by policy category) far deeper insight into how policy violations are distributed can be gained much easier than has been possible before through the data exports - or required significant custom Excel work.When the bottom of a level is reached and the legend is clicked again, the visualization resets to company-wide level.![alert status 0]({{ site.baseurl }}/assets/docs-images/minerva_dash_requirements/screenshots/viz-drill-org0.png)Clicking down into the different dimensions allows for views such as:![alert status 1]({{ site.baseurl }}/assets/docs-images/minerva_dash_requirements/screenshots/viz-drill-org1.png)Or such as these:![alert status 2]({{ site.baseurl }}/assets/docs-images/minerva_dash_requirements/screenshots/viz-drill-org2.png)The filters can be applied selectively:![alert status 3]({{ site.baseurl }}/assets/docs-images/minerva_dash_requirements/screenshots/viz-drill-org3.png)At the moment, these are restricted in depth but with the alerts having a (practical) depth of 6 layers into the organization, there is no reason not to go deeper - it will just require an \"escape to top\" button to reset. Alternative, initially we can encourage people to double click an environment or provider to reset.Level of depth is largely a question of size of the aggregate data by each of the dimensions (organization, environment, provider). As the row size of that 3-dimensional matrix doens't change by number of alerts, this should work well for both High and Medium alerts, and similar file size (compared to the concept, which are based on raw data exports).","alertscloud-accounts-ratio-heatmap-organizational-scorecard#Alerts/Cloud Accounts Ratio Heatmap Organizational Scorecard":"While this was developed before the Alerts Status Multi View, it follows a similar idea of drilling 3 levels into the organization. And this visualization similarly resets once the lowest level bottom is reached. This is currently limited by the HSDB portal data export of 4 levels of organizational hierarchy, and is missing environment information. A future improvement could make this also filterable by environment and cloud provider.![ratio heatmap 1]({{ site.baseurl }}/assets/docs-images/minerva_dash_requirements/screenshots/i-heatmap1.png)Drill into the organization by clicking the main board area titles.![ratio heatmap 2]({{ site.baseurl }}/assets/docs-images/minerva_dash_requirements/screenshots/i-heatmap2.png)And again drill into the next layer of the organization.![ratio heatmap 3]({{ site.baseurl }}/assets/docs-images/minerva_dash_requirements/screenshots/i-heatmap3.png)\nNote: it will be interesting to see the difference between ratio heatmaps at different stages in time - i.e. start of the year/quarter and current state. This should just be a question of datasets for different dates feeding into two different instances of the data visualization. Probably not an MVP requirement, but should be considered for v1, or ongoing improvement.","further-analysis#Further Analysis":"Following the division of the submenu in Dashboards and Flow, further analysis provides further inside into the landscape that gets us more into data science-like territory. These try to provide more insight than the current views can do. The Dendrogram Heatmaps are a good example of that, of trying to give different views of the dataset to find new patterns that otherwise would not be obvious.","ratio-and-alerts-by-policy-category-dendrogram-heatmaps#Ratio and Alerts by Policy Category Dendrogram Heatmaps":"There is a lot going on these dendrogram heatmaps, and more info in the relevant README.md. They track alerts by policy category by L3 business unit in the overall view, L4 business units in the board area-specific version. Rather than order the heatmap simply by organization or business unit name, which would by arbitrary, a euclidian hierarchical clustering is used, to find the relative distance between each business unit across this multi-dimensional space.This is perhaps just a fancy way of saying that business units that are similar to each other are put close to each other, which gives us this vertical pattern from higher ratios and alerts across categories, from red/orange to green. Note that the scale is based on a log scale, just as the Ratio Heatmaps, but on a variable scale based on the data set, rather than a fixed scale.![dendrogram heatmap]({{ site.baseurl }}/assets/docs-images/minerva_dash_requirements/screenshots/dendro.png)As it is difficult (impossible) to give the policy categories in a reasonable way, without changing the size of the charts dramatically, so the columns are indicated by letters. There is a tooltip included, though, which provides more information for each individual cells, including the organizational hierarchy.![dendrogram heatmap tooltip]({{ site.baseurl }}/assets/docs-images/minerva_dash_requirements/screenshots/dendro-tooltip.png)The board area-specific version goes down to L4 level for a particular board area - under the assumption that most teams, board area delegates and security exports in the LoBs would be more concerned where they are relative to business units in their own board area. The number of L4 varies and can even be quite small, so in some cases the size of the legend determines the SVG height, rather than the clustered heatmap itself, to avoid that legend doesn't get cut off.![dendrogram heatmap T&I]({{ site.baseurl }}/assets/docs-images/minerva_dash_requirements/screenshots/dendro-ti.png)\nNote: the board area specific version currently has its filter hard-coded. Navigation to this should be discussed. While the visualization is ready, there is no specific requirement to include this in the MVP.","usability#Usability":"We should want to make the Minerva dashboards as easy to use as is practical. Given that we have users that have wide access but likely particular filter preferences, it would be desirable to allow users to set defaults in some way, or save the state of a drill path.","url-re-writing-to-allow-users-to-bookmark-a-drill-state#URL re-writing to allow users to \"bookmark\" a drill state?":"My understanding is that React allows for URL rewriting to allow users to bookmark a particular state at a preferred drill level. If this is feasible, this would be a nice usability feature.","pre-filter-appropriate-charts-to-users-board-area#Pre-filter appropriate charts to user's board area?":"While not restricting access to data, it is conceivable that board area delegates, BISOs and security experts within a particular board area would want to default charts to their board area. For instance, for the Board Area specific dendrogram heatmap, or in the [Alerts Status Multi View](### Alerts Status Multi View), or even conceivably in a similarly drillable YTD progress chart.","user-preferences#User preferences?":"We could consider a setting page with user preferences where users could select particular default values or store drill stages (should URL re-writing be feasible)."}},"/internal/ARCHIVED/Minerva_docs/compliance_scanning/minerva_dash_requirements/hsportalintegration":{"title":"HS Portal Integration","data":{"":"Multi Cloud Portal is already established as the main portal for Multi Cloud related information and access to services - most specifically the Accounts section which we also use during security incidents to look up account owners and security officers, and in return account owners use to update their account metadata as part of the CALM process.HS Portal is a thin client on top of HS API which is driven off of the content of HSDB, including RBAC, etc.","central-multi-cloud-data-access-enablement-hs-daas#Central Multi Cloud Data Access Enablement (HS DaaS)":"A couple of strands between Minerva Dashboarding, HS DaaS (\"Data-as-a-Service\": an effort within HS as a whole to share and enable access to hyperscaler data of various kinds), HSDB and resource asset info collection, and the need for an overall strategy within Multi Cloud to operationalize and provide access to the data we collect came together around a Multi Cloud Data Access platform that leverages what HS Engineering has already built.Beyond Minerva alone, there is a lot of pressure on the HS team to make data available quickly forcing a certain urgency and practical decisions. With HSDB already containing all necessary account metadata and ownership, and HS API already providing API access to either HS Portal Apps or LoBs using it for automation, it only makes sense to build on what already exists.The diagram below evolved out of initial discussions between Tim and Jay with Skylar and Roberto, exploring options for Minerva to integrate with HS Portal. This gave us a good direction, which then escalated dramatically through the SCD2 HS DaaS/Assets MVP, part of Q4 deliverables - and driven specifically by the Asset Management remediation requirements associated with the October Audit Committee report. This concept is now the platform for Multi Cloud data.![HS API Platform]({{ site.baseurl }}/assets/docs-images/minerva_dash_requirements/screenshots/MC-api-platform.png)","hs-assets-vs-hs-daas#HS Assets vs. HS DaaS":"The eventual terminology is yet to be determined but for short-hand's sake, HS Assets refers to data collected \"wide and shallow\" by HS Engineering, HS DaaS (in our context) refers specifically to additional metadata collected from CloudHealth. (See also Data Sets). For the former there is a clear path to integration within HSDB, for HS DaaS data this is yet to be determined.","deep-and-light-integration#Deep and Light Integration":"This concept conceives of both deep and light integration options when it comes to data:\nDeep: move data into HSDB\nLight: intermediate data access through the HS API, letting the HS API backend make a service request to an API to request data","deep-integration#Deep Integration":"In this option, data would be transported at regular frequency from its source data store to HSDB directly. This would mean the data resides directly with the HSDB data, and would provide the fastest response times.","light-integration#Light Integration":"It is likely neither desirable nor practical to in all cases import all data into HSDB from the various data sources we have. In case of Minerva dashboarding, for instance, it may not be practical to move all data from ELK into HSDB, but for instance have queries for alerts associated with specific cloud accounts be made accessible through a live ELK query. In this option, requests to the HS API for such data would result in the HS API backend making a service request to an API exposing that data directly. Such queries could be appended with WHERE clauses dependent on the user's role and data access defined in HSDB.","minerva-dashboarding-as-example-of-deep-integration#Minerva Dashboarding as example of deep integration":"Other Multi Cloud teams are still adjusting to this new strategy, but as we already were committed to HS Portal as the \"host\" for Minerva Dashboarding, we are the 'tip of the spear' for this HS Data Access Enablement platform. HS Engineering is similarly committed. Given our RBAC, data access and data requirements (which include HSDB cloud account data, as well resource data in HS Assets), this can only facilitate the analytics described in Dashboards and Flow.To accommodate resource asset information, HS Engineering is already upgrading their data base platform. This will also allow us to host aggregated scan data in HSDB. All of the analytics included in the requirements operate on aggregated data (i.e. rolled up to a certain level of organizational hierarchy, potentially enriched with public cloud provider and/or environment type), so for most use cases this is likely a feasible approach.","minerva-dashboarding-as-example-of-light-integration#Minerva Dashboarding as example of light integration":"For use cases that cannot be practically accommodated this way, we will need to work with HS Engineering on API access against our Minerva ELK data store. This may be for account owner- or security officer-specific data for specific cloud accounts they are responsible for, regardless of organizational cost hierarchy derived from ISP. This will certainly become more relevant for time-based charts tied to account owners or security officers (\"Account-specific Viewers\").","hs-portal-integration#HS Portal Integration":"Minerva Dashboarding should be accessible from HS Portal's main tile menu (likely replacing the current Prisma tile), and include the HS Portal Banner (even if a modified one with a Minerva logo - TBD with HS Engineering):\n![HS Portal Banner]({{ site.baseurl }}/assets/docs-images/minerva_dash_requirements/screenshots/mc-portal-banner.png)"}},"/internal/ARCHIVED/Minerva_docs/compliance_scanning/minerva_dash_requirements/mcportalintegration":{"title":"[ARCHIVE] HS Portal Integration","data":{"":"HS Portal is already established as the main portal for HyperScaler related information and access to services - most specifically the Accounts section which we also use during security incidents to look up account owners and security officers, and in return account owners use to update their account metadata as part of the CALM process.HS Portal is a thin client on top of HS API which is driven off of the content of HSDB, including RBAC, etc.","central-hyperscaler-data-access-enablement-hs-daas#Central HyperScaler Data Access Enablement (HS DaaS)":"A couple of strands between Minerva Dashboarding, HS DaaS (\"Data-as-a-Service\": an effort within HS as a whole to share and enable access to hyperscaler data of various kinds), HSDB and resource asset info collection, and the need for an overall strategy within HyperScaler to operationalize and provide access to the data we collect came together around a HyperScaler Data Access platform that leverages what HS Engineering has already built.Beyond Minerva alone, there is a lot of pressure on the HS team to make data available quickly forcing a certain urgency and practical decisions. With HSDB already containing all necessary account metadata and ownership, and HS API already providing API access to either HS Portal Apps or LoBs using it for automation, it only makes sense to build on what already exists.The diagram below evolved out of initial discussions between Tim and Jay with Skylar and Roberto, exploring options for Minerva to integrate with HS Portal. This gave us a good direction, which then escalated dramatically through the SCD2 HS DaaS/Assets MVP, part of Q4 deliverables - and driven specifically by the Asset Management remediation requirements associated with the October Audit Committee report. This concept is now the platform for HyperScaler data.![HS API Platform]({{ site.baseurl }}/assets/docs-images/minerva_dash_requirements/screenshots/MC-api-platform.png)","hs-assets-vs-hs-daas#HS Assets vs. HS DaaS":"The eventual terminology is yet to be determined but for short-hand's sake, HS Assets refers to data collected \"wide and shallow\" by HS Engineering, HS DaaS (in our context) refers specifically to additional metadata collected from CloudHealth. (See also Data Sets). For the former there is a clear path to integration within HSDB, for HS DaaS data this is yet to be determined.","deep-and-light-integration#Deep and Light Integration":"This concept conceives of both deep and light integration options when it comes to data:\nDeep: move data into HSDB\nLight: intermediate data access through the HS API, letting the HS API backend make a service request to an API to request data","deep-integration#Deep Integration":"In this option, data would be transported at regular frequency from its source data store to HSDB directly. This would mean the data resides directly with the HSDB data, and would provide the fastest response times.","light-integration#Light Integration":"It is likely neither desirable nor practical to in all cases import all data into HSDB from the various data sources we have. In case of Minerva dashboarding, for instance, it may not be practical to move all data from ELK into HSDB, but for instance have queries for alerts associated with specific cloud accounts be made accessible through a live ELK query. In this option, requests to the HS API for such data would result in the HS API backend making a service request to an API exposing that data directly. Such queries could be appended with WHERE clauses dependent on the user's role and data access defined in HSDB.","minerva-dashboarding-as-example-of-deep-integration#Minerva Dashboarding as example of deep integration":"Other HyperScaler teams are still adjusting to this new strategy, but as we already were committed to HS Portal as the \"host\" for Minerva Dashboarding, we are the 'tip of the spear' for this HS Data Access Enablement platform. HS Engineering is similarly committed. Given our RBAC, data access and data requirements (which include HSDB cloud account data, as well resource data in HS Assets), this can only facilitate the analytics described in Dashboards and Flow.To accommodate resource asset information, HS Engineering is already upgrading their data base platform. This will also allow us to host aggregated scan data in HSDB. All of the analytics included in the requirements operate on aggregated data (i.e. rolled up to a certain level of organizational hierarchy, potentially enriched with public cloud provider and/or environment type), so for most use cases this is likely a feasible approach.","minerva-dashboarding-as-example-of-light-integration#Minerva Dashboarding as example of light integration":"For use cases that cannot be practically accommodated this way, we will need to work with HS Engineering on API access against our Minerva ELK data store. This may be for account owner- or security officer-specific data for specific cloud accounts they are responsible for, regardless of organizational cost hierarchy derived from ISP. This will certainly become more relevant for time-based charts tied to account owners or security officers (\"Account-specific Viewers\").","hs-portal-integration#HS Portal Integration":"Minerva Dashboarding should be accessible from HS Portal's main tile menu (likely replacing the current Prisma tile), and include the HS Portal Banner (even if a modified one with a Minerva logo - TBD with HS Engineering):\n![HS Portal Banner]({{ site.baseurl }}/assets/docs-images/minerva_dash_requirements/screenshots/mc-portal-banner.png)"}},"/internal/ARCHIVED/Minerva_docs/compliance_scanning/minerva_dash_requirements/rbacdataaccess":{"title":"[ARCHIVE] RBAC and Data Access","data":{"":"While this requirement is ordered higher up in the list, for context it is good to also read HS Portal Integration to get a better sense of what is meant by \"HS API mediation\".","rbac-and-hsdb#RBAC and HSDB":"In order to provide access to Minerva Dashboarding, a user should have a role within the HSDB, either as a Global Viewer or Account-Specific Viewer as described in Roles and Personas. For Account-specific Viewers their access should may (or perhaps even should) include Global Views, but have the ability to see analytics based on their existing roles in HSDB - as already maintained through the CALM processes.Rather than try to replicate this in ELK, or in a separate API, mediating requests through the HS API gives us the RBAC we require that is already maintained in HSDB. And whether through the CALM processes or through existing HSDB global roles for additional new users, we do not require a separate onboarding process for Minerva Dashboarding access. With all cloud accounts already automatically onboarded to scanning, Minerva Dashboarding access should be similarly automatic - short of a process of adding new Global Viewers (check with HS Engineering - a process for this clearly already exists given our own global access, as well as SGS GSO).","hs-api-mediation#HS API Mediation":"Given the ability to leverage infrastructure that already exists, and the Multi Cloud strategy for Central Multi Cloud Data Access Enablement as described in HS Portal Integration, RBAC and Data Access should be mediated through the HS API, in collaboration with HS Engineering."}},"/internal/ARCHIVED/Minerva_docs/compliance_scanning/minerva_dash_requirements/rolesandpersonas":{"title":"[ARCHIVE] Roles and Personas","data":{"":"When it comes to users for Minerva Dashboarding, we can distinguish between a large number of different organizational functions and roles, ranging from SGS leadership, compliance functions, as well as security incident response, threat detection and policy teams; to board area delegates that are not particularly technical or security experts as well as BISOs who often are both, security experts fragmented through the organization, or other functions such as Security Validation, One Strike Q-Gate Validators, or other current and future and compliance functions.\nNote: these requirements cover specifically HS Portal-based Minerva Dashboarding, and not API access to alerts or Minerva consumer container users\nWhen you abstract the specific purpose of why a particular user should have access down to what that means, it currently seems to break down to just two main Personas:\nGlobal Viewers\nAccount-specific Viewers\n\nEach example of the functions in the first paragraph appear to break down to one of the two. While it is certainly true that many Global Viewers will be primarily interested in a particular board area (Board Area Delegates) or Business Unit (BISOs and L1 Global Vice Presidents and/or their COO Office), I am not convinced that therefore their access should be restricted to that level. There is value in allowing board areas and business units to measure themselves against each other - and with many teams in SAP Product Engineering and other board areas often dependent on Technology & Innovation platforms and infrastructure, it seems reasonable to provide also such users insight in how they look compared to others in the company.\nNote: See the Usability section in Dashboard Views for handling the board-area or LoB specific use case","global-viewers#Global Viewers":"Quis custodiet ipsos custodes?\nGlobal Viewers get to see the entire dataset, aggregated down to the organizational level that is practical. I see no immediate need for reporting for these users to go down to a named individual level, even if use cases can be conceived for individual account level, which may also facilitate potential data privacy issues and make the size of aggregated data sets more manageable. Note that account metadata access is already available via the Accounts section in HS Portal. Their role in the organization is to watch and follow up on those responsible for individual cloud accounts. Account Owners are accountable for the compliance status of the cloud accounts in their name (and are typically not the admins actually responsible for remediation), and therefore need to make remediation a priority.They are overseen by the Global Viewers performing a compliance or security role of some kind - whether as board area delegate, BISO, or those with cross-board area responsibilities, such as SGS or various compliance roles in the company.\nArguably, Global Viewers and Account-specific Viewers in the LoBs also oversee us and SGS Defensive Architecture, through their follow-up and review of alert scans in their landscapes","account-specific-viewers#Account-specific Viewers":"Account-specific Viewers are viewers that are in some way associated with specific cloud accounts, either as Account Owners, Security Officers and Infrastructure Owners, as well as potentially Security Contacts (including DLs?) and Cost Center Owners. That is, they are somehow named directly in an HSDB field for any particular cloud accounts.The number of cloud accounts monitored by a particular individual varies widely, from just one or a handful, to potentially thousands.Data queries for these users should be filtered down to the cloud accounts they are responsible for.\nNote: I would not consider Account-specific Views to be part of MVP release but for v1 release scheduled for end of Q2/22.","should-account-specific-viewers-also-see-the-global-view---additional-functionality#Should Account-specific Viewers also see the Global View - additional functionality?":"My feeling is that the Account-specific Viewer is an additional role or feature, rather than an either/or exclusive one. Similarly to board area delegates and BISOs being able to measure themselves against each other - would that not also go for Account-specific Viewers? Especially since the data exports already provide the data in raw form to anyone on the Office Hours distribution list.Therefore, ideally Global Views are the default - with additional access granted to Account-specific Viewers for filtered views.Ideally, Account-specific Viewers should be able to drill down to individual accounts. Arguably, so may eventually certain Global Viewers.","question-is-there-a-real-difference-between-a-global-viewer-and-account-specific-viewer#Question: Is there a real difference between a Global Viewer and Account-specific Viewer":"There are other ways to consider the roles and personas. Perhaps there is no real difference in data access - but the abillity to query individual accounts is simply a feature all users should have?That is, is this mostly a question of usability/UX and what a particular user might be most interested in? That is, user preference?","future-additions#Future Additions?":"As elsewhere in these requirements, any header with a ? indicates an aspirational goal, but may also be a feature that is never implemented once discussed. They are not requirements (necessarily) for either MVP or v1 release, or even 2022.","show-me-this-particular-account-users-as-subset-of-global-viewers#\"Show me this particular account\" Users (as subset of Global Viewers)?":"As is the case during security incident response already, there is often a need to check the particular security and compliance posture of a particular account. It is therefore good to plan for an ad-hoc query capability that returns the alerts data for only that particular cloud account.","scan-now-users#\"Scan Now\" Users?":"There has been discussion before for users to potentially be able to kick off an ad-hoc scan based on their role in the HSDB from the HS Portal, rather than through the consumer container. Demand for such a feature has primarily come from colleagues whose role in the organization is more in the oversight area, and tend to not have or easily get access to an API key/secret for a cloud account - or may not even be technical enough to know how to run a docker container command line.","lob-requested-personas-office-hours-feedback-etc#LoB requested personas (Office Hours feedback, etc.)?":"After the release of the MVP (and potentially before that in early previews) it is likely that new use cases emerge directly from LoBs, board area delegates or BISOs. It would be good to have a process to collect feature requests (simply via github?)."}},"/internal/ARCHIVED/Minerva_docs/compliance_scanning/minerva_reporting":{"title":"[ARCHIVE] Minerva Reporting","data":{"":"The links below references the Minerva documentation in detail including Minerva overview, API access, and related policy information which is outdated as of October,2023."}},"/internal/ARCHIVED/Minerva_docs/compliance_scanning/minerva_reporting/data_exports":{"title":"[ARCHIVE] Data Exports","data":{"":"Every Thursday, the results of the Monday scan are shared with the recipients in the DL MC SAE Hyperscaler Security Extended Distro (External). This document describes the process of producing the data export files from the archived files in the GCP Archive Storage Bucket, and sending it out to the audience. Note that these data exports are also the basis for the weekly status reporting. See Status Reporting Data Preparation.So far, exports are sent Thursdays afternoon Pacific time. There is no set time, but they are always sent before 6PM so colleagues globally have them Friday morning.The main steps are:\nScan results validation\nDownloading files\nAdd HSDB account data and insert pivot tables\nUpload to Teams\nSend to recipients with light commentary","scan-results-validation#Scan results validation":"In Elastic, compare results w/previous week for benchmark. Set this filter https://minerva.multicloud.int.sap:5601/app/discover#/?_g=(filters:!(),refreshInterval:(pause:!t,value:0),time:(from:now-12d,to:now))&_a=(columns:!(),filters:!(),index:'6f81b2a0-2a94-11ec-9911-559d0f7a0f46',interval:auto,query:(language:kuery,query:''),sort:!(!(timestamp,desc)))\nSelect 1st scan for filter\nOpen Controls Error Reporting Dashboard in another tab\nSelect 3rd scan for new filter\nOpen Controls Error Reporting Dashboard in another tabControls Error Reporting Dashboard\nFollowing attributes needs to be verified:\nTotal Results of scans (e.g. 10M 3/27/2023 for EU/APAC; 3/26/2023 for US) | 2nd April US; 3rd for EU/APAC: Should be ~10M +- x 4%\nErrors Rate: < 0.02%\nAccounts Scanned: Review Accounts#; should be same or greater for each cloud provider - 100; China 0- -15 on negative\nErrors By Control ID: Sort Number by Descending for new scan only\nIdentify all controls with number of errors >=3\nCompare against exception list with accepted number of digits for errors:\n2_02_iam_group_admin_access -> 3/4 digit, 3_02_storage_account_blobs_logging -> 3/4 digit, 3_02_storage_account_tables_logging -> 3/4 digit, 3_02_storage_account_queues_logging -> 3/4 digit, AWS S3 bucket related controls -> upto 3 digits, storage related controls for ALY -> upto 3 digit\n\n\n\n\n\n\nData validation is done!","downloading-files#Downloading files":"The data exports are in the GCP Archive Storage Bucket\nOpen the bucket and open the excel folder\nSort the Created folder so the newest files are on top most recent scan by filtering on the current weeks Monday scan (e.g. for 4/6 type minerva-2023-04); minerva-2023-<month_no>-<monday's date>\nDownload the minerva-<date>-high-all-bulk-archive.xls and the three minerva-<date>-medium-<TI/PE/Others>-bulk-archive.xls files\nDownload a copy of the HSDB database via this link. If you don't have access to this, make sure you have Reader role in HSDB.\n\n\nNote that the HSDB download can be somewhat unreliable if the connection is interrupted somehow before the download is fully done. Verify that it is a full download by making sure size of the file is around 19.5 MB. You may have to repeat the download!\nOn local machine:\nCreate a new folder using todays date - naming convention  ()\nCopy/Paste file from download area to this new directory","add-account-data-and-insert-pivot-tables#Add Account Data and Insert Pivot Tables":"Our internal stakeholders have come to expect the exports in a particular format and structure. Some teams build automation on these files, so it is good to keep the same format week-to-week, and announce changes in the future during Office Hours.\nOpen each downloaded excel file and save it using this naming convention:\nHighs: Minerva_Open_High_<3-letter month><2 digit days><2 digit year>_ELK.xlsx. Example: Minerva_Open_High_Aug1822_ELK\nMediums: Minerva_Open_Medium_<3-letter month><2 digit days><2 digit year>_ELK_<PE/TI/Other>.xlsx. Examples: Minerva_Open_Medium_Aug1822_ELK_TI.xslx, Minerva_Open_Medium_Aug1822_ELK_PE.xlsx, Minerva_Open_Medium_Aug1822_ELK_Other.xlsx\n\n\nRename the worksheet with the alerts to Minerva High, Minerva Medium (PE), Minerva Medium (TI) or Minerva Medium (Other) as appropriate\nAdd a new worksheet called Accounts. Copy the content of the HSDB download into this worksheet\nGo back to the alerts worksheet and select all\nFrom the Excel menu, insert a Pivot Table in a new worksheet. Rename the worksheet to Pivot Org\nAdd the following fields to the Pivot Rows in Order:\nL1\nL2\nL3\nL4\nL5\nL6\nL7\nL8\ncloud_id\ncontrol_title\n\n\nadd control_title to the Pivot Values\nRight-click (win) or Control-click (mac) one of the L1 fields in the pivot table and select Expand/Collapse > Collapse Entire Field. For the PE and TI mediums, use the L2 field (as they are single board areas)\nRight-click (win) or Control-click (mac) the worksheet and Move or Copy... > Pivot Org, making sure to tick the Create a copy checkbox. Rename the copied worksheet to Pivot Alerts\nRemove all but control_title from the Pivot Rows, then sort the Count of control_title column largest to smallest\nSet File > Always Open Read-Only for MAC and for windows follow these steps: File > Save As > More options... > A dialog box will appear > Click on the dropdown for Tools > General Options.. > Check the Read-only recommended box.\n\nPivot Org\n\n\nPivot Alerts\n\n\nYou should now have four files ready for upload.Minerva High example file (see same HS Secure Architecture & Engineering Sharepoint folder for Medium examples)\nNote: Steps 8 and 9 could be done as well by creating a new pivot table from the alert list and only adding control_title in both Pivot Rows and Pivot Values. The copy approach ensures that the list always matches the Pivot Org numbers so is done for consistency and data quality purpose.","upload-to-teams#Upload to Teams":"The files are stored in MC Secure Architecture & Engineering\nNavigate to the the above mentioned link\nUpload the 4 files you have created","send-to-recipients-with-light-commentary#Send to Recipients with Light Commentary":"Much of the following is essentially templatized. However, it is nice to give a quick update with how the week went. It is therefore recommended to first do at least some of the Status Reporting Data Preparation before distributing the data exports.The screenshot below shows a recent data export mail. The highlighted sections are what changes every week (but of course the format can be changed), the other text can typically stay the same.Data Export Mail\n\n\n\nFind last week's data export mail and Reply All to it. Remove all superfluous text before the \"Hi everyone,\".\nUpdate the Subject line (i.e. remove the Re: that is added, and update the date). Example:\nRe: Hyperscaler Secure Architecture & Engineering Alert Remediation - Minerva Open High and Medium Apr 6, 2023\nHyperscaler Secure Architecture & Engineering Alert Remediation - Minerva Open High and Medium Apr 6, 2023\n\n\nUpdate the links section (first highlighted section) with the links for the newly uploaded files.\nOpen the MC Secure Architecture & Engineering/Weekly Minerva Data Dumps folder in Sharepoint (Menu: Open in Sharepoint)\nClick the link/share icon behind each file and click it\nClick People you specify can view and select People in SAP SE with the link. Click Apply\nClick Copy under Copy Link\nInsert into the data export mail\n\n\nUpdate the commentary (yellow highlighted section) with the changes of the week.\nProof read and send to DL MC SAE Hyperscaler Security Extended Distro (External) and Cc: DL GCS Multi Cloud Security Operations\n\n\nNote: since the recipient list is over 500, expect a significant number of out-of-office notifications in response."}},"/internal/ARCHIVED/Minerva_docs/compliance_scanning/minerva_reporting/manual_dashboard_updates":{"title":"[ARCHIVE] Manual Dashboard Updates","data":{"":"Main steps:\nUpdate data files on local machine\nUpload these data files in the storage bucket located in one of the GCP projects\n\nAt the bottom of this document are the associated KQL queries. These can be pasted these in the Dev Tools query builder and will remain cached in your browser. Since they are set up using relative date math, only the control_impact numbers need to updated.","update-data-files#Update data files":"Get a copy of all the data files locally from the bucket in GCP project. You can use this command: gsutil cp -r gs://sap-mce-app-stage-minerva <local path where you need to copy these files>Use the KQL queries listed below and paste them into Kibana's Dev Tools. Once they are there, they will be cached. You can select any of them by clicking inside of one, and click the Run icon (little triangle) to run the query. Simply Select All to grab the JSON query result.\nSelect the right query and if necessary update the control_impact\nRun the query\nControl/Command-A within the results window to select the whole JSON file\nSelect the right data file for the query. It should be obvious which one it is. The naming convention is <chart type code>-elk-<high|medium>.json. Select All and Paste the copied JSON from ELK into the file. Save it.\nRepeat for each data file\n\nThere are 7 data visualizations each for high and medium. You should have 14 files updated after this.","accountscsv#Accounts.csv":"The Ratio Heatmaps and Dendrogram charts also require an updated download of HSDB data. This is the same data download as described in Data Exports. Download a full copy and overwrite/save it over accounts.csv.","pushing-these-data-files-to-remote-bucket-in-gcp#Pushing these data files to remote bucket in GCP":"There are two projects (staging and Prod), first we need to push these data files to staging bucket using following steps:\ngcloud auth login\ngcloud config set project sap-mce-app-stage\nNavigate to the base path where you have all these data files\nPush these data files to the bucket gsutil cp -r . gs://sap-mce-app-stage-minerva\nVerify if all the dashboards are as per expected from this link: https://test-portal.multicloud.int.sap/minerva (all the dashboards must be visible without any errors, if there are any errors, paste the data again from the Dev tools into the data files and re-push to the bucket)\nOnce verification is done push the changes to prod using following steps:\ngcloud config set project sap-mce-app-prod\ngsutil cp -r . gs://sap-mce-app-prod-minerva\n\nThis should update all the dashboards for the week.","kql-queries#KQL Queries":"The following is a list of KQL queries that feed different data visualizations. They are clearly marked, so it should be obvious which is for which. Note that some charts require an updated account list. You can get that the same way as described in Data Exports.","delta-charts#Delta Charts":"For this and each of the queries, update control_impact according to severity (0.8 = high, 0.5 = medium)\n#delta charts\nGET /inspec-*/_search\n{\n\"size\": 0,\n\"query\": {\n\"bool\": {\n\"filter\": [\n{\n\"range\": {\n\"timestamp\": {\n\"gte\": \"now-10d/d\",\n\"lt\": \"now/d\"\n}\n}\n},\n{\n\"term\": {\n\"control_impact\": \"0.8\"\n}\n},\n{\n\"term\": {\n\"status.keyword\": \"failed\"\n}\n},\n{\n\"terms\": {\n\"mcdb_record.status.keyword\": [\n\"ACTIVE\",\n\"NOTINORG\",\n\"\"\n]\n}\n}\n]\n}\n},\n\"aggs\": {\n\"my_buckets\": {\n\"composite\": {\n\"size\": 65000,\n\"sources\": [\n{\n\"date\": {\n\"date_histogram\": {\n\"field\": \"timestamp\",\n\"calendar_interval\": \"1d\",\n\"format\": \"yyyy-MM-dd\"\n}\n}\n},\n{\n\"L1\": {\n\"terms\": {\n\"field\": \"mcdb_record.cost_object.l4_name.keyword\"\n}\n}\n},\n{\n\"L2\": {\n\"terms\": {\n\"field\": \"mcdb_record.cost_object.l5_name.keyword\"\n}\n}\n},\n{\n\"L3\": {\n\"terms\": {\n\"field\": \"mcdb_record.cost_object.l6_name.keyword\"\n}\n}\n},\n{\n\"provider\": {\n\"terms\": {\n\"field\": \"mcdb_record.type.keyword\"\n}\n}\n},\n{\n\"environment\": {\n\"terms\": {\n\"field\": \"mcdb_record.sec_attrs.environment.keyword\"\n}\n}\n},\n{\n\"policy_name\": {\n\"terms\": {\n\"field\": \"control_title.keyword\"\n}\n}\n}\n]\n}\n}\n}\n}","alerts-overview-alerts#Alerts Overview (alerts)":"# alert overview map\nGET /inspec-*/_search\n{\n\"size\": 0,\n\"query\": {\n\"bool\": {\n\"filter\": [\n{\n\"range\": {\n\"timestamp\": {\n\"gte\": \"now-3d/d\",\n\"lt\": \"now/d\"\n}\n}\n},\n{\n\"term\": {\n\"control_impact\": \"0.8\"\n}\n},\n{\n\"term\": {\n\"status.keyword\": \"failed\"\n}\n},\n{\n\"terms\": {\n\"mcdb_record.status.keyword\": [\n\"ACTIVE\",\n\"NOTINORG\",\n\"\"\n]\n}\n}\n]\n}\n},\n\"aggs\": {\n\"my_buckets\": {\n\"composite\": {\n\"size\": 65000,\n\"sources\": [\n{\n\"date\": {\n\"date_histogram\": {\n\"field\": \"timestamp\",\n\"calendar_interval\": \"1d\",\n\"format\": \"yyyy-MM-dd\"\n}\n}\n},\n{\n\"L1\": {\n\"terms\": {\n\"field\": \"mcdb_record.cost_object.l4_name.keyword\"\n}\n}\n},\n{\n\"L2\": {\n\"terms\": {\n\"field\": \"mcdb_record.cost_object.l5_name.keyword\"\n}\n}\n},\n{\n\"L3\": {\n\"terms\": {\n\"field\": \"mcdb_record.cost_object.l6_name.keyword\"\n}\n}\n},\n{\n\"L4\": {\n\"terms\": {\n\"field\": \"mcdb_record.cost_object.l7_name.keyword\"\n}\n}\n},\n{\n\"provider\": {\n\"terms\": {\n\"field\": \"mcdb_record.type.keyword\"\n}\n}\n},\n{\n\"environment\": {\n\"terms\": {\n\"field\": \"mcdb_record.sec_attrs.environment.keyword\"\n}\n}\n},\n{\n\"policy_name\": {\n\"terms\": {\n\"field\": \"control_title.keyword\"\n}\n}\n}\n]\n}\n}\n}\n}","passed-skipped-failed-psf#Passed Skipped Failed (psf)":"# passed - skipped - failed\nGET /inspec-*/_search\n{\n\"size\": 0,\n\"query\": {\n\"bool\": {\n\"filter\": [\n{\n\"range\": {\n\"timestamp\": {\n\"gte\": \"now-3d/d\",\n\"lt\": \"now/d\"\n}\n}\n},\n{\n\"term\": {\n\"control_impact\": \"0.8\"\n}\n},\n{\n\"terms\": {\n\"mcdb_record.status.keyword\": [\n\"ACTIVE\",\n\"NOTINORG\",\n\"\"\n]\n}\n}\n]\n}\n},\n\"aggs\": {\n\"my_buckets\": {\n\"composite\": {\n\"size\": 65000,\n\"sources\": [\n{\n\"date\": {\n\"date_histogram\": {\n\"field\": \"timestamp\",\n\"calendar_interval\": \"1d\",\n\"format\": \"yyyy-MM-dd\"\n}\n}\n},\n{\n\"L1\": {\n\"terms\": {\n\"field\": \"mcdb_record.cost_object.l4_name.keyword\"\n}\n}\n},\n{\n\"L2\": {\n\"terms\": {\n\"field\": \"mcdb_record.cost_object.l5_name.keyword\"\n}\n}\n},\n{\n\"L3\": {\n\"terms\": {\n\"field\": \"mcdb_record.cost_object.l6_name.keyword\"\n}\n}\n},\n{\n\"L4\": {\n\"terms\": {\n\"field\": \"mcdb_record.cost_object.l7_name.keyword\"\n}\n}\n},\n{\n\"status\": {\n\"terms\": {\n\"field\": \"status.keyword\"\n}\n}\n},\n{\n\"provider\": {\n\"terms\": {\n\"field\": \"mcdb_record.type.keyword\"\n}\n}\n},\n{\n\"environment\": {\n\"terms\": {\n\"field\": \"mcdb_record.sec_attrs.environment.keyword\"\n}\n}\n},\n{\n\"policy_name\": {\n\"terms\": {\n\"field\": \"control_title.keyword\"\n}\n}\n}\n]\n}\n}\n}\n}","year-to-date-chart-ytd#Year-to-date Chart (ytd)":"Note: this query starts on May 5, 2022, after the initial validation period of March and initial fixes during April were absorbed. That makes the tracking more meaningful.\n#ytd-chart\nGET /inspec-*/_search\n{\n\"size\": 0,\n\"query\": {\n\"bool\": {\n\"filter\": [\n{\n\"range\": {\n\"timestamp\": {\n\"gte\": \"2022-05-05\",\n\"lt\": \"now/d\"\n}\n}\n},\n{\n\"term\": {\n\"control_impact\": \"0.5\"\n}\n},\n{\n\"term\": {\n\"status.keyword\": \"failed\"\n}\n},\n{\n\"terms\": {\n\"mcdb_record.status.keyword\": [\n\"ACTIVE\",\n\"NOTINORG\",\n\"\"\n]\n}\n}\n]\n}\n},\n\"aggs\": {\n\"my_buckets\": {\n\"composite\": {\n\"size\": 65000,\n\"sources\": [\n{\n\"date\": {\n\"date_histogram\": {\n\"field\": \"timestamp\",\n\"calendar_interval\": \"1d\",\n\"format\": \"yyyy-MM-dd\"\n}\n}\n},\n{\n\"L1\": {\n\"terms\": {\n\"field\": \"mcdb_record.cost_object.l4_name.keyword\"\n}\n}\n},\n{\n\"L2\": {\n\"terms\": {\n\"field\": \"mcdb_record.cost_object.l5_name.keyword\"\n}\n}\n},\n{\n\"L3\": {\n\"terms\": {\n\"field\": \"mcdb_record.cost_object.l6_name.keyword\"\n}\n}\n}\n]\n}\n}\n}\n}","ratio-heatmaps#Ratio Heatmaps":"Note: this visualization also requires an updated accounts.csv\n#ratio\nGET /inspec-*/_search\n{\n\"size\": 0,\n\"query\": {\n\"bool\": {\n\"filter\": [\n{\n\"range\": {\n\"timestamp\": {\n\"gte\": \"now-3d/d\",\n\"lt\": \"now/d\"\n}\n}\n},\n{\n\"term\": {\n\"control_impact\": \"0.5\"\n}\n},\n{\n\"term\": {\n\"status.keyword\": \"failed\"\n}\n},\n{\n\"terms\": {\n\"mcdb_record.status.keyword\": [\n\"ACTIVE\",\n\"NOTINORG\",\n\"\"\n]\n}\n}\n]\n}\n},\n\"aggs\": {\n\"my_buckets\": {\n\"composite\": {\n\"size\": 65000,\n\"sources\": [\n{\n\"date\": {\n\"date_histogram\": {\n\"field\": \"timestamp\",\n\"calendar_interval\": \"1d\",\n\"format\": \"yyyy-MM-dd\"\n}\n}\n},\n{\n\"L1\": {\n\"terms\": {\n\"field\": \"mcdb_record.cost_object.l4_name.keyword\"\n}\n}\n},\n{\n\"L2\": {\n\"terms\": {\n\"field\": \"mcdb_record.cost_object.l5_name.keyword\"\n}\n}\n},\n{\n\"L3\": {\n\"terms\": {\n\"field\": \"mcdb_record.cost_object.l6_name.keyword\"\n}\n}\n},\n{\n\"L4\": {\n\"terms\": {\n\"field\": \"mcdb_record.cost_object.l7_name.keyword\"\n}\n}\n}\n]\n}\n}\n}\n}","dendrogram#Dendrogram":"Note: this visualization also requires an updated accounts.csv\n#dendro\nGET /inspec-*/_search\n{\n\"size\": 0,\n\"query\": {\n\"bool\": {\n\"filter\": [\n{\n\"range\": {\n\"timestamp\": {\n\"gte\": \"now-3d/d\",\n\"lt\": \"now/d\"\n}\n}\n},\n{\n\"term\": {\n\"control_impact\": \"0.8\"\n}\n},\n{\n\"term\": {\n\"status.keyword\": \"failed\"\n}\n},\n{\n\"terms\": {\n\"mcdb_record.status.keyword\": [\n\"ACTIVE\",\n\"NOTINORG\",\n\"\"\n]\n}\n}\n]\n}\n},\n\"aggs\": {\n\"my_buckets\": {\n\"composite\": {\n\"size\": 65000,\n\"sources\": [\n{\n\"date\": {\n\"date_histogram\": {\n\"field\": \"timestamp\",\n\"calendar_interval\": \"1d\",\n\"format\": \"yyyy-MM-dd\"\n}\n}\n},\n{\n\"L1\": {\n\"terms\": {\n\"field\": \"mcdb_record.cost_object.l4_name.keyword\"\n}\n}\n},\n{\n\"L2\": {\n\"terms\": {\n\"field\": \"mcdb_record.cost_object.l5_name.keyword\"\n}\n}\n},\n{\n\"L3\": {\n\"terms\": {\n\"field\": \"mcdb_record.cost_object.l6_name.keyword\"\n}\n}\n},\n{\n\"policy_name\": {\n\"terms\": {\n\"field\": \"control_title.keyword\"\n}\n}\n}\n]\n}\n}\n}\n}","compliance-heatmap#Compliance Heatmap":"GET /inspec-*/_search\n{\n\"size\": 0,\n\"query\": {\n\"bool\": {\n\"filter\": [\n{\n\"range\": {\n\"timestamp\": {\n\"gte\": \"now-3d/d\",\n\"lt\": \"now/d\"\n}\n}\n},\n{\n\"term\": {\n\"control_impact\": \"0.8\"\n}\n},\n{\n\"terms\": {\n\"mcdb_record.status.keyword\": [\n\"ACTIVE\",\n\"NOTINORG\",\n\"\"\n]\n}\n}\n]\n}\n},\n\"aggs\": {\n\"my_buckets\": {\n\"composite\": {\n\"size\": 65000,\n\"sources\": [\n{\n\"date\": {\n\"date_histogram\": {\n\"field\": \"timestamp\",\n\"calendar_interval\": \"1d\",\n\"format\": \"yyyy-MM-dd\"\n}\n}\n},\n{\n\"L1\": {\n\"terms\": {\n\"field\": \"mcdb_record.cost_object.l4_name.keyword\"\n}\n}\n},\n{\n\"L2\": {\n\"terms\": {\n\"field\": \"mcdb_record.cost_object.l5_name.keyword\"\n}\n}\n},\n{\n\"L3\": {\n\"terms\": {\n\"field\": \"mcdb_record.cost_object.l6_name.keyword\"\n}\n}\n},\n{\n\"L4\": {\n\"terms\": {\n\"field\": \"mcdb_record.cost_object.l7_name.keyword\"\n}\n}\n},\n{\n\"status\": {\n\"terms\": {\n\"field\": \"status.keyword\"\n}\n}\n}\n]\n}\n}\n}\n}"}},"/internal/ARCHIVED/Minerva_docs/compliance_scanning/minerva_reporting/status_reporting_dataprep":{"title":"[ARCHIVE] Status Reporting Data Preparation","data":{"":"Status reporting of security compliance alerts has evolved in a SecDevOps way since August of 2019 to what it is today. Status reporting changes slowly, to ensure that there is consistency from week to week. Some things, like the bar charts colors or delta table structure, go back to January 2020 when we first had Prisma exports for High alerts.Since we have yearly targets and on executive level there is an expectation for quarterly progress updates, changes should be carefully introduced.Before starting data preparation for the status deck, make sure you have the data exports done.The data preparation involves the following steps.\nConsolidate Mediums\nUpdate the Year-to-Date Tracking\nTop 10 Deltas","consolidate-mediums#Consolidate Mediums":"The data exports for Mediums, to keep file sizes manageable, are shared as three separate files. In order to report on the full data set, we have to create a consolidated file.\nCreate a new Excel file, and name it Minerva_Open_Medium_<3-letter month><2 digit days><2 digit year>_ELK_ALL.xlsx. The file name ultimately doesn't matter as you won't share this - but it makes it easier for yourself to keep it consistent.\nCopy the alerts from the TI, PE and Other files into this worksheet. For the 2nd and 3rd, remove the header row. You should now have the full set in one worksheet\nSelect all and insert a Pivot table\nFollow the same steps 5-9 as described in the Data Exports section. You don't need the Accounts copied in. Again, you're not sharing this.","update-the-year-to-date-tracking#Update the Year-to-Date Tracking":"The latest will be shared on hand-over:Start with the Minerva High worksheet.","update-the-tables#Update the Tables":"Insert a new columns wherever the latest results are shown\n\n\n\n\nSelect the last column in the top table, then drag it over another column from the bottom right handle. You should end up with a duplicate, but the date incremented by 1\n\n\n\n\nUpdate the date. This is not a real date, just change the number\nDo the same for the index tables. Select the last column of both tables and drag it over.\n\n\n\n\nUpdate the date. These are real dates and are used in the line charts, so you will need to give this correct dates in date format\nThe first row in the first index chart is the total number of scans. Note that this is all of them, not just the highs. We should change this once the compliance rate KPI is approved. Now we can update it here, then have it updated automatically to the mediums.\nGo into Kibana and find the total number of scans for this particular scan\ninsert it in the field in Excel - this calculates the high failed alerts/total scans once we add in this weeks results in the top table\n\n\nThe first row in the 2nd ratio table is for the total number of active accounts. This allows us to calculate the ratio of high severity alerts/cloud account\nOpen the latest high alert data export you have just created\ngo to the Accounts worksheet and select all\nInsert a pivot table in a new worksheet\nadd cloud_id to the Pivot Values, type to Pivot Rows and status to filter\nFilter the set so only ACTIVE, NOTINORG and blanks are included (i.e. exclude DELETED, INACTIVE, etc.)\nTake the total and put it in Excel\n\n\n\nNow we need to update the delta table. This is a bit trickier, as there are a lot of formulas driving this.\nUpdate the dates in the column headers (see upcoming picture)\nSelect the top cell from the previous week, and drag it over to the next column\n\n\n\n\nUpdate the next column so it reads from the newly added column in the top table\nDo the same for the percentage calculation\n\n\n\n\nSelect the three cells we've updated for this top row and drag it down for all board areas. Do NOT include the totals line\n\n\n\n\nSelect last week's total cell, and drag it over to the next column\n\n\n\n\nUpdate the next column so it reads from the totals of the newly added column in the top table\nDo the same for the percentage calculation\n\n\n\nYou should now be ready to enter the high severity alerts from the Highs data export in the top table, and the delta table should update.The process for Medium alerts is the same, with the exception of entering total scans and number of accounts. The mediums ratio table has formulas that read those values from the Highs worksheet, and will grab the new values as the previous week column is dragged one column over to the new week.","update-line-charts#Update Line Charts":"For each of the two line charts in both worksheets, we need to explicitly add in the new column of data. For each:\nRight/Control-click the chart and Select Data\nFor each of the data series, update the Y-axis to include the new column, as well as the horizontal (categorical) axis labels, by incrementing the last column in the data array.","top-10-deltas#Top 10 Deltas":"This one can be a bit tricky, and a bit annoying as Excel will protest a lot that the formulas are incorrect. Just click that away.\nNote: Because this delta will change each week, before anything else, save the last week's Top 10 sheet with an updated date!\nWe start with the previous week's delta table\n\n\nDelete the first column with numbers, i.e. 2nd column\n\n\n\n\nDelete the 'delta' column (last one)\n\n\n\n\nSort the table with Count of Alerts largest to smallest\n\n\n\n\nOpen the high or medium (ALL) alerts data export for the week, and copy the top 10-12 or so from the Alerts Pivot worksheet\nPaste it next to the existing table with last week's results\nNow comes the tricky part of lining the control titles in column A with those in column C. You will have to insert rows and cut & paste this or last week's control title and number to make the two data sets line up\nRemove any unnecessary rows, delete column C with the 2nd set of control titles\nadd 'delta' as next column header, then add a formula in the first cell of = <this week> - <previous week>\nSort the table on the current week's alert numbers Smallest to Largest\nFor the chart, choose Select Data, and select ONLY the first column and the delta column\nSelect all data points and set the fill color to either dark red or light green, depending on which are more above of below zero\nAdjust the color of individual data points in the series going the other way\nAdjust the dates in the title"}},"/internal/ARCHIVED/Minerva_docs/compliance_scanning/minerva_reporting/status_reporting_deck":{"title":"[ARCHIVE] Status Reporting Deck","data":{"":"The Status Reporting deck needs to be created before Friday as they are used by others in developing the Executive Update on Friday.","alerts-status-reporting-purpose#Alerts Status Reporting Purpose":"We have multiple data channels for Minerva compliance data. This is one of the most important ones. While the Mailers and Data Exports have a wider reach into the organization, the status reporting keeps central security teams up to data, as well as executive leadership across the board areas. It is intended to make current compliance status visible, as well as any major changes. It is also how we as a team are most visible to senior leadership.Luckily, much of this by now has an established format. Most of the work will be copy and paste, with commentary added to help interpret what is being shown. The commentary allows us to highlight particular successes or challenges we want to bring attention to.This deck also gives us the opportunity to communicate the projected and actual (once deployed) impact of upcoming releases. The community always appreciates the heads-up, as well as what effect the control changes had on the landscape. Such information also allows central security teams in the LoB to prepare the teams they represent, as well as their leadership. This is especially important in case the projections lead to an increase in alerts.","status-reporting-preparation#Status Reporting Preparation":"Before starting the status reporting deck, make sure you do the data preparation, as well as update the Minerva dashboards - even if just locally. They are inputs to the status deck.","status-reporting-deck#Status Reporting Deck":"There is a sequence to the deck. An example is here\nAny major announcements - new policies to be introduced, start of Preventive Controls Deployment Rings, etc.\n\"Release Note\" slides of policy changes with projected impact (if release still to be deployed to Prod) or actual impact (first Monday scan results after a new release). Given the 2-weekly release cycle, we should expect this to be a standard section in the deck\nHigh Severity Alerts\nYear to date overview and delta table with high level commentary\nTop 10 violations and weekly top 10 delta (or major changes)\nRatio progress tracking\nRatio Heatmap\n\n\nMedium Severity Alerts\nYear to date overview and delta table with high level commentary\nTop 10 violations and weekly top 10 delta (or major changes)\n(Campaign Slide: Key Rotation Alerts Tracking)\nRatio progress tracking\nRatio Heatmap\n\n\n\"Standard\" Slides\nPreventive Controls Table\nMinerva Remediations\nMinerva slide, including link to latest Minerva Consumer Container\nLinks\n\n\n\n\nNote: Additional slides may be added and become recurrent based on current campaigns.","major-announcements#Major Announcements":"Start with providing any major announcements. e.g. a new high-impact control that needs to be discussed, or any announcement of significance.Currently, the preventive control slide should be part of the Major Announcement section. Once Deployment Ring 4 is completed it can be placed in the standard slides.","release-note-slides#Release Note Slides":"We have developed a nice sequence with the 2-weekly Minerva updates of being able to announce ahead of the deployment what the projected impact of control and policy changes is, based on pre-prod, following the next week by the actual impact we found in the first production scans. This is a really good way to keep the community informed and engaged with the changes.","high--medium-slides#High & Medium Slides":"Make sure you have done the reporting data preparation and the manual dashboard updates.","year-to-date-overview-and-delta-table-with-high-level-commentary#Year to date overview and delta table with high level commentary":"Take a screenshot of the Year-to-date Minerva Dashboard (your local dev instance, most likely) and crop it to just the line charts\n\n\n\n\nNote: maximize your browser window screen area (and if necessary, refresh the page) to make the screenshot.\n\nPaste it into the slide and resize it over the existing line chart from the previous week. Send it to back, and delete the previous.\nHide all but the last 4 delta columns in the minerva-high-alert-chart-2022.xsx Excel sheet, and copy the delta table.\nPaste it into the slide as an image and resize it over the table from the previous week. Send it to back, and delete the previous.\n\n\n\n\nDelete the text (it's often easier to do the commentary last, when you have done all the slides, or at least the next one) and update the date in the title","top-10-violations-and-weekly-top-10-delta-or-major-changes#Top 10 violations and weekly top 10 delta (or major changes)":"Take a screenshot of the Alerts Overview Minerva Dashboard and crop it to just the Top 10 Alerts by Board Area and Environment\n\n\n\n\nPaste it into the slide and resize it over the existing chart from the previous week. Send it to back and delete the previous week's chart.\nCopy the Top 10 delta chart from the top10 Excel sheet, and paste it as image into the slide deck. Resize it, place it over the existing and move to back. Delete last week's.\nUpdate the list with the controls having the most change, up or down. In a good week, highlight progress, but include top riser at least. In a bad week, highlight the main risers, but include some good news (if there is any)\nUpdate the date in the title\nCopy the ratio tables from minerva-high-alert-chart-2022.xsx Excel sheet while the hidden columns are still hidden. Paste it into the slide as image. Resize and place over the existing, and remove the previous.\n\n\n\n\nUnhide the hidden columns in the Excel sheet to get definition in the line charts\nCopy each of the two line charts from the minerva-high-alert-chart-2022.xsx Excel sheet. Absolute numbers on top, ratios below.\nPaste each as image into the slide deck, resize and place over the ones from the previous week, and send to back. Remove the previous week's chart.\n\n\n\n\n\n\n\nUpdate the commentary with the change for the week\nUpdate the title date","compliance-heatmap#Compliance Heatmap":"Take a screenshot of the Compliance Heatmap Minerva Dashboard and crop it e.g.:\n\n\n\n\nPaste it into the slide, resize it, and place it over the previous week's. Send it to back and delete the old\nCopy the alerts/cloud account ratio and change from the previous slide and replace the previous week's in the commentary.","touching-up#Touching Up":"If you haven't provided a commentary yet on the first slide with the year to date progress, do it now. Do a final check around the slides for any missing content or edits.","standard-slides#Standard Slides":"The standard slides don't change as much, but make sure to update the release name in the Minerva Consumer Container link. Update any links in the final links slide as necessary."}},"/internal/ARCHIVED/Minerva_docs/release_process":{"title":"[ARCHIVE] Release Process","data":{"":""}},"/internal/ARCHIVED/Minerva_docs/release_process/controls_deployment_workflow":{"title":"[ARCHIVE] Minerva Release WorkFlow","data":{"":"","overview#Overview:":"This document describes detailed workflow of a Minerva release lifecycle. Minerva release cycle is of 2 weeks and will run through the course of entire sprint. Each Minerva release will be referenced by a unique global city name.","general-development#General Development:":"All the tasks/features will be worked on individual feature branches, once development work is complete the changes are merged in develop through a Pull request that will require approval from at-least 2 team members. These feature branches will be created based on the develop branch.","release-initiation#Release Initiation:":"This describes the process to initiate a release which includes release branch creation, naming convention and cut-off time for the updates to be included in the release.\nRelease branch is created on Monday, start of the sprint at 12 PM PST.\nThe release branch will be created based on the latest state of develop branch.\nThe name of branch should be in this format: release-city_name.\nAll the PR's/changes that are planned to be in the release must be merged into develop before the time mentioned.\nNo other changes will be included in the release after that time unless its a high priority.\nThis release branch is pushed to remote.\nA PR is created to merge this release branch into main.\nTitle for PR will be in this format: Release-City_name.\nRelease-City_name PR description needs to be updated based on format here","pre-production-release#Pre-Production Release:":"This section describes how a Pre-Production release workflow is followed. This includes details related to format of version bumps and also explains high level working of CD pipelines with detailed steps for manual deployments if necessary.\nFirst deployment to Pre-Prod will be done on the Monday, start of the release cycle.\ncreate new branch out of release-city_name branch.\nVersion bumps will be done for hyperscalers that have the changes in the release. For instance if Alicloud has no changes in the release, the version bumps will just be done for AWS, Azure and GCP. Alicloud version remains the same.\nIncrement the version in following files:\nservice/consumer-hyperscaler/inspec.yml. (e.g. 4.10.0-rc1 bumps up to 4.11.0-rc1)\ndeploy/values.preprod.yaml (e.g. 4.10.0-rc1 bumps up to 4.11.0-rc1)\n\n\nCreate a PR for these version bumps to be merged back into release-city_name branch.\nThis PR will again require approvals from 2 team members.\nAs soon as the version bumps PR is merged into release-city_name branch, CD pipeline will trigger.\nManual approval will be required for the pipelines. This can be given by Justin, Rohit or Carmelo.\nThe pipline stages will build and push the images, deploy all the workloads for relevant hyperscalers to the Pre-Prod project.\nAt the moment we have following workloads:\ninspec cluster: aws, azure, gcp, aws-concur, azure-accenture\ninspec-cluster-sg: alicloud, aws-cn, azure-cn\n\n\nFor any reason, if the pipelines are not running or pipelines have failed we will have to do the manual deployment. This step-by-step guide can be found here","pre-production-release-results-verification#Pre-Production release results verification:":"Once the Pre-Production release is completed, before moving forward withe the Production the result needs to analysed and verified. Any outliers found in the results needs to be justified. There are various parameters that will require an overview and verification which included number of accounts scanned, overall error rate, etc. and if needed detailed analysis for any of the components.\nThe Pre-Prod deployment results can be verified following Wednesday.\nFollowing parameters should be mainly looked at from the Controls Error reporting dashboard which is already created:\nTotal number of records.\nOverall Error rate, Individual hyperscaler error rate, total number of errors, number of controls affected.\nTotal number of accounts scanned for each hyperscaler.\nTotal number of alerts for High's and Medium's -> These numbers should pretty much tally up with expected impact of numbers based on the control updates included in the release.\nThese numbers should be more or less similar to the previous scans -> any outliers should be justified with a reason.","second-pre-prod-release-optional#Second Pre-Prod release (Optional):":"Second PreProd release is optional depending on what the results are for first scan. If there are any additional updates/fixes these are first merged in the relase branch through a PR and second Pre-Production release is done in the following way:\nThis will be done on following Thursday if necessary.\nIf there are any errors discovered from the first pre-prod scan results, these errors will be fixed. PR will be created to merge that fix back into the release branch.\nThere should just be one commit for the fix.\nCommit of this fix should have [skip ci] at its end. This is done as we don't want the pipelines to trigger before updated version bumps. Adding [skip ci] at the end of commit will prevent the pipelines from triggering.\nThe PR is merged into release branch with 'rebase and merge' strategy.\nVersion bumps:\nservice/consumer-hyperscaler/inspec.yml. (e.g. 4.11.0-rc1 bumps up to 4.11.0-rc2)\ndeploy/values.preprod.yaml (e.g. 4.11.0-rc1 bumps up to 4.11.0-rc2)\n\n\nFrom this point same process will be followed as it was done during the first one and results will be verifed on the following Monday (second half of sprint).","production-release#Production release:":"This section describes detailed steps for final Production release.\nCM ticket will be created. One example is mentioned here\nVersion bumps for Prod release:\nservice/consumer-hyperscaler/inspec.yml. (e.g. 4.11.0-rc1 bumps up to 4.11.0)\ndeploy/values.prod.yaml (e.g. 4.10.0 bumps up to 4.11.0)\n\n\nThese version bumps will be done only for those hyperscalers that were part of Pre-Prod release.\nThis should be merged back into the release branch through a PR. This PR should just have one commit for version bumps.\nCommit of these version bumps should have [skip ci] at its end to prevent from the pipelines re-triggering for Pre-Prod again.\nThe release PR will require two approvals one from Jay and other from one team member.\nOnce the PR is approved it should be merged in main branch.\nAt this point the pipeline will trigger to complete the deployment or if manual deployment needs to be done, step-by-step guide can be found here.","post-production-release-tasks#Post-Production release tasks:":"Once deployment to production is complete, all the necessary version bump commits and fixes/updates if any from the current release needs to be included back in the develop branch. New release needs to be created, tagged and release notes must be updated. This section describes how this process is carried out.\nSwitch to main -> Do 'git pull'\nSwitch to develop -> Do 'git pull'\nCreate a new branch out of develop -> do 'git merge origin/main'\nPush the branch and create new PR to merge this into develop.\nThis PR will again require approvals from 2 team members.\nOnce this PR is merged into develop, develop branch should be up-to-date with main.\nCreate new release here and tag it at the latest commit in main branch. The tag should follow vCity_name as naming convention.\nRelease notes should be updated in this release. The release PR description should be taken as reference.","documentation-update#Documentation Update:":"The end-user documentation has to be updated in our GitHub pages. This process is documented in this section.\nSwitch to main branch in cloud comliance repo -> make sure it is up-to-date\nThe DevSecOps docs repo needs to be cloned since all the control details will be updated there.\nNavigate to that relevant hyperscaler controls folder in cloud comliance repo:\nservice/consumer-hyperscaler\ninspec json . > 'local path to root of DevSecOps docs repo'/docs/_data/sap-hyperscaler.json\n\n\nUpdate the consumer versions for hyperscaler that are part of release in the docs repo -> devsecops-docs/docs/_data/release-metadata.yaml\nCreate a PR to merge that into main in docs repo. This will update the documentation.","public-facing-artifactory-update#Public facing Artifactory update:":"The updated ad-hoc images from the current release will have to be pushed to the remote artifactory.\nUpdate the image names as per the hyperscaler and its corresponding version in the release as follows:\ndocker tag eu.gcr.io/sap-mcsec-inspec-prod/inspec/sap-hyperscaler:tag mcsec.common.repositories.cloud.sap/inspec/profiles/sap-hyperscaler:tag\n\n\nFinally push the images.\ndocker push mcsec.common.repositories.cloud.sap/inspec/profiles/sap-hyperscaler:tag"}},"/internal/ARCHIVED/Minerva_docs/release_process/controls_development":{"title":"[ARCHIVE] Controls Development with Chef Inspec","data":{"":"This document describes the overall Controls development process.\nThe examples and the commands documented below are specific to AWS profile however, similar process can be followed for Azure, GCP and Alicloud as well since the profile structure for AWS, Azure, GCP and Alicloud hyperscalers are all the same.\nTerraform tests:\nThe terraform tests include good tests, bad tests and tests required to cover edge cases for a control.\nIn good tests resources are deployed in the way that ideally the control should pass that test. In bad tests resources are deployed in the way that ideally the control should fail that test.\nThe terraform tests in the repository are structured in modules:\nCreate relevant resource in any of the existing modules or if a module does not exist create a new one.\nThe attribute values for the resource in a module are parameterised in the form of variables in order for its reusability. The value for these variables are then added in good terraform main.tf and bad terraform main.tf depending whether it is a good or a bad resource.\n\n\n\n\n\n\nStep 1: Set environment variables to write terraform tests and run Chef Inspec:\nRelevant environment variables must be set depending on for which hyperscaler control development is done.\n\n\nAWS\nAWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, AWS_DEFAULT_REGION, AWS_REGION, AWS_AVAILABILITY_ZONE, AWS_MASTER_ACCOUNT_ID\nAWS_DEFAULT_REGION = us-east-1\nAWS_REGION = us-east-1\nAWS_AVAILABILITY_ZONE = us-east-1a\n\n\nAzure\nARM_CLIENT_ID, ARM_CLIENT_SECRET, ARM_SUBSCRIPTION_ID, ARM_TENANT_ID, AZURE_SUBSCRIPTION_ID, AZURE_CLIENT_ID, AZURE_TENANT_ID, AZURE_CLIENT_SECRET,  ARM_ORG_ID\n\n\nGCP\nGOOGLE_APPLICATION_CREDENTIALS, GCP_PROJECT_ID, GCP_ORG_ID, TF_VAR_GOOGLE_PROJECT\nGCP_PROJECT_ID = sap-mcsec-inspec-test2\nTF_VAR_GOOGLE_PROJECT = sap-mcsec-inspec-test2\n\n\n\n\nStep 2: Create Good/Bad Terraform tests\nDeploying a specific module in good terraform (similar process for bad terraform)\n- cd profiles/sap-aws/test/integration/good-terraform\n- terraform init\n- terraform plan -target=module.<module_name>\n- terraform apply -target=module.<module_name>\n\nOnce testing is done Do not forget to destroy the resources\nTo destroy resources:\n- cd profiles/sap-aws\n- terraform destroy\n\n\n\n\n\nStep 3: Writing/Testing Chef Inspec control\nThe chef inspec controls are present in following directory of the repo: profiles/sap-aws/controls\nThere is a wiki tag (eg: SAP: 1.60.05) associated with each of the control. Each control is present in a file with .rb extension named under this wiki tag  (eg: sap_1_60_05).\nThis Wiki tag can be found in the control tracking sheet.\nFor new control, if a file with that tag already exist, it will be written in the same file. Ff a file with that tag does not exist a new file is created in the same format and control is added in that file.\nControl components:\nThe name of the control should be in following format: 'sap_<wiki_tag>_<small_description_of_the_control>'. One example would be 'sap_1_50_05_s3_bucket_public'\nThe title can be taken directly from the controls tracking sheet\nThe description has to be formulated with reference from SGS Wiki chapter\ntag sgs_wiki_url: SGS Wiki chapter link has to be added\n\n\nTesting Controls: Once control has been written\nMethod 1: Testing in current repository\nRun a specific control:\nAWS\n- cd profiles/sap-aws\n- inspec exec . -t aws:// --input aws_master_account_id=<enter_master_account_id> --controls <enter_your_control_name>\n\nAzure\n- cd profiles/sap-azure\n- inspec exec . -t azure:// --controls <enter_your_control_name>\n\nGCP\n- cd profiles/sap-gcp\n- inspec exec . -t gcp:// --input gcp_project_id=sap-mcsec-inspec-test2 gcp_organization_id=<enter_gcp_org_id> --controls <enter_your_control_name>\n\n\n\nRun all the controls:\nAWS\n- cd profiles/sap-aws\n- inspec exec . -t aws:// --input aws_master_account_id=<enter_master_account_id>\n\nAzure\n- cd profiles/sap-azure\n- inspec exec . -t azure://\n\nGCP\n- cd profiles/sap-gcp\n- inspec exec . -t gcp:// --input gcp_project_id=sap-mcsec-inspec-test2 gcp_organization_id=<enter_gcp_org_id>\n\n\n\n\n\nMethod 2: Testing using Inspec Shell\nClone the AWS Inspec resource pack (AWS Resource Pack)\nNavigate to base of the repo:\n- cd inspec-aws\n- inspec shell -t aws:// --depends .\n\nThis will open up an inspec shell where you can run the entire control or even 'cd' into a resource and test the attributes individually . eg: To test attributes for a s3 bucket\n- cd aws_s3_bucket(bucket_name: 'bucket_name')\n- ls\n\n\n\n\n\n\n\nStep 4: Running Integration Test suite\nOnce all the terraform tests are written, control is developed and it is tested locally next step is to run the test suite:\nFollow this process:\n- cd profiles/sap-aws\n- rake test:update_expected_results\n\nThe test suite run is automated and will do the following:\nInitialize good terraform\nInitialize bad terraform\nCreate good terraform resources\nRun inspec tests against those resources\nDestroy good terraform resources\nCreate bad terraform resources\nRun inspec tests against those resources\nDestroy bad terraform resources\nRender results of the run in 'expected_results.json' file. This file can be found here: /profiles/sap-aws/test/results\nThis file contains list of all the controls and their expected good configuration and bad configuration results.\nIt is necesary to verify if only that new control which was developed is added into this file with its expected results. (Ideally expected results should be good configuration passing and bad configuration failing)\n\n\n\n\n\n\nStep 5: Building docker images\nThere are two docker images whose built needs to be verified as a part of control development.\nTo verify this follow this process:\n- cd profiles/sap-aws\n- rake build:all[sap-mcsec-compliance-dev,3.0.0-alpha.1]\n\nThis will build PubSub and ad-hoc docker images which should be successful.\n\n\nStep 6: Creating a PR\nOnce all the steps have been followed, a PR is created to merge into 'develop' branch.\nThe PR should consist screenshots for good and bad tests and a small description regarding the control and the test cases covered.\n\n\n\n\nCreate a new feature branch from develop.\nWrite Good/Bad terraform tests.\nDevelop a control.\nTest the control locally using any of the 2 methods mentioned above.\nRun the test suite and verify just that new control is added in expected_results file and rest of the results for all other controls are the same.\nVerify Docker images are built successfully.\n\n\nAWS Resource Pack\nAzure Resource Pack\nGCP Resource Pack"}},"/internal/ARCHIVED/Minerva_docs/release_process/infra_release_workflow":{"title":"[ARCHIVE] Infrastructure Release WorkFlow","data":{"":"","overview#Overview:":"This document describes detailed workflow of Infrastructure release lifecycle. Infrastructure release is done monthly for preprod on Wednesdays starting at CET 10:00 AM, and for prod 1 week after on Tuesday CET 10:00 AM. Each infrastructure release will be referenced by its version number following Semantic Versioning. We use release-please to generate the changelogs, therefore the conventional commits pattern must be followed on commit messages. A pipeline runs before each merge on develop branch, which enforces such commit message patterns.In case some issue is observed the respective person should be informed, or a message in #mc_devsecops_developer slack channel should be posted.","general-development#General Development:":"All the tasks/features will be worked on individual feature branches, once development work is complete the changes are merged in develop through a PR that will require approval from at least 2 team members. These feature branches will be created based on the develop branch.","release-initiation#Release Initiation:":"This section describes the process to initiate a release, which includes release branch creation, naming convention and cut-off time for the updates to be included in the release.\nRelease or release candidate branch is created 1 day before the release (Tuesdays for preprod and Mondays for prod).\nThe release branch will be created based on the latest state of develop branch.\nThe name of branch should be in this format: release-vX.X.X-rc for release candidates or release-vX.X.X. Release manager can find out the exact followup release version by running a dry run of the release-please tool. See here for instructions on how to run the dry-run command for releases.\nAll the PRs/changes that are planned to be in the release must be merged into develop before the time mentioned.\nNo other changes will be included in the release after that time unless its a high priority.\nFor preprod: A release candidate branch of format release-vX.X.X-rc is created out of develop.\nFor prod: A release branch of version release-vX.X.X should be created from the release-vX.X.X-rc, then a PR is created to merge the release-vX.X.X branch into main.\nIn the case of needing to deploy a HOTFIX, a hotfix-vX.X.X branch should be created from release-vX.X.X and then merged to main AND develop through a PR process.\nTitle for these PRs will be in this format:Merge *branch* into main or Merge *branch* into develop.\nFor each process in the release chain, involving a \"PR\", IT IS MANDATORY TO USE A MERGE COMMIT instead or REBASE AND MERGE. MORE INFO BELOW\nWhen merging any PR into the main branch, a pipeline is triggered and utilizes the release-please tool. This tool opens automatically a PR targeting the main branch, and updates the CHANGELOG.MD file, with all the changes coming up to this version.\nOn the changelog, should be explicitly specified which of the directories (cicd, helm, k8s, terraform) has had changes, together with the commit message. This is achieved by strictly following the conventional commits, but also appending the folder name on the commit message.","important-info-to-consider#Important info to consider:":"WHEN MERGING A HOTFIX OR RELEASE THROUGH A PR, WHETHER IT IS MAIN OR DEVELOP, PLEASE USE A MERGE COMMIT INSTEAD OF A REBASE AND MERGE\n(Doing a rebase and merge will give that specific commit a new hash on main or develop, and on the subsequent release, this particular commit will be identified again as a new feature by release-please)WHEN WRITING THE COMMIT MESSAGES (AFTER THE MESSAGE HEAD), STARTING WITH A BLANK SPACE AND LOWER CASE LETTERS IS MANDATORY.\nEXAMPLE:\ngit commit -m \"type(folder):<one_blank_space>this commit message always starts with a lowercase letter\"\nCompliant commit message examples:\ngit commit -m \"feat(cicd): feature1 #issue_number\"\n\ngit commit -m \"feat(helm): feature2 #issue_number\"\n\ngit commit -m \"feat(k8s): feature3 #issue_number\"\n\ngit commit -m \"feat(terraform): feature4 #issue_number\"\nRelease-please would produce CHANGELOG file that looks like this:Featurescicd: feature1 (ISSUE_NUMBER_HYPERLINK)(COMMIT_HYPERLINK)helm: feature2 (ISSUE_NUMBER_HYPERLINK)(COMMIT_HYPERLINK)k8s: feature3 (ISSUE_NUMBER_HYPERLINK)(COMMIT_HYPERLINK)terraform: feature4 (ISSUE_NUMBER_HYPERLINK)(COMMIT_HYPERLINK)","deployment-process#Deployment process:":"As mentioned above, in this repository there are 4 main folders where different build commands and approaches are used. Depending on the folder, where updates are made, the following steps must be followed:It is important to mention that we do not have a central management solution for secrets. Whenever you deploy any changes that have to do with creation/deletion/update of secrets: please make sure to cross check if that is a shared secret, update the value or version on Secret Manager, and bump also the Version property on .values file on each service that the secret is being used.Please start deploying your newly developed features beginning with /terraform folder. Since terraform is at the core of our infrastructure, it needs to be deployed first, so other components such as k8s and helm do not have unmet dependencies.","cicd#/cicd":"As per current state, CI/CD folder is in its initial stages and only performs terraform checks and some pipelines for running the release-please tool. This documentation will be updated according to upcoming changes.","helm#/helm":"Open terminal and cd into the respective folder:\ncd /devsecops-infra/helm/sap-compliance-reporting\nor\ncd /devsecops-infra/helm/stackdriver-metrics-adapter\n\nAuthenticate with sap-mcsec-inspec-preprod or sap-mcsec-inspec-prod GCP project:\nUse this terminal command to login via your SAP linked google account:\ngcloud auth login\nWhen logging in for the first time:\nNavigate to Google Cloud Console.\nMake sure you are on the right project depending on the release(preprod or prod).\nNavigate to Kubernetes Engine -> Clusters.\nOn the column of elastic-cluster, click on three dots, that should open the tab with instructions to gain access on the cluster.\nDownload the kubeconfig file for the respective elastic cluster, and check if you are performing operations in the correct cluster by verifying via these commands:\nkubectl config get-contexts\nor\nkubectl get pods -n <namespace>\nEXAMPLE: kubectl get pods -n elastic-system\nWhen you already have downloaded the config file before:\nTo get the different projects that you already have access to:\nkubectl config get-contexts\nthen run the following command without need to go the google cloud console:\nkubectl config use-context <cluster-name>\nEXAMPLE: kubectl config use-context gke_sap-mcsec-inspec-prod_europe-west4_elastic-cluster\n*please note that you have to be connected to the SAP VPN to perform kubectl operations.\n\n\nCheck the available commands with:\nrake -T -A\n\nAlways perform a rake dry_run_command[env] and cross check the changes that will be applied or any error occurring. For example:\nrake dry_run_deploy_elastic[env]\n\nUpon completion of step 4, apply the changes. Example:\nrake deploy_elastic[env]\n\nExplore other rake tasks with the commands from step 4, in case of need.\nFollow the steps defined here, to verify that changes are applied correctly, and have not affected existing infrastructure.","k8s#/k8s":"Open terminal and cd into the respective folder:\ncd /devsecops-infra/k8s/gatekeeper\nor\ncd /devsecops-infra/k8s/network-policies\n\nMake sure you have access to the cluster, if that is not the case follow step 2 here.\nIn case you are making changes on the /gatekeeper folder:\n*You might need clusterrolebinding access (you may not be able to install CRD-s in case these rights do not exist):\nCheck the available rake commands commands and their explanation with:\nrake -T -A\n\nRun the following command to create clusterrolebinding (in case permission is not previously granted):\nrake install:clusterrolebinding[your_email_address]\n\nRun the following command to install/uninstall all CRD-s:\nrake install:crds\nrake uninstall:crds\nrake install:specific_crd[crd_folder_name]\nrake uninstall:specific_crd[crd_folder_name]\n\nCheck if the install/uninstall has been completed successfully by running:\nkubectl get constraints\nIn case you are making changes on the /network-policies folder:\nCheck the available rake commands and their explanation with:\nrake -T -A\n\nRun the following commands to install and uninstall network policies :\nrake install:net_policies_<respective-cluster>\nrake uninstall:net_policies_<respective-cluster>\n\n\n\nFollow the steps defined here, to verify that changes are applied correctly, and have not affected existing infrastructure.","terraform#/terraform":"Currently, terraform state is saved into a GCP bucket. This bucket is particularly important, because is a single point of failure for the whole infrastructure.\nTerraform structure is due to changing for the infrastructure repository, however, the following guide is relevant in deploying changes, as per current repository state.\nOpen terminal and cd into the respective folder:\ncd /devsecops-infra/terraform\n\nNavigate Google Cloud Console.\nMake sure you are on the correct project(preprod or prod).\nNavigate to Cloud Storage/Buckets section to confirm the bucket name that holds the terraform state for the infrastructure you want to deploy.\n\n\nThere are different buckets depending on the cluster and environment.For PREPROD these buckets are used:\nmcsec-inspec-state-preprod\nmcsec-elastic-state-preprod\n\nFor PROD these buckets are used:\nmcsec-elastic-state-prod\nmcsec-inspec-state-prod\n\n\nCheck the available rake commands and their description with:\nrake -T -A\n\nInitiate terraform in the current directory by running:\nrake init[gcp_project,<cluster>,<bucket>]\n\nEXAMPLE: rake init[sap-mcsec-inspec-preprod,elastic-cluster,mcsec-elastic-state-preprod]\n\nIn case you have initiated terraform before, run this command to get the latest state, as someone might have changed it lately:\nrake refresh[gcp_project_name,env,<cluster>]\n\nRun a terraform plan, to cross check what are the resources that are to be modified/deleted/created and observe whether that is aligned with the changes associated to the PRs:\nrake plan[gcp_project_name,env,<cluster>]\n\nIf checks for step 6 are passed successfully, apply the changes using this command:\nrake apply[gcp_project_name,env,<cluster>]\n\nIf you run into the errors where terraform is complaining about resources already existing, you have to sync the terraform state with the resources existing on the infrastructure.\nIdentify the resource name, and initiate import with this command:\ncd /<folder_of_the_cluster>\nterraform import  -var-file=<environment>.tfvars -var='project_id=<your_project_id>' <resource-type> <resource-name>\nFollow the steps defined below, to verify that changes are applied correctly, and have not affected existing infrastructure.","post-release-checks#Post Release Checks:":"After completing the applying the changes, it is very important to perform the following checks in order to verify that the changes are applied correctly and the systems are up and running with no errors.\nAfter the release, the infra team members in each region, should constantly monitor to make sure no disruption of services occurs.\nNavigate Google Cloud Console/Clusters preprod or Google Cloud Console/Clusters prod, and make sure no visible errors exist on the clusters, from the web interface.\nNavigate Google Cloud Console/Services preprod or Google Cloud Console/Services prod, and make sure no visible errors exist on the services, from the web interface.\nNavigate Google Cloud Console/Workloads preprod or Google Cloud Console/Workloads prod, and make sure no visible errors exist on the workloads, from the web interface.\nClick on the specific workloads that changes are made, observe the logs, notice any causalities.\nIn case there were updates on the /k8s/gatekeeper make sure:\nThe desired CRD-s are installed/uninstalled, with this command:\nkubectl get constraints\n\nCheck the Total Violations of the installed CRD-s whether existing workload have failed the compliance checks that this CRD enforces:\nkubectl describe <constraint_kind> <constraint_name>\n\n\n\nLogin to Elastic, check and verify that old data exists.","failure-mitigation-strategy#Failure Mitigation Strategy:":"In case of failures in /helm directory:\nperform a helm rollback with the commands provided by rake file.\ncd /devsecops-infra/helm/sap-compliance-reporting\nrake -T -A\nor\ncd /devsecops-infra/helm/stackdriver-metrics-adapter\nrake -T -A\n\n\n\nIn case of failures in /terraform directory:\nNavigate to the artifacts of the last stable release.\nChange directory into the right folder:\ncd /devsecops-infra/terraform\n\nexplore commands with rake -T -A  and apply (you can also follow steps from here)\n\n\nIn case of failures in /k8s directory:\nChange directory into the right folder:\ncd /devsecops-infra/k8s/gatekeeper\nor\ncd /devsecops-infra/k8s/network-policies\n\nexplore commands with rake -T -A  and apply the respective uninstall command to remove the module that is failing.\n\n\nIn case of a failure with ELK stack, where elastic has to be redeployed and data is lost, check Data Recovery Documentation.","post-deployment-release-process#Post Deployment Release Process:":"After the successful release, where all the steps above have been completed successfully, a documentation update should be followed.\nFor each merge into the main branch, a pipeline is triggered running release-please which opens another PR against main, adding automatically the changes mentioned above. Release manager, should followup and make sure that this PR is approved and merged to main.\nWhen the release-please PR from step 1 is merged, another pipeline is triggered, and it automatically creates a release, with a bumped version accordingly, creates a tag, and links the artifacts.","dry-run-for-release-please#Dry Run for release-please:":"This tool could not be run against a local branch. Therefore, to be able to perform a --dry-run and generate the exact upcoming release version, this CLI tool must target a new remote branch (example:version-of-next-release) that is a merge of the current main and latest develop.To create this branch and push it to remote:\ngit checkout develop\ngit pull\ngit checkout -b version-of-next-release\ngit merge origin/main\ngit push origin HEAD\nTo run release-please against this branch and get the next release version number:\nrelease-please release-pr \\\n--token=YOUR_GITHUB_TOKEN \\\n--config-file=cicd/release-please-config.json \\\n--manifest-file=cicd/.release-please-manifest.json \\\n--repo-url=mce/devsecops-infra \\\n--api-url=https://github.tools.sap/api/v3 \\\n--graphql-url=https://github.tools.sap/api/v3 \\\n--target-branch=version-of-next-release \\\n--dry-run\nAfter user has noted the version number, this branch must be deleted locally and remote:\nLocally:\ngit branch -d version-of-next-release\nRemote:\ngit push origin --delete version-of-next-release"}},"/internal/ARCHIVED/Minerva_docs/release_process/minerva_release_notes_format":{"title":"[ARCHIVE] Release Notes Format","data":{"":"#     Minerva Release Notes Format\n\n# Release City_name\n\n### Controls:\n\n- AWS:\n- Control title for which there is a change\n- Specify what the change is and what the impact is.\n\n- Azure:\n- Control title for which there is a change\n- Specify what the change is and what the impact is.\n\n- GCP:\n- Control title for which there is a change\n- Specify what the change is and what the impact is.\n\n- Alicloud:\n- Control title for which there is a change\n- Specify what the change is and what the impact is.\n\n### Remediations:\n- AWS: /external/compliance_scanning/minerva_aws_controls\n- Azure: /external/compliance_scanning/minerva_azure_controls\n- GCP: /external/compliance_scanning/minerva_gcp_controls\n- Alicloud: /external/compliance_scanning/minerva_alicloud_controls\n\n### Container image version for Boston release:\n\n| Hyperscaler   | Consumer version           | Dispatcher version           |\n| ------------- | -------------------------- | ---------------------------- |\n|      AWS      |  Present Consumer version  |  Present Dispatcher version  |\n|      GCP      |  Present Consumer version  |  Present Dispatcher version  |\n|      Azure    |  Present Consumer version  |  Present Dispatcher version  |\n|      Alicloud |  Present Consumer version  |  Present Dispatcher version  |"}},"/internal/ARCHIVED/Minerva_docs/release_process/release_documentation":{"title":"[ARCHIVE] Release Documentation","data":{"":"infra/helm/sap-inspec-consumer/Chart.yaml (appVersion)\nprofiles/sap-aws/inspec.yml (version)\nprofiles/sap-azure/inspec.yml (version)\nprofiles/sap-gcp/inspec.yml (version)\n\n\nAuthenticate with 'sap-mcsec-compliance-dev' GCP project\n\"gcloud auth login\"\n\n\nAuthenticate Docker with the project in order to push Docker images\n\"gcloud auth application-default login\"\n\"gcloud auth configure-docker\"\n\n\nBuild and Push Docker Images:\ncd cloud-compliance/profiles/sap-aws (directory)\n\"rake build:all[sap-mcsec-compliance-dev,<current_release_version]\"\n\"rake push:all[sap-mcsec-compliance-dev,<current_release_version]\"\n\n\ncd cloud-compliance/profiles/sap-azure (directory)\n\"rake build:all[sap-mcsec-compliance-dev,<current_release_version]\"\n\"rake push:all[sap-mcsec-compliance-dev,<current_release_version]\"\n\n\ncd cloud-compliance/profiles/sap-gcp (directory)\n\"rake build:all[sap-mcsec-compliance-dev,<current_release_version]\"\n\"rake push:all[sap-mcsec-compliance-dev,<current_release_version]\"\n\n\n\n\nConnect to the cluster:\n\"gcloud container clusters get-credentials inspec-cluster --region europe-west4 --project sap-mcsec-compliance-dev\"\n\n\nDeploying consumers:\ncd cloud-compliance/infra/helm/sap-inspec-consumer (directory)\nkubectl config current-context -> This is used to verify if we are poiniting to correct cluster\nkubectl config use-context <'name of context you need to switch to'> -> Used to switch context to Dev cluster\nhelm list -> This will display list of all the current deployments.\nrake 'deploy[dev,aws]'\nrake 'deploy[dev,azure]'\nrake 'deploy[dev,gcp]'\n\n\nGo to the console -> clusters -> workloads: verify if all the deployed pods are up and running\n\n\nTo verify if deployment is successful ( This would be checked after the cron job is triggered)\nGo to 'aws-scheduler-sub' subscription and verify if the graph raises and then falls eventually (this will verify its is able to workaway itself with the queue)\nGo to 'azure-scheduler-sub' subscription and verify if the graph raises and then falls eventually (this will verify its is able to workaway itself with the queue)\nGo to 'gcp-scheduler-sub' subscription and verify if the graph raises and then falls eventually (this will verify its is able to workaway itself with the queue)\nGo to 'report-egress-sub' and verify if the graph is going up (this would verify its getting to the end of the queue)\n\n\n\n\nAuthenticate and switch to 'sap-mcsec-inspec-preprod' GCP project\n\"gcloud auth login\"\n\n\nAuthenticate Docker with the project in order to push Docker images\n\"gcloud auth application-default login\"\n\"gcloud auth configure-docker\"\n\n\nBuild and Push Docker Images:\ncd cloud-compliance/profiles/sap-aws (directory)\n\"rake build:all[sap-mcsec-inspec-preprod,<current_release_version]\"\n\"rake push:all[sap-mcsec-inspec-preprod,<current_release_version]\"\n\n\ncd cloud-compliance/profiles/sap-azure (directory)\n\"rake build:all[sap-mcsec-inspec-preprod,<current_release_version]\"\n\"rake push:all[sap-mcsec-inspec-preprod,<current_release_version]\"\n\n\ncd cloud-compliance/profiles/sap-gcp (directory)\n\"rake build:all[sap-mcsec-inspec-preprod,<current_release_version]\"\n\"rake push:all[sap-mcsec-inspec-preprod,<current_release_version]\"\n\n\n\n\nConnect to the cluster:\n\"gcloud container clusters get-credentials inspec-cluster --region europe-west4 --project sap-mcsec-inspec-preprod\"\n\n\nDeploying consumers:\ncd cloud-compliance/infra/helm/sap-inspec-consumer (directory)\nkubectl config current-context -> This is used to verify if we are poiniting to correct cluster\nkubectl config use-context <'name of context you need to switch to'> -> Used to switch context to Pre-prod cluster\nhelm list -> This will display list of all the current deployments.\nrake 'deploy[preprod,aws]'\nrake 'deploy[preprod,azure]'\nrake 'deploy[preprod,gcp]'\n\n\nGo to the console -> clusters -> workloads: verify if all the deployed pods are up and running\n\n\nTo verify if deployment is successful ( This would be checked after the cron job is triggered)\nGo to 'aws-scheduler-sub' subscription and verify if the graph raises and then falls eventually (this will verify its is able to workaway itself with the queue)\nGo to 'azure-scheduler-sub' subscription and verify if the graph raises and then falls eventually (this will verify its is able to workaway itself with the queue)\nGo to 'gcp-scheduler-sub' subscription and verify if the graph raises and then falls eventually (this will verify its is able to workaway itself with the queue)\nGo to 'report-egress-sub' and verify if the graph is going up (this would verify its getting to the end of the queue)\n\n\n\n\nAuthenticate with 'sap-mcsec-inspec-prod' GCP project\n\"gcloud auth login\"\n\n\nAuthenticate Docker with the project in order to push Docker images\n\"gcloud auth application-default login\"\n\"gcloud auth configure-docker\"\n\n\nBuild and Push Docker Images:\ncd cloud-compliance/profiles/sap-aws (directory)\n\"rake build:all[sap-mcsec-inspec-prod,<current_release_version]\"\n\"rake push:all[sap-mcsec-inspec-prod,<current_release_version]\"\n\n\ncd cloud-compliance/profiles/sap-azure (directory)\n\"rake build:all[sap-mcsec-inspec-prod,<current_release_version]\"\n\"rake push:all[sap-mcsec-inspec-prod,<current_release_version]\"\n\n\ncd cloud-compliance/profiles/sap-gcp (directory)\n\"rake build:all[sap-mcsec-inspec-prod,<current_release_version]\"\n\"rake push:all[sap-mcsec-inspec-prod,<current_release_version]\"\n\n\n\n\nConnect to the cluster:\n\"gcloud container clusters get-credentials inspec-cluster --region europe-west4 --project sap-mcsec-inspec-prod\"\n\n\nDeploying consumers:\ncd cloud-compliance/infra/helm/sap-inspec-consumer (directory)\nkubectl config current-context -> This is used to verify if we are poiniting to correct cluster\nkubectl config use-context <'name of context you need to switch to'> -> Used to switch context to Prod cluster\nhelm list -> This will display list of all the current deployments.\nrake 'deploy[prod,aws]'\nrake 'deploy[prod,azure]'\nrake 'deploy[prod,gcp]'\n\n\nGo to the console -> clusters -> workloads: verify if all the deployed pods are up and running\n\n\nTo verify if deployment is successful ( This would be checked after the cron job is triggered)\nGo to 'aws-scheduler-sub' subscription and verify if the graph raises and then falls eventually (this will verify its is able to workaway itself with the queue)\nGo to 'azure-scheduler-sub' subscription and verify if the graph raises and then falls eventually (this will verify its is able to workaway itself with the queue)\nGo to 'gcp-scheduler-sub' subscription and verify if the graph raises and then falls eventually (this will verify its is able to workaway itself with the queue)\nGo to 'report-egress-sub' and verify if the graph is going up (this would verify its getting to the end of the queue)"}},"/internal/ARCHIVED/Minerva_docs/release_process/orca_release_workflow":{"title":"[ARCHIVE] Internal Orca controls release workflow","data":{"":"","table-of-contents#Table of Contents:":"Introduction\n1.1 WHAT?\n1.2 WHY?\n1.3 WHO?\n1.4 HOW?\n\n\nTechnical implementation of Orca Release Process\n2.1 Git branching strategy\n2.2 Generalised release strategy\n2.2.1 Continuous release process\n2.2.2 Monthly standard release process\n2.2.3 Hotfix release process","1-introduction#1. Introduction:":"","11-what-#1.1 WHAT ?":"This document outlines the process for orca controls release management from a technical perspective. This will involve git workflows. The scope of this document will cover technical implementation for the release process designed in Orca release process.","12-why-#1.2 WHY ?":"There are lot components involved in release process. Hence, it is necessary to understand how everything fits with the process designed and technical git implementation. This in-depth release document will help spread necessary knowledge across the team and enable everyone in the team to conduct the release.","13-who-#1.3 WHO ?":"The target audience for this document is going to be internal GCS SRRC Hyperscaler security operations and engineering team.","14-how-#1.4 HOW ?":"The detailed release flow diagram is as follows:","2-technical-implementation-of-orca-release-process#2. Technical implementation of Orca Release Process:":"","21-git-branching-strategy#2.1 Git branching strategy":"There will be main branch and release branches\nThe release branches will have following naming convention: release-1.0.x\nAll the controls will be merged into respective release branches. This will depend on what label the control issue created has.\nAt a time there will be two active release branches\nFirst release branch will have controls deployed as informational\nActive controls development will be done in second release branch\n\n\nWhen release branch is merged in main, main branch will be merged in the second release branch so it has all the latest changes from main.\nThe release branches should always have all the changes from main i.e. release branch should be ahead of main","22-generalised-release-strategy#2.2 Generalised release strategy":"The overview of the release process is as follows:\n\nThere will be 3 release processes running in parallel depending on what updates are being made:\nContinuous Release Process\nMonthly Standard Release\nHotfix Release\n\nNote: All the controls to be released during a monthly standard release, a new release branch is created off main and the severity is changed from informational to High/Medium. If there are more controls developed than what is supposed to be in the release, a new release branch is created to include those additional controls.","221-continuous-release-process#2.2.1 Continuous release process":"This will consist of new controls requests from SGS that will be initially released as informational\nThese controls will be developed in a feature branch, this feature branch will be made out of main\nOnce the control development is done, the severity is set to 1.0\nPR is created to merge this PR directly into main\nOnce the PR is merged, the control will be deployed to production as informational","222-monthly-standard-release-process#2.2.2 Monthly standard release process":"New Controls\nThe list of controls to be released will be pre-decided\nThese new controls PRs are merged as follows:\nAll the controls to be included in the next release branch with SGS definition severity (high or medium)\nto next-next release branch with SGS definition severity (high or medium) if there are additional controls than to be included in the next release\nsteps 2. iterates (e.g. next-next-next ...) if previous next release already has the controls to be included\n\n\n\n\nBugfixes\nThis includes high controls with less than 20% false positive or medium controls with false positive PRs are merged to next release branch\n\n\nControl Updates\nexisting controls update PRs are merged to next release branch\ndeactivated controls PRs are merged to next release branch\nhigh controls with less than 20% false positive PRs are merged to next release branch\nmedium controls with false positive PRs are merged to next release branch\n\n\nNext release branch is merged to main every 4 weeks","223-hotfix-release-process#2.2.3 Hotfix release process":"high controls with more than 20% false positive PRs are merged to main\nno longer valid controls PRs are merged to main\nasset model change controls PRs are merged to main"}},"/internal/ARCHIVED/Minerva_docs/runbooks":{"title":"[ARCHIVE] L2 Minerva Runbooks","data":{"":"This is the L2 Runbook for the HS Compliance scanning tool Minerva. Minerva is a service that uses a scan engine (using Chef Inspec) to provide implementations of declarative security control compliance information for HyperScaler accounts within Multi Cloud.The runbook will cover the following sections:\nControl Explanation and Remediation: Overview of what cloud security controls Minerva scans for and the policies used to inform them. Remediation steps on how to fix security vulnerabilities found by Minerva scans.\nException Process & SEP Tickets: Process for requesting exceptions and SEP tickets for exceptions.\nMinerva FAQ: Common questions LOBs ask about Minerva, including where to find out more information about Minerva."}},"/internal/ARCHIVED/Minerva_docs/release_process/versioning_process":{"title":"[ARCHIVE] Versioning Process","data":{"":"In order to provide a valuable versioning workflow, given our current approach,\nwe have to consider different version definitions:\ncode-version: this version is the one mantained in the code for the\nspecific service, e.g. for consumer it is maintained in the inspec.yaml\nfile. This version specifies the status of the code and the associated\nimage version to be built\ndeploy-version: this is the appVersion value mantained in the helm chart\nand it is used to deploy the image version\nrunning-version: this is the currently running version of the service\nand it is retrievable in the GCP console under the relative GKE workload\nreleased-versions: this is a list of all versions officially released\n\nIn our current software lifecycle approach when a PR is merged, different\nprocesses maybe triggered depending on the combination of the status of the\nversions definitions above. For example a new image might be built or updated,\nand then deployed in a landscape. If the landscape is production, then a release\nis done. The following diagram illustrates in details how the defined versions\nabove interact with each other and can trigger different processes:Note: all services should follow this approach.Minerva release concerns consumers only. It adopts an alphabetical codename\napproach for versioning based on cities.Each codename release has associated the semver version of each consumer and the\nrelease date as following:\nName\tAlicloud version\tAWS version\tAzure version\tGCP version\tRelease date\tAtlanta\t4.2.0\t4.3.0\t4.2.0\t4.2.0\t2022-03-01\tBuenos Aires\t4.3.0\t4.4.0\t4.2.0\t4.3.0\t2022-04-01\tCairo\t4.4.0\t4.4.0\t4.3.0\t4.3.0\t2022-05-01","consumers-versioning#Consumers versioning":"Each consumer hyperscaler version is maintained in the relative inspec.yaml\nfile. This indicates the code-version and it's used to create the\ncorresponding image. Each deployed hyperscaler version is maintained in the helm\nvalues file.Please note that while normally the deploy-version is up to date with the\nlatest build, there might be times where this is not the case, especially if\nbetween the build of the new image and the deployment there is a considerable\ntime gap.","release-notes#Release notes":"The release notes contain the mapping of each official release with the\ncorresponding hyperscalers consumer version. Adhoc image users should refer to\nthe release notes to understand which image version they need to download.Helm charts maintain a chart version value which is used to keep track of\nchanges of the chart itself. Each time a change in the chart is made the version\nshould be increased accordingly."}},"/internal/ARCHIVED/Minerva_docs/runbooks/Exception-Process-SEP-Tickets":{"title":"[ARCHIVE] Exception Process & SEP Tickets","data":{"":"Introduction\nException Process\nRequesting an Exception\nChecking an Exception Request and SEP Ticket","introduction#Introduction":"SAP uses so called “Detective Controls” to monitor the security configuration of ALL SAP HyperScaler Accounts (e.g. Aws, GCP, Microsoft Azure & Alibaba Cloud) towards the SAP HyperScaler Security Reference Architecture and configuration standards. These security configuration requirements are defined by SAP Global Security and published in the Security Policy Framework / Wiki. While Minerva scans for these Detective Controls, LOBs can request Exceptions so they are excluded from certain detective controls.","exception-process#Exception Process":"The HyperScaler Exception Process is determined by SGS and describes the criteria for exceptions. If a SNOW ticket requests information on the Exception Process, please sign post the user to the SGS HyperScaler Exception Process and SGS Security Exception Management Process.","requesting-an-exception#Requesting an Exception":"If an LOB has a SNOW ticket asking for an exception, refer them to the Exception Process.\nIf the LOB needs to apply for a security exception, the exception MUST be submitted in the SAP Global Security(SGS) -Security Exception Management Process (SEMP).Exception Request{: .btn .btn-purple .mx-auto }","checking-an-exception-request-and-sep-ticket#Checking an Exception Request and SEP Ticket":"If a user is asking about an existing exception request they have put in, they should have an SEP ticket reference. This will normally have a format of SEP-(Number) e.g. SEP-000. If the SEP reference is not in the SNOW ticket, please ask the user for it.To check if a user has a valid SEP exception ticket, please log in to the Security Jira and enter the SEP reference in to the search box in the upper right corner.If a user does not have a ticket but feels they need an Exception, refer to Requesting an Exception."}},"/internal/ARCHIVED/Minerva_docs/runbooks/Control-Explanation-and-Remediation":{"title":"[ARCHIVE] Control Explanation and Remediation Steps","data":{"":"Introduction\nHyperScaler Detective Controls and Remediations\nSGS Policy","introduction#Introduction":"This part of the runbook will provide links to the detective controls Minerva has implemented. The links will provide a high level overview of the controls for each HyperScaler. These links list the current controls, give a high level overview of what the control does, and also provide remediation steps.","hyperscaler-detective-controls-and-remediations#HyperScaler Detective Controls and Remediations":"If a user asks for information about a particular control and how to fix it, make sure the control reference is in the SNOW ticket. The format will be HyperScaler + Policy Number + Control title. For example:AWS 4.1 - AWS S3 buckets do not have server side encryptionAzure 3.1 - Export Activity LogOnce the control is in the SNOW ticket, signpost the user to the correct HyperScaler Control and remediation steps.\nAliCloud Controls and Remediation\nAWS Controls and Remediation\nAzure Controls and Remediations\nGCP Controls and Remediations","sgs-policy#SGS Policy":"SGS HyperScaler hardening policy is what informs the detective controls Minerva uses. SGS hardening policies for each HyperScaler can be found in the following locations:AliCloud SGS Hardening Policy\nAWS SGS Hardening Policy\nAzure SGS Hardening Policy\nGCP SGS Hardening PolicyIf a user wishes to raise a question about an SGS policy, they should contact SGS directly. The user should be signposted to the SGS Security Procedures, Good Practices and Prod. Standard Sec. Requirement feedback form to raise a policy question, by selecting Hardening Procedures from Document Category, them the HyperScaler (for example, Amazon Web Services) under Document Name."}},"/internal/dev_workflow":{"title":"Dev WorkFlow","data":{"":""}},"/external/compliance_scanning/orca_azure_controls":{"title":"Orca Azure Control Details","data":{"":"","how-to-use-this-document#How to Use This Document":"Below you will find detailed information for each released Orca Azure control. Remediations for the controls are found below using manual steps (console-based) and automated steps (Terraform-based). Follow the manual steps if you do not use Terraform to manage your resources, or add the Terraform configurations to your Terraform modules. Please thoroughly test and adapt the Terraform configurations as needed.","control-severity-details#Control Severity details:":"Following are control severities:\n8.0 represents HIGH/MUST severity controls\n5.0 represents MEDIUM/SHOULD severity controls\n1.0 represents informational controls","note#Note:":"The informational controls will not be reported on. These controls will eventually be release as high or medium in coming releases."}},"/internal/ARCHIVED/Minerva_docs/runbooks/Minerva-FAQ":{"title":"[ARCHIVE] Minerva FAQ","data":{"":"Introduction\nWhere do users find the latest release information on Minerva?\nIf a user wants to understand Minerva or raise feature requests how can they do that?\nIf a user wants the weekly summary of Minerva findings how can they get that?\nI am a L2 Engineer and I have a suggestion or question relating to these runbooks so how do I contact the HS SecDevOps team about this?","introduction#Introduction":"This section relates to common queries that LOBs might ask in tickets, or that provide additional information that might be useful.","where-do-users-find-the-latest-release-information-on-minerva#Where do users find the latest release information on Minerva?":"This can be found under the Minerva Release Notes, which contains the version notes and what changes have been made in the release.","if-a-user-wants-to-understand-minerva-or-raise-feature-requests-how-can-they-do-that#If a user wants to understand Minerva or raise feature requests how can they do that?":"The user should attend the HS SecDevOps team's weekly HyperScaler Security Office hours meetings to hear about Minerva and raise any questions or requests they have there. To join this meeting please see details on the HyperScaler Security Office Hours meeting JAM page.","if-a-user-wants-the-weekly-summary-of-minerva-findings-how-can-they-get-that#If a user wants the weekly summary of Minerva findings how can they get that?":"The user should subscribe to the Secure Cloud Delivery 2 -extended team distribution list.","i-am-a-l2-engineer-and-i-have-a-suggestion-or-question-relating-to-these-runbooks-so-how-do-i-contact-the-hs-secdevops-team-about-this#I am a L2 Engineer and I have a suggestion or question relating to these runbooks so how do I contact the HS SecDevOps team about this?":"The best way to reach the HS SecDevOps team for runbook feedback is on the L2/L3 Sec Ops slack channel: #mc-security-l2-l3-support"}},"/internal/dev_workflow/change-management":{"title":"Change Management process","data":{"":"","process-context#Process context":"This Change Management process is based on the Hyperscaler Change\nManagement process, which can be found\nhere. However,\nit is not the same as some modifications have been applied to make it more\nrelevant for the DevSecOps team.","definitions#Definitions":"A change is defined as any modification on hardware, software or objects\nwithin a cloud project. A Change Management (CM) process aims to understand\nand minimize risks of doing changes to an IT productive system. Therefore,\nappropriate plan, review and approval must be in place.Each change needs to be:\ndocumented\nclassified and prioritized\nrisk assessed\napproved\nnotified to relevant parties where relevant","change-classification#Change classification":"In terms of change classifications, there are 3 classes of changes:\nStandard: this is a low-risk change that can be done quickly and has low\nimpact on the IT system\nImportant: this is a change that impacts the running system and takes\ndedicated time in order to be implemented, e.g., updating a service, a\ncluster, applying a security patch. This type of change needs to be planned\nappropriately.\nUrgent: this change concerns security or business critical changes, for\nexample because of a P1 or P2 incident ticket. This only applies if an\nimmediate action is required, otherwise an Important change should be\nraised instead. In case of P1 incidents, a problem management ticket must be\nraised.","risk-classification#Risk classification":"As for risk classification, there are 3 classes of risks:\nLow: this is a change that has low impact on the system, normally a minor\nchange, e.g., fixing a bug, or updating a configuration\nMinor: this change can potentially impact the running system, e.g.,\ndeploying a new feature, in which case it is required to:\ncommunicate to relevant parties (if applicable)\ncreate a rollback plan (if applicable)\n\n\nHigh: this change has significant impact on the running system and highly\naffects business operations, for example in case of a significant change to a\ncore service. In this case it is required to:\ncommunicate to relevant parties (if applicable)\ncreate a rollback plan","risk-score#Risk score":"The table below allows to quantify a risk score which will determine the risk\ntype for the change.\nDescription\tValue/Quantity\tScore\tOutage time\t0 mins\t0\tOutage time\t0 to 15 mins\t3\tOutage time\t15+ mins\t6\tAffected user base\tNone\t0\tAffected user base\tKibana/API users\t1\tAffected user base\tSingle Cloud\t3\tAffected user base\t2 or more Clouds\t6\tService modification (pod, OS, etc.)\tper component\t2\tCritical service modification (K8s, ELK, etc.)\tper component\t5\tChange time (to complete the change)\t0 to 30 mins\t0\tChange time (to complete the change)\t30+ mins\t3\tRollback time\t0 to 30 mins\t0\tRollback time\t30+ mins\t3","documenting-a-change#Documenting a change":"In order to document a change a github ticket needs to be created using the\nCM template in zenhub. The following fields as well as the risk assessment need\nto be filled appropriately:\nDescription:\nJustification:\nValidity of change (if applicable):","risk-assessment#Risk assessment":"Normally CM tickets are created by admins as normal work routine on production\nsystem. In this case they need to make the risk assessment described below. In\nthe rare case that a CM ticket is created by a non-admin person, the approver\nneeds to make the risk assessment.The risk assessment is done by calculating the associated risk score, after\nfilling the following fields in the ticket description:\nEstimated Outage:\nAffected user base:\nService(s) modifications:\nChange time:\nRollback time:\nTotal score:\nOnce the risk assessment is done, the risk can be classified as:\nScore\tRisk\t0 to 4\tLow\t4 to 9\tMedium\t10+\tHigh","plan-and-execution#Plan and execution":"After the risk being classified a task can be scheduled according to the\nnecessary time needed to be carried out and its urgency. A plan can also be\nworked out depending on the risk class of the change, e.g.:\nSchedule a time to carry out the change\nInform relevant parties (if applicable)\nCreate a detailed rollback plan\nCreate or check backups (if necessary)\n\nThe plan and any relevant information should be recorded in the CM ticket,\ni.e., adding comments as necessary.","change-management-approvers#Change management approvers":"Following is a list of approvers for change management requests:\nName\tI-number\tTime zone\tRole\tSystem\tAndrea Edwards\tI866863\tCentral Standard Time\tAdmin\tELK\tJustin Nikles\tI506539\tEastern Standard Time\tAdmin\tELK, GCP\tCarmelo Ragusa\tI069103\tGreenwich Mean Time\tAdmin\tELK, GCP\tRohit Prasad Joshi\tI539556\tPacific Standard Time\tAdmin\tGCP\tJames Yan\tI355896\tSingapore Time\tAdmin\tGCP\tJatin Rajpura\tI500686\tChina Standard Time\tAdmin\tGCP Singapore\nEach request needs 2 approvals."}},"/internal/dev_workflow/controls_development":{"title":"Controls Development with Chef Inspec","data":{"":"This document describes the overall Controls development process.\nThe examples and the commands documented below are specific to AWS profile however, similar process can be followed for Azure, GCP and Alicloud as well since the profile structure for AWS, Azure, GCP and Alicloud hyperscalers are all the same.\nTerraform tests:\nThe terraform tests include good tests, bad tests and tests required to cover edge cases for a control.\nIn good tests resources are deployed in the way that ideally the control should pass that test. In bad tests resources are deployed in the way that ideally the control should fail that test.\nThe terraform tests in the repository are structured in modules:\nCreate relevant resource in any of the existing modules or if a module does not exist create a new one.\nThe attribute values for the resource in a module are parameterised in the form of variables in order for its reusability. The value for these variables are then added in good terraform main.tf and bad terraform main.tf depending whether it is a good or a bad resource.\n\n\n\n\nCookstyle Linting:\nCookstyle linting is an enforced check as part of the Pull Request validation automation. PR's will be automatically checked for Cookstyle violations.\nTo run this locally, run Cookstyle . when in the profile directory, or run cookstyle -D --format offenses for a nicer formatted output.\nCookstyle can also automatically fix a lot of linting issues (if not, it will guide you how to fix them) by running cookstyle -A\n\n\n\n\nStep 1: Set environment variables to write terraform tests and run Chef Inspec:\nRelevant environment variables must be set depending on for which hyperscaler control development is done.\n\n\nAWS\nAWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, AWS_DEFAULT_REGION, AWS_REGION, AWS_AVAILABILITY_ZONE, AWS_MASTER_ACCOUNT_ID\nAWS_DEFAULT_REGION = us-east-1\nAWS_REGION = us-east-1\nAWS_AVAILABILITY_ZONE = us-east-1a\n\n\nAzure\nARM_CLIENT_ID, ARM_CLIENT_SECRET, ARM_SUBSCRIPTION_ID, ARM_TENANT_ID, AZURE_SUBSCRIPTION_ID, AZURE_CLIENT_ID, AZURE_TENANT_ID, AZURE_CLIENT_SECRET,  ARM_ORG_ID\n\n\nGCP\nGOOGLE_APPLICATION_CREDENTIALS, GCP_PROJECT_ID, GCP_ORG_ID, TF_VAR_GOOGLE_PROJECT\nGCP_PROJECT_ID = sap-mcsec-inspec-test2\nTF_VAR_GOOGLE_PROJECT = sap-mcsec-inspec-test2\n\n\n\n\nStep 2: Create Good/Bad Terraform tests\nDeploying a specific module in good terraform (similar process for bad terraform)\n- cd profiles/sap-aws/test/integration/good-terraform\n- terraform init\n- terraform plan -target=module.<module_name>\n- terraform apply -target=module.<module_name>\n\nOnce testing is done Do not forget to destroy the resources\nTo destroy resources:\n- cd profiles/sap-aws\n- terraform destroy\n\n\n\n\n\nStep 3: Writing/Testing Chef Inspec control\nThe chef inspec controls are present in following directory of the repo: profiles/sap-aws/controls\nThere is a wiki tag (eg: SAP: 1.60.05) associated with each of the control. Each control is present in a file with .rb extension named under this wiki tag  (eg: sap_1_60_05).\nThis Wiki tag can be found in the control tracking sheet.\nFor new control, if a file with that tag already exist, it will be written in the same file. Ff a file with that tag does not exist a new file is created in the same format and control is added in that file.\nControl components:\nThe name of the control should be in following format: 'sap_<wiki_tag>_<small_description_of_the_control>'. One example would be 'sap_1_50_05_s3_bucket_public'\nThe title can be taken directly from the controls tracking sheet\nThe description has to be formulated with reference from SGS Wiki chapter\ntag sgs_wiki_url: SGS Wiki chapter link has to be added\n\n\nTesting Controls: Once control has been written\nMethod 1: Testing in current repository\nRun a specific control:\nAWS\n- cd profiles/sap-aws\n- inspec exec . -t aws:// --input aws_master_account_id=<enter_master_account_id> --controls <enter_your_control_name>\n\nAzure\n- cd profiles/sap-azure\n- inspec exec . -t azure:// --controls <enter_your_control_name>\n\nGCP\n- cd profiles/sap-gcp\n- inspec exec . -t gcp:// --input gcp_project_id=sap-mcsec-inspec-test2 gcp_organization_id=<enter_gcp_org_id> --controls <enter_your_control_name>\n\n\n\nRun all the controls:\nAWS\n- cd profiles/sap-aws\n- inspec exec . -t aws:// --input aws_master_account_id=<enter_master_account_id>\n\nAzure\n- cd profiles/sap-azure\n- inspec exec . -t azure://\n\nGCP\n- cd profiles/sap-gcp\n- inspec exec . -t gcp:// --input gcp_project_id=sap-mcsec-inspec-test2 gcp_organization_id=<enter_gcp_org_id>\n\n\n\n\n\nMethod 2: Testing using Inspec Shell\nClone the AWS Inspec resource pack (AWS Resource Pack)\nNavigate to base of the repo:\n- cd inspec-aws\n- inspec shell -t aws:// --depends .\n\nThis will open up an inspec shell where you can run the entire control or even 'cd' into a resource and test the attributes individually . eg: To test attributes for a s3 bucket\n- cd aws_s3_bucket(bucket_name: 'bucket_name')\n- ls\n\n\n\n\n\n\n\nStep 4: Running Integration Test suite\nOnce all the terraform tests are written, control is developed and it is tested locally next step is to run the test suite:\nFollow this process:\n- cd profiles/sap-aws\n- rake test:update_expected_results\n\nThe test suite run is automated and will do the following:\nInitialize good terraform\nInitialize bad terraform\nCreate good terraform resources\nRun inspec tests against those resources\nDestroy good terraform resources\nCreate bad terraform resources\nRun inspec tests against those resources\nDestroy bad terraform resources\nRender results of the run in 'expected_results.json' file. This file can be found here: /profiles/sap-aws/test/results\nThis file contains list of all the controls and their expected good configuration and bad configuration results.\nIt is necesary to verify if only that new control which was developed is added into this file with its expected results. (Ideally expected results should be good configuration passing and bad configuration failing)\n\n\n\n\n\n\nStep 5: Building docker images\nThere are two docker images whose built needs to be verified as a part of control development.\nTo verify this follow this process:\n- cd profiles/sap-aws\n- rake build:all[sap-mcsec-compliance-dev,3.0.0-alpha.1]\n\nThis will build PubSub and ad-hoc docker images which should be successful.\n\n\nStep 6: Creating a PR\nOnce all the steps have been followed, a PR is created to merge into 'develop' branch.\nThe PR should consist screenshots for good and bad tests and a small description regarding the control and the test cases covered.\n\n\n\n\nCreate a new feature branch from develop.\nWrite Good/Bad terraform tests.\nDevelop a control.\nTest the control locally using any of the 2 methods mentioned above.\nRun the test suite and verify just that new control is added in expected_results file and rest of the results for all other controls are the same.\nVerify Docker images are built successfully.\n\n\nAWS Resource Pack\nAzure Resource Pack\nGCP Resource Pack"}},"/internal/dev_workflow/git-guide":{"title":"Git Guide","data":{"":"This guide provides some suggestion on how to use git for a normal development\nprocess. However, please consider that this is not the only way to do this and\nalternative ways are also valid. Also, only cli examples are used in this\nguide. If you are using a different client, you will need to find the\ncorrespondent way to perform each operation.  Finally, this guide is not about\nexplaining the basic git commands and terminology for which you are expected to\nhave some knowledge. When not sure always refer to the official documentation.","standard-workflow#Standard workflow":"The develop branch is the main developing branch from which one normally\ncreates a new branch from to work on a new feature or a bugfix.A naming convention is suggested to be adopted in the team to use a prefix for\nthe branch name:\nfeature: for any new feature or updating an existing one\nbugfix: for any bugfix\nhotfix: for any hotfix which is bug that affects released code running in\nproduction and needs fixed ASAP\n\nUsing those prefixes branch names would be:\nfeature/super-feature\nbugfix/bad-bug\nhotfix/urgent-bug\n\nTo update your local develop branch with the remote develop, you first\nneed to make your local repo aware of changes in the remote repo:\ngit fetch --prune\nThen you can align your local develop with the remote one by doing a hard\nreset:\ngit checkout develop            # change your working branch to develop\ngit reset --hard origin/develop # reset your local develop to the remote one\nIf your local develop branch is up to date with the remote\norigin/develop, you can create your new branch as:\ngit checkout develop\ngit checkout -b feature/super-feature\nOnce you start working on your feature/bugfix branch and other developers work\nget merged to the remote develop branch, your local develop diverges\nfrom the remote and you need to realign yours. This can be done through a rebase\nof your local feature/bugfix branch to the develop branch on the remote\nrepo. Rememeber that to do a rebase your working branch needs to be your\nfeature/bugfix branch, which you can switch to with a checkout:\n# Only need the following command if your working branch is not the one you\n# need to do the rebase\ngit checkout feature/super-feature\n\ngit rebase --rebase-merges=no-rebase-cousins origin/develop\nThe first time that you push your local feature/bugfix to the remote repo you\nneed to create a remote branch with the same name as your local branch:\ngit push origin HEAD\nSubsequent updates can be pushed to your remote branch as:\ngit push --force-with-lease origin","the-meaningful-changes-approach-to-git-commits#The meaningful changes approach to git commits":"A PR should only have meaningful changes to the code that allows who ever looks\nat the git history to easyly see:\nThe state of the code before this PR and after the PR is merged\nThis means that the commits should reflect the logical separation of the changes\nbeing made. For example if the PR is changing a service and its deployment files\nit would be advisable to have two different commits, one for the service and the\nother one for the infrastructure deployment.  Also, the number of commits for\nchanges to the same file or group of logical files should be one. Creating\nadditional commits just because the same file(s) has been changed multiple times\nduring the development work, doesn't provide any help to whoever looks at the\ngit history or in case some fix needs to be performed by using the\nhistory. Remember what was stated above, only meaningful changes. There are\ntwo ways to achieve this.","soft-reset#Soft reset":"In this approach you continuosly create commits one after the other. Obviously\nthis piles up commits on top of each and defeats the meaningful changes\napproach. In this case doing a soft reset at the point before your work starts\nallows to drop all the commits, but keep all the changes. In practice after you\nperform this you will see all the files that you have changed as unstaged, which\nallow to create your clean commits following the meaningful changes\napproach. Below is an example:\n# Example of git history of your changes on top of the develop branch\n\n6f09df1 # change to file A\n51f5cdb # change to file B\n773fb95 # change to file B\n0467f10 # change to file A\nfa03972 # last change in develop\nTo do a soft reset at the last change in develop as follow:\ngit reset --soft fa03972\nAfter this apply the stage and commits commands shown above.","amend-and-fixup#Amend and fixup":"In this approach you create your clean history from the start and maintain it\nclean by updating the same commits. This depends on which commits you need to\nupdate, last one or one further down your history.To amend the last commit:\ngit add path/to/my/local/file         # Stage your change\ngit commit --amend                    # Amend the last commit\nUse a instant fixup to amend one of the commits in your branch that is older\nthan the last commit:\ngit add path/to/my/local/file         # Stage a fix\ngit commit --fixup=a0b1c2d3           # Perform the commit to fix broken a0b1c2d3\ngit rebase -i --autosquash a0b1c2d3~1 # Now merge fixup commit into broken commit","references#References":"Git docs https://git-scm.com/docs\nGit fixup\nhttps://stackoverflow.com/questions/3103589/how-can-i-easily-fixup-a-past-commit"}},"/internal/dev_workflow/developer-guide":{"title":"Developer Guide","data":{"":"This document aims to provide guidelines for developers in the compliance\ndevsecops team.","pr-merging-process#PR merging process":"As a CICD is enabled, there are some important things to consider in order to\nsave time and avoid issues.The CICD adds a status check on the develop branch so that each new PR\nbranch needs to be up to date before merging. This is to ensure that the branch\nhas been tested.Each time that a PR is created a CI pipeline run is started. This includes some\ntests such as hadolint tests and inspec control static checks. More will be\nadded as time passes, for example unit tests.Github has a status check enabled which allows a PR to be merged only if it is\nup to date with the develop branch, which means that the PR has to be\nrebased with develop. Please follow the git guidelines\nhere.\nDo not use the **update branch** button as this will merge the develop branch\ninto your PR. We are using a rebase strategy and this will cause issues.\nIt is advisable some level of coordination among developers to save time to\navoid trying to merge PRs at the same time.A dedicated slack #hs_devsecops_developer channel is to be used for such\ncoordination.Each time a PR is ready to be merged the responsible developer makes an\nannouncement in the channel that is about to merge his/her PR. This is\nequivalent to take a virtual token for the merging process. This means that the\nPR is already being rebased on develop and once the CI tests are completed\nand passed the PR can be merged.If in the mean time another developer with a PR ready to merge comes along,\nhe/she announce in the channel to be the next in line for merging his/her\nPR. The latter developer should wait to rebase the PR once the previous PR is\nmerged as the develop branch will be updated.Once the first developer has merged his/her PR, he/she must say so in the\nchannel so that the token is released and the next developer in the queue can\ntake it, which means rebase his/her PR, push it and a start a new CI run for the\nPR.As more developers come along the same process applies.","merging-strategy#Merging strategy":"In terms of merging strategy, github provides 3 different ways:\nCreate a merge commit: use this when there are more than 1 commit in the\nPR, as it creates an additional merge commit which wraps up all commits in the\nPR\nSquash and merge: don't use this, as all commits are squashed into one and\nyou loose the commits history of your PR\nRebase and merge: use this when there is a single commit in the PR, as\nadds a single commit to the PR, and there is no use of having a merge commit\nin addition\n\nOnce a PR is merged, the branch should be deleted in github using the button\navaialable.","daily-integration-tests#Daily integration tests":"As at the moment controls tests inlcude only integration tests that need real\nsystems and therefore use service accounts to perform actions in those systems,\nonly one run at the time can be started. A daily run of the CI perfoming such\nintegration tests is performed at 6AM UCT. Any possible integration issues on\nthe controls is reported in those runs and pushed to a dedicated slack\n#mc_devsecops_cicd_tests. Developers are encouraged to check the reported\nissues and create bugfix PRs to address them. These PRs will follow the exact\nsame process above."}},"/internal/dev_workflow/user-access-management-process":{"title":"User Access Management process","data":{"":"This document describes the User Access Management (UAM) process to handle\nuser access and revocation to the Security Cloud Compliance Scanning Framework\nproduction systems such as GCP sap-mcsec-inspec-prod project and ELK. This\nconcerns user onboarding and offboarding, as well as temporary permissions to\ncarry out a specific task.A UAM request falls under a Change Management (CM) request, for which the CM\nprocess must be followed. The majority of UAM requests\nshould be a standard low risk change. However, for users offboarding the CM\nticket should have a HIGH priority.The following roles have been identified:\nRole\tSystem\tPermissions\tUsage\tOwner\tGCP\tAll\tDon't miss important notifications\tAdmin\tGCP, ELK\tRead and write on all resources\tAdministration and management of the system\tUAM admin\tGCP, ELK\tRead and write of users\tAdministration and management of users\tEditor\tELK\tRead and write of dashboard resources\tAdministration and management of dashboards\tViewer\tGCP, ELK\tRead resources\t- team members and sgs people to see kibana dashboards- team members when on patch management duty to check GCP production system status\nDuring onboarding the standard role should be viewer for\nmost users. If a user needs temporarily elevated permissions to perform some\nspecific task, a justification must be provided in the ticket request.A zenhub ticket using the CM template is used to create UAM tickets. As UAM is\njust a special case of Change Management, the ticket creator needs to add UAM\nin the ticket title prefix.Note: as zenhub is only available to the SecOps team, tikets from external\nrequests can only be added by a team member.","offboarding-process#Offboarding Process":"After a team member has left the team, a UAM ticket should be created to offboard the user from all necessary team accounts and communications.Offboarding should include:\nRemoval from any Hyperscaler or ECK accounts the team uses.\nRemoval from any pipeline access the team uses.\nRemoval from any mailing lists the team is part of.\nRemoval from any private team slack channels.\nNotify Github adminstrators the person is no longer in the team.\nKnowledge Transfer task should be completed.\nRemoval from any exisiting tasks.\nRemove user from any team documentation.","tru-or-security-officer-for-cloud-account-leaving#TRU or Security Officer for cloud account leaving":"Where the person leaving is a TRU or Security officer for a team account, it should be agreed before hand who is taking over as the TRU or Security Officer within the team and this should be updated in the UAM ticket.","knowledge-transfer#Knowledge transfer":"Defined areas of knowledge transfer should be included in the ticket, and be updated with progress before the team member leaves.","removing-users-from-existing-tasks#Removing users from existing tasks":"When removing a departing team member from a ZenHub task, make sure the task is still valid, and if so, is either placed in to backlog if it is low priority, or has another team member assigned to it.","uam-approvers#UAM approvers":"Following is a list of approvers for UAM requests. Normal approvals for UAM\ntickets are done by UAM admins. In exceptional cases (e.g. when a UAM admin is\nnot available) a normal Admin can also approve the request.Each request needs 2 approvals.\nName\tI-number\tTime zone\tRole\tSystem\tJohn Conway\tI552669\tGreenwich Mean Time\tUAM Admin\tELK\tJoshua Bowman\tI541354\tEastern Standard Time\tUAM Admin\tELK\tUsman Rajput\tI551248\tGreenwich Mean Time\tUAM Admin\tGCP\tJustin Nikles\tI506539\tEastern Standard Time\tUAM Admin\tGCP\tMaoliang Gu\tI561497\tChina Standard Time\tUAM Admin\tELK, GCP\t=============\t=========\t=====================\t===========\t==========\tAndrea Edwards\tI866863\tCentral Standard Time\tAdmin\tELK\tJustin Nikles\tI506539\tEastern Standard Time\tAdmin\tGCP\tCarmelo Ragusa\tI069103\tGreenwich Mean Time\tAdmin\tGCP, ELK\tRohit Prasad Joshi\tI539556\tPacific Standard Time\tAdmin\tGCP\tJames Yan\tI355896\tSingapore Time\tAdmin\tGCP\tJatin Rajpura\tI500686\tChina Standard Time\tAdmin\tGCP Singapore"}},"/internal/dev_workflow/gitflow_branching_strategy":{"title":"Gitflow Branching Strategy","data":{"":""}},"/external/hs_account_related/changeownership":{"title":"Change Ownership of a Hyperscaler Account","data":{"":"To change the ownership of any hyperscaler account, you need to open a ticket in Multi-Cloud's SNOW (Service Now Ticketing) system.We need an approval from the old and new owner, so I recommend the current owner to open this ticket.Go to Our Service Desk in ServiceNow and click on \"GCS Hyperscalers Management\"\n\nSelect \"Hyperscaler Account & User Management\"\n\nSelect \"GCS - Hyperscaler - Manage Users\"\n\nPlease select which Hyperscaler holds the account/subscription/project in question:\n\nOn the next line, select the change in ownership of the account in question:\n\nNow fill in the needed informations.If you are NOT the actual owner, please provide an approval from the current owner. If the current owner is not available anymore (left the company) please state this in the details and provide an approval from the responsible manager.\n\nDon't forget to upload the approval (if needed):\n\nThe change should be done in about 24 to 48 hours."}},"/internal/ARCHIVED/Dev_Workflow/Patch-Tasks":{"title":"[ARCHIVE] Patch Tasks","data":{"":"This document breaks down the tasks to be carried out by the patch Duty Manager. Unless otherwise stated, patch tasks should be carried out daily.\nThe Duty Manager for a given period will be in line with the Pager Duty Rota i.e. the person who is the Primary that week on Pager Duty will be the patch Duty Manager that week too.\nMore information on the general Patch Management Process for the team can be found in the Patch Management Process document.\nUnless other stated, patch tasks should be carried out every week.The primary tasks of the patch Duty Manager are:\nCarry out patch reviews in line with guidance provided below.\nRaise any tickets in Zenhub and notify the team on the mc_devsecops_developer slack channel a ticket has been raised for a patch task. Please make sure any patch tickets raised are tagged with the patch label.\n\nNB Please note any changes required for our Production environment should adhere to our Change Management process","authenticating-in-to-devpre-prodprod#Authenticating in to Dev/Pre-Prod/Prod":"Assuming you already have the gcloud cli installed, you can authenticate in to Dev/Pre-Prod/Prod using the following gcloud commands from Terminal (Mac) or PowerShell (Windows):\ngcloud auth login\nYou can change your project to Dev/Pre-Prod/Prod by running:\ngcloud config set project PROJECT_ID","contents#Contents":"Cyber Defence CVE Bulletins\nGCR CleanUp\nGKE Update\nUAM review\nVersioning review","cyber-defense-cve-bulletins#Cyber Defense CVE Bulletins":"The Cyber Defense and Design team has a tool that provides security bulletins for various applications and services based on recent CVEs. As part of the patching duties, the patch Duty Manager should review any emails on a daily basis. The shared mailbox name is SAP GCS MC DSOPS. Please contact John Conway if you have access issues with this mailbox.Before raising a task, you should do some checks on our infrastructure to see if the security bulletin is applicable. Primarily, you should check our code repositories for:\nThe container images we use in our Dev/Pre-Prod/Prod environments.\nAny Docker or Rake files we use in our Dev/Pre-Prod/Prod environments.\nAny code versions for languages we use in our team (if applicable).","container-images#Container Images":"From the GCP console, go to GCR - Container Images - Inspec Only the 3 most recent versions of Inspec should be there. For reference, check the release notes to see the release history.","gke-updates#GKE Updates":"Run the GKE Update script in the gke-patch-check script in the Devsecops Utils repo. This will show what version of GKE our clusters have what the latest version available is. If the latest version and our installed version is out of sync, please create a ticket.","uam-review#UAM Review":"UAM review should be conducted every 90 days instead of each week. Currently UAM review is scheduled for the following dates:\n2nd August 2022\n31st October 2022\n29th January 2023\n\nPlease raise a UAM ticket at the start of each week for UAM review and notify the team in the team mc_devsecops_developer slack channel.","versioning-review#Versioning Review":"Versioning review relates to the software versions we use on our stack. The check for updates of vulnerable versions we may have is partially covered by the Cyber Defence CVE Bulletin process discussed above.The remainder of the check involves checking the mc_devsecops_cicd_notifications slack channel and reviewing Trivy detections for failed builds. Click on the build that failed in the slack channel, and it will bring you to our Azure DevOps pipeline where Trivy should identify any known versions of software that have security patches available. Where there are patches available, create a ticket."}},"/internal/onboarding":{"title":"Onboarding","data":{"":"","this-section-provides-documentation-and-steps-for-new-team-member-onboarding#This section provides documentation and steps for new team member onboarding.":""}},"/internal/operations/calm":{"title":"CALM","data":{"":"More details about CALM, the API and account management."}},"/internal/onboarding/onboarding":{"title":"Onboarding Steps","data":{"":"","gcs-srrc-hyperscalers-security-engineering--operations-hseo#GCS SRRC Hyperscalers Security Engineering & Operations (HSEO)":"The team is spread across the following SAP office locations:\nBelfast, Northern Ireland\nBellevue, United States\nDallas, United States\nShanghai, China\nSingapore, Singapore\nWalldorf, Germany","meet-the-team#Meet the Team":"US West\tUS South\tUK\tGermany\tSingapore\tChina\tCharles Garrido\tMitchell Rose\tJohn Conway\tLars Wendler\tDennis John\tMaoliang Gu\tRohit Prasad Joshi\t\tCarmelo Ragusa\t\tDominic Rong\tJatin Rajpura\tNoryuszalina Mohd Yusop\t\t\t\tJames Yan\t\t\t\t\t\tYeo Shun Wen","team-comms--slack-channels#Team Comms / Slack Channels":"Private Team Channels:\nhs_devsecops (Main Channel)\nhs_devsecops_apac (Regional)\nhs_devsecops_developer (PR Reviews)\nhs_devsecops_docs (Documentation)\nhs_devsecops_notify (Git Monitoring)\nhs_devsecops_random (Casual Discussion)\nhs_devsecops_release_updates (Specific to Releases)\nhs_devsecops_orca (Orca Discussion)\nhs_devsecops_tickets (ServiceNow Tickets)\nhs_orca_exception_logs (Orca Exceptions)\nhs-devops-support (DevOps Collaborative Channel)\nhs-engineering (Eng. Team)\nhs_orca_offboarder (Account Offboarder Service)\n\nPublic Team & Service Channels:\nsap-multicloud-security (Standard SAP wide communication channel for SAP Labs)","useful-urls#Useful URLs":"Cloud Account Lifecycle Management (CALM)\tDocumentation\tException CSV\tHyperdash\tIncident Response\tISO / SOC Audit\tOrca\tSGS\tSAP\tService Now\tTicket Template\tGitHub Pages\tSecurity Procedure Exception Handling (SPEH)\tOffice Hours View\tIncident Reporting\tGCP Compliance Reports Manager\tOrca\tAlicloud Hardening Guidelines\tCerberus\tHCSM / SNOW Support Jam Page\tHSDB\tConfluence\tSharepoint\tPersonal View\t\tCloud Service Provider Certification Sharepoint\tOrca GitHub\tAWS Hardening Guidelines\tBridge\tBulk / Single User Access Requests to SNOW\tAPI Reference\t\tSEMP\t\t\tMicrosoft Service Trust Portal\tGCS-Hyperscaler-Orca Service Request\tAzure Hardening Guidelines\tMulti-Cloud Splunk\tAdding yourself to an assignment group once you have SNOW access\t\t\tMVP Data\t\t\t\tOrca Teams Channel\tGCP Hardening Guidelines\tSAP Employee Directory\tAgent workspace reference sheet\t\t\t\t\t\t\tOrca Support Email\tHyperscaler Acceptable Use Policy (AUP)\t\tSNOW Terminology Overview\t\t\t\t\t\t\t\tHyperscaler Exception Handling Process\t\t\t\t\t\t\t\t\t\tHyperscaler Secure-by-Default Controls","vpn-agent#VPN Agent":"F5 Edge Client (SSO)\nIf you have IPv6 you will be always-on VPN via the M$ agent.","introduction-to-the-orca-team#Introduction to the Orca Team":"Colin Keogh   | Customer Success\nJordan Sander | Customer Success\nKamil Kedra   | Customer Success","new-user-hr-onboarding#New User HR onboarding":"Step 1 > Check your Locations Updated IT process.\nOnboarding PortalEquipment Checklist:\nLaptop\nDocking Station\nMonitor\nWeb Cam\nHeadphones","details#Details":"Additional software can be found running Software Center from your Windows Laptop","meeting-invites#Meeting Invites":"Stand-Up\nTeam Call\nCloud Security Office Hours (Optional)\nOrca & SAP Biweekly Sync (US/APAC)","cam-profiles#CAM Profiles":"Cloud Access Manager (CAM) is an internal access approval tool - read more here. The below CAM profiles have been set up to simplify access for our team. All profiles should only be granted to members of our team, with the exception of the profiles under the External Access section. The profiles in the External Access section can be granted to members of our team, as well as members of other teams as long as we are provided with a sufficient use case. Basic Access profiles can be granted to any user on our team. Advanced access profiles should only be granted to team members with a sufficient use-case, as these profiles give sensitive access to pre-production and production resources.","basic-access#Basic Access:":"MultiCloud_SecDevOps_Developer\nThis is the most basic level of access for someone on our team. Anyone who joins the team should request this profile. It provides the following:\nAccess to development / testing accounts\nBasic access to Azure DevOps\nViewer access on production accounts\n\n\nMulticloud_SecDevOps_DevDeployer\nProvides access to deploy resources to the development GCP account - sap-mcsec-compliance-dev.","advanced-access#Advanced Access:":"Multicloud_SecDevOps_PreprodDeployer\nProvides access to deploy to the pre-production environment for the following:\n(GCP) sap-mcsec-inspec-preprod\n\n\nMulticloud_SecDevOps_ProdDeployer\nProvides access to deploy to the production environment for the following:\n(GCP) sap-mcsec-orca-operations\n(GCP) sap-mcsec-inspec-prod\n\n\nMulticloud_SecDevOps_SecretRotation\nProvides access to rotate secrets for the following:\n(GCP) sap-mcsec-orca-operations, role=Secret Manager Admin\n\n\nGCS_HypEng_SecDevOps_OrcaExceptionPublisherProvides access to publish the messages to the GCP Pub/Sub \"orca-publish-exceptions-subscription-prod\" in (GCP) sap-mcsec-orca-operations project.\nMulticloud_SecDevOps_OrcaOperationsProvides troubleshooter access to Orca.\nMulticloud_SecDevOps_OrcaAdminProvides admin access to Orca.Note: This profile should not be requested for daily operations unless absolutely necessary.\nMulticloud_SecDevOps_OrcaOpsAdminProvides admin access to Orca operations GCP accounts:\nsap-mcsec-orca-operations\nsap-mcsec-orca-operations-dev\n\nNote: This profile should not be requested for daily operations unless absolutely necessary.\nMulticloud_SecDevOps_CICDAdmin\nThis profile provides administrator access to our azure devops organization - mcdevsecops. Additionally, Contributor access is granted on the sap-devsecops Azure subscription, which runs our CI/CD workloads.\nMulticloud_SecDevOps_IAMAdminProvides IAM access to assign roles to groups in our team's accounts, including production accounts.\nMulticloud_SecDevOps_FullProdAdmin\nProvides admin access to production, with the exception of IAM.\nMulticloud_SecDevOps_Owner\nProvides \"Owner\" access to all HS SecDevOps accounts which require an owner. This profile must only be granted to users who do not regularly use our accounts. These users will receive special account notifications. Only 2 users should be granted this profile.","external-access#External Access:":"Multicloud_SecDevOps_OrcaViewers\nProvides viewer access to all cloud accounts in Orca.\nMulticloud_SecDevOps_OrcaViewerNoAPI\nProvides viewer access to all cloud accounts in Orca without the ability to create API tokens.\nMulticloud_SecDevOps_OrcaSplunkUserProvides splunk integration access to Orca. This should only be granted to users performing splunk integration.","cloud-admin-account#Cloud Admin Account":"","background#Background":"In our team's day to day work, access to a wide range of cloud accounts is required. For example, if we receive a ticket which requires to investigate false alert or findings reported by LoBs on a cloud account, it is helpful to be able to investigate resources on that account without needing additional privileges. Another example may include troubleshooting compliance scan errors which occur on a specific account, or multiple accounts. There are different privileges that are required to have this sort of access on each cloud provider, but one thing remains the same - a cloud admin account should be used, which requires additional auditing and security steps.","provisioning-a-cloud-admin-account#Provisioning a Cloud Admin Account":"Pre-requisites:\nAcquire an RSA Soft Token\nAbility to login to the ALM Tool\n\n\n\nLogin to the ALM Tool and go to the \"Requests\" tab under \"Accounts\"\nClick on \"Create Request\"\nChoose workflow type \"Cloud Admin Account Template v1\"\nChoose your account name, following the convention <c/d/i user>c IE, if your I number is I123456, your cloud admin account will be I123456c\nFill out all required fields. Under the description, be sure to include the account use case, tasks that the account will enable, environments needed, and environment types needed. Important! You need to fill out the description under \"Account Configuration\" as well or else your application will still be considered being a DRAFT! If your ALM request is still in the DRAFT state, it is incomplete. You must fully complete the request and submit it for it to be reviewed. For example:\nAccount use case: -Team: GCS SRRC Hyperscaler Security Engineering & Operations led by John Conway -Task that the account will enable me to perform: Cloud Security Issue investigation on the Hyperscaler Security Engineering & Operations team -Environments needed: GCP, Azure, Alicloud, AWS cloud accounts -All environment types will be needed, (PROD, TEST, DEV, etc.)\n\nOnce the account is approved, move on to acquiring permissions","accessing-cloud-admin-credentials#Accessing Cloud Admin Credentials":"To access your Cloud Admin account credentials, log into Thycotic with your regular corporate account credentials. The RSA token will be required here.After logging in, search for your cloud admin account name under the \"All Secrets\" tab. Once you click on your account, you should be able to find your generated password under the \"General\" tab. When logging in to your account, your username/email will be in the format <cloud admin ID>@global.corp.sap , which is different from the main corporate accounts.","acquiring-permissions-for-cloud-admin-accounts#Acquiring Permissions for Cloud Admin Accounts":"The permissions required for each cloud provider are different, so follow along with the required section.","aws-and-azure#AWS and Azure":"Request this access package, created for our team's usage. Note: Make sure to login with your cloud admin credentials. Once this request is submitted, it will require approval by our team manager (or GCS SRRC manager in his absence) and then by GCS HSE Hyperscaler Identity Cloud Engineering. This access package needs to be periodically renewed. If unable to view the access package, contact \"DL GCS HSE Hyperscaler Identity Cloud Engineering\".Once access package is approved, you may access AWS/Azure with your cloud admin credentials at the following links:\nAWS Portal: To view other AWS accounts, switch role to \"MSCrossAccountReadOnlyRole\" and provide the AWS account number to view.\nAzure Portal: Switch between tenants as needed.\n\nIn addition, if you need to work on applying exceptions for Azure preventive controls, open a ServiceNow ticket request with the Azure team using the GCS - Hyperscaler - Additional Help request type and request for the Azure AD Privileged Identity Management \"Policy Contributor\" role for both Azure shared and corporate tenants.","gcp#GCP":"Request this profile in the CAM tool. Unlike Azure and AWS, this role will be applied directly to your regular corporate user. Since GCP requires a mail inbox for a user, we cannot use cloud admin accounts for GCP. Your regular corporate user should auto-populate when accessing the above link."}},"/internal/onboarding/required_logins":{"title":"Required Logins","data":{"":"HS DevSecOps Team members should have access to the following accounts:","devsecops-specific-accounts#DevSecOps specific accounts":"(*Region specific)\n*AliBaba Test Account Used for testing controls and other DevSecOps.\n*AWS Test Account Used for testing controls and other DevSecOps.\n*Azure Test Account Used for testing controls and other DevSecOps.\n*GCP Test Account Used for testing controls and other DevSecOps.\n*GCP HSE DevSecOps Developer Account Account used for the Cloud Infrastructure for the MCE DevSecOps services.\nServiceNow Ticketing account Ticketing system for HSE DevSecOps team.","sap-general-accounts#SAP General accounts":"Bridge Account Used for recording what SAP staff spent their time working on, needs to be up to date by the end of each month.\nSuccess Map Used primarily for all SAP Mandatory training courses."}},"/internal/operations/GCPKeyRotationDocumentation":{"title":"GCP Key Rotation","data":{"":"In Secret Manager, check the following to check the current version of\nthe keys in place:In sap-mcsec-inspec-prod project > Security > Secret Manager\n> inspec-gcp-credsIn sap-mcsec-inspec-preprod project > Security > Secret\nManager > inspec-gcp-credsIn sap-mcsec-compliance-dev project > Security > Secret\nManager > poc-chef-scan-gcp\n\nIn sap-mcsec-compliance-dev project > Security > Service\nAccounts > go to\ngcp-mc-chefcompute-dev@sap-mcsec-compliance-dev.iam.gserviceaccount.com\n> click on Keys > click on Add Key > Create New Key > open\nkey in text editing program of your choice > copy text\n\nLet's update the keys to the new version.For example, In sap-mcsec-compliance-dev project > Security >\nSecret Manager > poc-chef-scan-gcp > click on Versions >\nNew Version > paste text into secret value box and click on Add\nNew VersionIn VS Code, use the github extension and navigate to the cloud\ncompliance folder.Make sure you're logged into gcloud using the gcloud auth login command:\ngcloud auth login\n\n\nIn GCP, go to Kubernetes Engine > Clusters > click on inspec\ncluster (or whatever cluster you're working on) > click on Connect\n\nCopy the command line access command to run in VS Code.Example: gcloud container clusters get-credentials inspec-cluster\n--region europe-west4 --project sap-mcsec-compliance-devThe following is referring to gcp dev deployment and gcp dev dispatcher:In VS Code, navigate to infra/helm/sap-inspec-consumer.Use the following command: cd infra/helm/sap-inspec-consumer.Check helm via command > helm list to see what versions are listed.\nAfter confirming the versions, click on values.dev.yaml > under GCP\nConfiguration, change secretVersion to whatever the new version\nis.\n\nApply rake command >\nrake \\'deploy\\[dev,gcp\\]'\nIn VS Code, navigate to infra/helm/sap-inspec-dispatchers. Use the\nfollowing command: cd infra/helm/sap-inspec-dispatchersCheck helm via command to see what versions are listed >\nhelm list\nClick on values.dev.yaml > under GCP Configuration, change\nsecretVersion to whatever the new version is.Apply rake command >\n*rake \\'deploy\\[dev,gcp\\]\\'\nRun the following command to check your k8s\ncontexts >\nkubectl config get-contexts\nThen run the following command (edited to fit the situation/cluster) to make\nsure the next steps don't impact the wrong cluster >\nkubectl config delete-cluster\ngke_sap-mcsec-inspec-preprod_europe-west4_inspec-cluster\nLet's get the connect command for the next cluster.In GCP, go to Kubernetes Engine > Clusters > click on inspec\ncluster (for prod) (or whatever cluster you're working on) > click on\nConnectGet the command > gcloud container clusters get-credentials\ninspec-cluster --region europe-west4 --project sap-mcsec-inspec-prodCopy the command line access command to run in VS Code.In VS code, navigate to values.prod.yaml > under GCP Configuration,\nchange secretVersionIn the same path (sap-inspec-dispatchers) > run the rake command >\nrake \\'deploy\\[prod,gcp\\]\\'\nChange directories to sap-inspec -consumers, using the command: cd\ninfra/helm/sap-inspec-consumerRepeat the above steps for values.prod.yaml under sap-inspec-consumer.Run the rake command >\n*rake \\'deploy\\[prod,gcp\\]\\'\nRun the following command to check your k8s contexts >\nkubectl config get-contexts\nThen run the following command (edited to fit the situation/cluster) >\nkubectl config delete-cluster* (fill in the appropriate details)\n> Cluster > to make sure the next steps don't impact the wrong\nclusterRepeat the above steps for values.preprod.yaml (for\nsap-inspec-consumer and sap-inspec-dispatcher) as well."}},"/internal/operations/calm/api":{"title":"CALM Automation","data":{"":"","steps-to-automate-the-calm-process-and-hsdb-tipstricks#Steps to automate the CALM process and HSDB tips&tricks":"","hsdb-authentication#HSDB Authentication":"Get HSDB JWT (Hyperscaler DataBase/Json Web Token). This authenticates the\nfollowing requests to update the accounts' security attributes\nOfficial HSDB api ref\nSet your HSDB user password here\nHSDB password docs\nPOST https://db.multicloud.int.sap/jwt-token/\n\nHEADERS: {\n\"Content-Type\": \"application/json\"\n}\n\nBODY: {\n\"username\": \"<your i/d/c number>\",\n\"password\": \"\"\n}\n\nField\tExample\tComments\tusername\t\"i337717\"\tMust have logged into portal at least once\tpassword\t\tMust pass the HSDB password strength check","update-the-accounts-security-attributes#Update the accounts security attributes":"Official HSDB api ref\nPATCH https://db.multicloud.int.sap/scd/security-attributes/<HSDB ID>/\n\nHEADERS: {\n\"Authorization\": \"Bearer <jwt token>\",\n\"Content-Type\": \"application/json\"\n}\n\nBODY: {\n\"security_officer\": \"sap.employee01@sap.com\",\n\"environment\": \"DEV\",\n\"has_customer_data\": \"True\",\n\"has_personal_data\": \"True\",\n\"is_iso\": \"False\",\n\"is_soc\": \"False\",\n\"is_pci\": \"False\"\n}\n\nField\tExample\tComments\tsecurity_officer\t\"sap.user01@sap.com\"\tMust be active in SAP Corporate Active Directory\tenvironment\t\"PROD\" / \"QA\" / \"SANDBOX\" / \"DEV\" / \"LAB\"\tPick one. All environments are subject to the same security controls\thas_customer_data\t\"True\" / \"False\"\tPick one\thas_personal_data\t\"True\" / \"False\"\tPick one\tis_iso\t\"True\" / \"False\"\tPick one\tis_soc\t\"True\" / \"False\"\tPick one\tis_pci\t\"True\" / \"False\"\tPick one","renew-cloud-account-lease#Renew Cloud Account Lease":"Official HSDB api ref\nGET https://db.multicloud.int.sap/scd/security-attributes/<HSDB ID>/set_leased_until/"}},"/internal/operations/calm/cloud_account_lifecycle_management":{"title":"Cloud Account Lifecycle Management","data":{"":"","required-steps#Required Steps":"There are 2 key requirements in CALM:\nVerifying Security Attributes are correct\nExtending the Account Lease\n\nThese must be verified at least once every 6 months. This can be done on either the Hyperscaler portal or using the Hyperscaler api.\nYou will receive automated notification emails 30 days before the end of the 6 month cycle.Please note that a decay cycle will start if this information goes stale.","using-the-ui#Using the UI":"Go to the Hyperscaler portal account listing\nClick the lock icon in the account's row\n\nCheck all info is correct then hit confirm. This will do both: verify security attributes are correct and extend the account lease by 365 days","mass-updating-security-attributes#Mass Updating Security Attributes":"There's now the capability to mass update multiple accounts' security attributes at one time. For details, see the Mass Security Attribute Update Guide","using-the-api#Using the api":"Please find the api help here\nThere is also a sample python script here"}},"/internal/operations/chef_ops/GCP_Bucket_Overview":{"title":"GCP Bucket Overview","data":{"":"Below is a brief summary of production buckets at present. As any buckets are added or removed, this document should be updated.\nGCP Bucket Name\tDescription\taccount-config-terraform-prod\tThis bucket holds account configuration customizations and default settings.\telastic-data-archive-bucket\tThis bucket is where the data exports are stored. This is crucial for Prisma and Minerva exports for weekly reports that are sent to LOBs via email.\telastic_repository_dev2021\tThis bucket is used for elastic backup and recovery activities\teu.artifacts.sap-mcsec-inspec-prod.appspot\\com\tThis bucket is the container repository.\tmcsec-elastic-state-prod\tThis bucket is used for Terraform activities for Elastic cluster.\tmcsec-inspec-state-prod\tThis bucket is used for Terraform activities for Inspec cluster.\tsap-mcsec-inspec-prod-bucket-logging\tThis bucket is for access logging.\tsap-mcsec-inspec-prod-elastic-snapshots\tThis bucket is used for Elastic back up and recovery.\tsap-mcsec-inspec-prod.appspot\\com\tThis bucket was created for Fire Store POC.\tstaging.sap-mcsec-inspec-prod.appspot\\com\tThis bucket was created for Fire Store POC."}},"/internal/operations/chef_ops":{"title":"Chef Ops","data":{"":""}},"/internal/operations/chef_ops/Minerva_Azure_Key_Rotation":{"title":"Minerva Azure Key Rotation","data":{"":"This document outlines the steps and considerations needed for Azure Key rotation for the Minerva project. HS Azure team are available for any questions about key expiration for the accounts we use, but the onus is on the HS DevSecOps team to monitor our keys and rotate them regularly.","preflight-check#Preflight check":"You will need to have access to the master account for Azure that Minerva uses.","steps-for-key-rotation#Steps for key rotation.":"Log in to our Azure Master account. From the Azure console, go to Directories + subscriptions. There are 2 tenants we have that should both be listed here:\n\n\nSAP SE\nSAP Shared Tenant\n\nAssuming both tenants are there, go to Azure Active Directory next. From the left hand pane, select App Registrations. The app that is used for Minerva should be listed here and is called:\n\n\nazrspn-MC-ChefCompute\n\nUnder the Certificates & Secrets heading you can see if the key is set to expire or not. The setting should be Current with a green status indicator, but if key expiration is imminent, the status indicator will be amber or red.\nTo view expiration information about the key, click on azrspn-MC-ChefCompute and then click on Certificate & secrets from the left hand menu. This will show you details about the current key, including the expiration date. Click on New Secret to create a new key, and set it to expire in 3 months.\nLogin to the GCP account for each environment (e.g. dev, pre-prod and prod) and go to Security then Secret Manager. From the list of secrets, find:\n\n\nazure-inspec-client-secret\n\nClick on the above secret, got to Versions and you will see the versions of the keys. Add the new key created in Azure here. This will give you a new version number.\nYou will then need to update the consumer code with the new version number. To do this, from the cloud-compliance repo go to infra/helm/sap-inspec-consumer/values.(environment).yaml and under the update the Azure Configuration section update the mainSecretVersion value to whatever the latest version is you created in Part 6. Do the same for infra/helm/sap-inspec-dispatcher/values.(environment).yaml also under the Azure Configuration heading and update mainSecretVersion here too."}},"/internal/operations/cloud_service_provider_certifications":{"title":"Cloud Service Provider Certifications: Administrative Tasks","data":{"":"An additional JIRA request type is being added under Additional_Services/Security/Cloud_Service_Provider_Certification","overview#Overview":"To make things more organized, accessible and secure, we have migrated our Security compliance reports repository to our Hyperscaler DevSecOps TeamSharepoint. In order to obtain the Cloud Service Provider security compliance reports, please use the followinglink:\nCloud Service Provider Certifications\n\nNOTE: These documents are CONFIDENTIAL for Internal SAP employees and contractors only. They are NOT to be redistributed outside of SAP. These reports contain highly confidential cloud service providers'data. Violation of their security compliance report policies like unauthorized redistribution might be regarded as a breach of SAP's SLAs with them. In order to obtain more information users can be directed to our Multi-Cloud Operation's ServiceDesk portal (https://servicedesk.multicloud.int.sap) to put in a request. If necessary, they can also reach out through mail to any the following individuals from our team.","prerequisites-for-access#Prerequisites For Access":"Recently, SGS requested us to create a more formal format in Service Desk for requesting access to these reports:\nContent of the Request:\nCustomer Name\nOptional: Opportunity ID\nReason for Requesting\nSign NDA yes/no\nHyperscaler\nWhich report is requested?\n(Optional) - GSIM Ticket number (SGS CISA Ticket system)","user-guidelines#User Guidelines":"Reports are for strict internal use only!\nThe User Instructions are general reports usage guidance based on our SLA/NDA with public cloud providers.\nDetails on how to access certain reports (GCP are password protected) are provided within the GCP report's directory. These reports are part of security compliance, so everyone should be able to access it on purpose.\nAll the cloud provider relevant reports and bridge letters are collected as per their availability.\nOur team tracks these reports on requests. We organize them for tracking access and responsibly providing easy access to information. Nothing stringent!","updating--obtaining-reports#Updating / Obtaining Reports":"","gcp#GCP":"GCP audit reports can be found on their Compliance Offerings page or their Compliance Reports Manager. If not listed you will have to open a support case, contact the team sales representitive and/or reach out to the GCP auditing team.","aws#AWS":"AWS artifacts can be searched on console.aws.amazon.com/artifact/reports.","azure#Azure":"Azure reports are easily found on the Service Trust Portal."}},"/internal/operations/docs_howto/How_to_Write_and_Post_Documentation_to_our_Github_Pages":{"title":"How to Write and Post Documentation to our GitHub Pages","data":{"":"","in-order-to-create-write-and-publish-documentation-for-our-github-pages-the-following-items-are-required#In order to create, write and publish documentation for our GitHub Pages, the following items are required:":"#1 - Clone the DevOps Docs Repo (https://github.tools.sap/mce/devsecops-docs).#2 - Run the Doc Helper Script (found in the DevOps Docs Repo, in the scripts folder), which will create the markdown file for you. Run the script from/in the root directory of the cloned DevOps Docs Repo. For those using Mac computers/laptops, you can open your Terminal, cd to the root directory of the cloned DevOps Docs Repo and run bash (whatever the path of the Doc Helper script). For those using PCs, you can cd to the root directory of the cloned DevOps Docs Repo and run the script using WSL, Cgywin, Git for Windows, etc.#3 - Write your documentation in the markdown file created by the Doc Helper Script, using markdown, but you can use HTML elements, images and styling elements: https://pmarsceill.github.io/just-the-docs/docs/ui-components.#4 - All images should be referenced like this:\n![the actual image file name without the file extension](/assets/docs-images/<path to specific doc images folder>)\n#5 - Move any image that you use in the documentation into the doc images folder that the Doc Helper Script created. The folder should be named after the title of your documentation and will be found using this path: /devsecops-docs/docs/assets/docs-images.\n#5a - If you'd like to include videos in your documentation, please upload them to https://web.microsoftstream.com/group/af278c11-5c40-4d7c-ae05-cea95b348a83. Click on the Upload a Video button or Click on the Upload Video link. Once the video is completely processed, click on the Publish button. Then click on Share > Embed. Choose the options you'd like to use for your video (Video Size, Autoplay, Responsive, etc.) and you'll see the code for your video below those options. Copy and paste it into your documentation markdown file.\n\n#6 - Please use one of the categories/folders from the Github Pages site: </>. Your documentation should fall under one of the listed categories/folders on the left side of the page. If not, please fill out this form and we will get back to you on whether or not a new category is needed (as soon as possible): https://1hlg3qldh6z.typeform.com/to/hNiVgBHN. If you're requesting a new category, please don't push your documentation to Github until the Github Pages Quality Board gets back to you with a response.#7 - Push the document and images, once done, to the mce/devsecops-docs repo. Then open a PR to get the documentation reviewed. Once it's been fully reviewed and approved, merge the PR and it should be published to the Github Pages site almost instantaneously.#8 - Spellcheck and proofread your documentation for any spelling, grammar and punctuation errors.#9 - Any questions you have regarding documentation creation, Github pages, blog creation, etc., can be asked in the #hs_devsecops_docs slack channel."}},"/internal/operations/docs_howto":{"title":"Contributing to Our Docs","data":{"":""}},"/internal/operations/docs_howto/How_to_Write_and_Post_a_Blog_to_our_GitHub_Pages":{"title":"How to Write and Post a Blog to our GitHub Pages","data":{"":"","in-order-to-create-write-and-publish-blogs-for-our-github-pages-the-following-items-are-required#In order to create, write and publish blogs for our GitHub Pages, the following items are required:":"#1 - Clone the DevOps Docs Repo (https://github.tools.sap/mce/devsecops-docs).#2 - Run the Blog Helper Script (found in the DevOps Docs Repo, in the scripts folder), which will create the markdown file for you. Run the script from/in the root directory of the cloned DevOps Docs Repo. For those using Mac computers/laptops, you can open your Terminal, cd to the root directory of the cloned DevOps Docs Repo and run bash (whatever the path of the Doc Helper script). For those using PCs, you can cd to the root directory of the cloned DevOps Docs Repo and run the script using WSL, Cgywin, Git for Windows, etc.#3 - Write your blog post in the markdown file created by the Blog Helper Script, using markdown, but you can use HTML elements, images and styling elements: https://pmarsceill.github.io/just-the-docs/docs/ui-components.#4 - All images should be referenced like this:\n![the actual image file name without the file extension](/assets/blog-images/<path to specific blog images folder>)\n#5 - Move any image that you use in the blog into the blog images folder that the Blog Helper Script created. The folder should be named after the title of your blog and will be found using this path: /devsecops-docs/docs/assets/blog-images.\n#5a - If you'd like to include videos in your documentation, please upload them to https://web.microsoftstream.com/group/af278c11-5c40-4d7c-ae05-cea95b348a83. Click on the Upload a Video button or Click on the Upload Video link. Once the video is completely processed, click on the Publish button. Then click on Share > Embed. Choose the options you'd like to use for your video (Video Size, Autoplay, Responsive, etc.) and you'll see the code for your video below those options. Copy and paste it into your documentation markdown file.\n\n#6 - Push the blog document and images, once done, to the mce/devsecops-docs repo. Then open a PR to get the blog post reviewed. Once it's been fully reviewed and approved, merge the PR and it should be published to the Github Pages site almost instantaneously.#7 - Spellcheck and proofread your post for any spelling, grammar and punctuation errors.#8 - Any questions you have regarding documentation creation, Github pages, blog creation, etc., can be asked in the #hs_devsecops_docs slack channel."}},"/internal/operations/docs_howto/How_to_use_the_DocHelperScript":{"title":"How to use the DocHelperScript","data":{"":""}},"/internal/operations/docs_howto/how_to_generate_docs_locally":{"title":"How to Generate Docs Locally","data":{"":"Jekyll provides us with the ability to run a local server which mimics how our documentation looks after being built with GitHub pages.Generating the documentation locally helps to ensure that the documentation that you write is properly formatted and styled. This documentation will provide an overview of building the documentation.","pre-requisites#Pre-requisites":"A few pre-requisites are required before generating the documentation:\nruby 3.0.0 - you should use a tool like rvm or rbenv to switch ruby versions\nbundler 2.1.2 - this can be installed by running gem install bundler:2.1.2","building-the-docs#Building the Docs":"If you'd like to check if the docs are properly building, run bundle exec jekyll build. You can skip this step entirely if you're planning on running a local server, which will also build the docs.You'll likely see a warning like this:\nGitHub Metadata: No GitHub API authentication could be found. Some fields may be missing or have incorrect data.\nDon't fear, the docs site will just be missing a few features, such as a list of contributors. This doesn't have a major impact.","serving-the-docs#Serving the Docs":"If you'd like to run a local server to view the docs, run bundle exec jekyll serve. This will start a jekyll server on localhost:4000 by default. If you already have something running on port 4000, you can specify the port by adding the --port option, like this: bundle exec jekyll serve --port 54321. To automatically reload the server when files are changed, run bundle exec jekyll serve --livereload. You may encounter some issues with this if you're running on windows.","serving-the-docs-as-a-docker-container#Serving the Docs as a docker container":"If you do not want to install the dependencies in your local system, or you run into an error while installing them, you can also use the provided dockerfile to build the image and run it as a container. This container is exposed in port 4000, and would do a hot reloading in case you update the files on the /docs folder. Hot reload is not instant, it takes about 10-15 seconds for jekyll to re-build the docs and serve an updated version.\nAll commands listed below should be run from the root of the devsecops-docs repository.\nBuilding the image:\ndocker build ./scripts -t jekyll-serve-mc-docs\n\nRunning the container:\ndocker run -p 4000:4000 -v $(pwd)/docs:/site jekyll-serve-mc-docs\n\n\nUpon succesful completion of step 2, container should be serving the docs on port 4000. See also logs from the shell for more info."}},"/internal/operations/elastic":{"title":"Elastic","data":{"":"Find all Elastic Ops related documention here!"}},"/internal/operations/elastic/elk_stack_monitoring_metricbeat":{"title":"ELK stack self monitoring with metricbeat documentation","data":{"":"This page documents important information about the process of the ELK stack monitoring, as well as some technical components that need to be maintained.","installation-process#Installation process:":"Installation process of metricbeat is to be done via rake tasks utilizing its respective helm chart, similar to any other component that is under /helm directory.\nPre-requisite configuration:\nMetricbeat requires a user with special permissions of kibana_admin, monitoring_user,remote_monitoring_agent in order to be able to send and retrieve metrics.\nThis user must be created before installation of metricbeat.\nUsername and password of this user must be encoded to base64.\nA k8s secret with the following details must be created with the data username and password as seen below:\napiVersion: v1\nkind: Secret\nmetadata:\nnamespace: elastic-system\nname: elk-monitoring-credentials\ntype: Opaque\ndata:\nelk-monitoring-user: User-please_encode_me_as_base64\nelk-monitoring-password: Password-please_encode_me_as_base64","post-installation-process#Post Installation Process:":"After installing metricbeat and its underlying components(kube-state-metrics), there are 2 main tasks that need to be completed/checked:\nNavigate to Stack Management > Rules and Connectors, in Kibana, and make sure that all the following rules are configured and enabled.\nNavigate to Stack Management > Index Lifecycle Policies, and create a retention policy for metricbeat data, to be deleted after 35 days.","monitoring-process#Monitoring process:":"Due to lack of a premium license, we could not integrate connectors that ship alerts via email, slack or teams, therefore monitoring is to be done every 1 month by a colleague (this is to be decided by the team, who and when), by checking the violated rules, under Stack Management > Rules and Connectors.\nAdditionally, some of the security settings are enabled via our codebase, therefore during this check, the colleague should take a look on the PR-s on the devsecops-infra repo, and observe any change on the ELK stack."}},"/internal/operations/exceptions":{"title":"Exceptions","data":{"":"Find all Exceptions Handling Process related documentation here!"}},"/internal/operations/exceptions/preventive_exceptions":{"title":"Preventive Controls Exceptions","data":{"":"Find all Preventive Controls Exceptions related documentation here!"}},"/internal/operations/incident_management":{"title":"Incident Management","data":{"":"Find documentation for Incident Management here!"}},"/internal/operations/hyperscaler_acct_list":{"title":"Hyperscalers Security Engineering & Operations (HSEO) Account List and Usage","data":{"":"The following table is the hyperscaler accounts list and their usage of the team.These accounts are under the devsecops group in Accounts Portal.Listing format:\nAccount ID\tAccount Name\tAccount Usage","alicloud-aly#AliCloud (ALY)":"Account ID\tAccount Name\tAccount Usage\t1891359093344755\tacchinaaccount+sap-mcsec-orca-aly-backend@sap.com\tOrca backend scanner account\t1951559441829708\tacchinaaccount+sap-mcsec-orca-aly-monitor@sap.com\tOrca backend monitoring account\t1500748695818345\tacchinaaccount+sap-mcsec-compliance-dev@sap.com\tTeam-managed Dev/Test account","amazon-web-services-aws#Amazon Web Services (AWS)":"Account ID\tAccount Name\tAccount Usage\t093985462227\tsap-mcsec-orca-backend\tOrca backend scanner account\t392461283862\tsap-mcsec-orca-subscanner\tOrca backend scanner account\t223317166624\tsap-mcsec-orca-in-account\tOrca backend in-account scanning account\t858071754418\tsap-mcsec-orca-saas\tOrca backend SaaS monitoring account\t126843302646\tsap-devsecops\tTeam-managed Dev/Test account\t485240016463\tsap-mcsec-aws-sandbox\tTeam-managed Learn/Test account (control testing that require multi-accounts)","aws-china-awc#AWS China (AWC)":"Account ID\tAccount Name\tAccount Usage\t337406457539\tsap-gcs-backend-dev\tOrca backend scanner account\t338875176586\tsap-gcs-monitoring-dev\tOrca backend monitoring account\t002945493082\tsap-gcs-sec-compliance-dev\tTeam-managed Dev/Test account","azure-azr#Azure (AZR)":"The following accounts are in SAP Shared Tenant.\nAccount ID\tAccount Name\tAccount Usage\t0808aa74-c021-4c7a-b435-3a6335033a7a\tsap-mcsec-orca-azr-scanner\tOrca backend scanner account\te6932354-bf0f-4d2f-9ba8-d8b35582a7ba\tsap-mcsec-orca-azr-scan4\tOrca backend additional scanner account\t2137e5eb-53d4-43cc-b3fc-4d3fc016072d\tsap-mcsec-orca-azr-scan3\tOrca backend additional scanner account\t8335fdda-1be0-4311-8758-0a293075ab87\tsap-mcsec-orca-azr-scan2\tOrca backend additional scanner account\t45241799-4f65-4fe1-a176-a01ad0921aa1\tsap-mcsec-orca-azrtest3\tOrca backend monitoring account for In-Account deployment\t694fec8a-bb0e-434d-9a41-84fc9a9533fa\tsap-mcsec-orca-azr-backend\tOrca backend SaaS monitoring account\tf797b97a-18b5-41f5-ab38-7be77cd30e35\tsap-gcs-sec-compliance-azr\tTeam-managed Dev/Test account","azure-china-azc#Azure China (AZC)":"Account ID\tAccount Name\tAccount Usage\t6eb58ebd-c03a-4e62-b37a-0277b764513d\tsap-orca-prod-gcs-backend\tOrca backend scanner account\t0c9ccd3b-b76c-496e-8d4b-c66ef188982a\tsap-orca-prod-gcs-monitoring\tOrca backend monitoring account","google-cloud-platform-gcp#Google Cloud Platform (GCP)":"Account ID\tAccount Name\tAccount Usage\tsap-mcsec-orca-gcp-in-acct\tsap-mcsec-orca-gcp-in-acct\tOrca backend scanner account\tsap-mcsec-orca-gcp-backend\tsap-mcsec-orca-gcp-backend\tOrca backend scanner account\tsap-mcsec-orca-gcp-test\tsap-mcsec-orca-gcp-test\tOrca backend in-account monitoring account\tsap-mcsec-orca-gcp-saas\tsap-mcsec-orca-gcp-saas\tOrca backend SaaS monitoring account\tsap-mcsec-sample-account\tsap-mcsec-sample-account\tTeam-managed Orca SSO default sample account\tsap-mcsec-orca-operations\tsap-mcsec-orca-operations\tTeam-managed Orca production services deployment account\tsap-mcsec-dependencies\tsap-mcsec-dependencies\tTeam-managed Python utils repository house (related to mce/devsecops-utils)\tsap-devsecops\tsap-devsecops\tTeam-managed Dev/Test account\tsap-mcsec-gcp-sandbox\tsap-mcsec-gcp-sandbox\tTeam-managed Learn/Test account (control testing that require multi-accounts)"}},"/internal/operations/elastic/elastic_api_key_generation":{"title":"Elastic API Key Usage","data":{"":"","document-control#Document Control":"The document control section describes the revision history and summary of changes made in the document. It will serve as the version control\nfor the contents of the document.\n**Minerva API access should only be granted in exceptional circumstances, and a requestor must provide business justification using the Minerva API request template, which is also included in the external Minerva API user guide.\nAccess Keys are a secure way to enable programmatic access to the Elastic Cloud API.\nYou can create an API key to quickly and easily authenticate, and then use the API to create and manage deployments, configure remote clusters, set up traffic filters, manage extensions, and much more.\nBy default, only a superuser has API access and can enable API access for other users.\nFurther documentation can be found on the Elastic API Documentation Page\n\nRemember:\nKeys should be set to expire within 90 days (Maximum)\nKeys set to expire in a timeframe greater than 90 days will be modified by the System Admin to expire within 90 days of its created date.\nOnce keys expire, you cannot reactivate an expired access key.\nIf an access key expires and the approved user still needs an API key, the end user should make another ticket requesting another key.","verifying-the-trusecurity-officer#Verifying the TRU/Security Officer":"","step-1--examine-the-ticket#Step 1 >> Examine the ticket":"In the ticket, either the user requesting API access is the TRU/Security officer, or they are requesting on behalf of the TRU/Security Officer. If the ticket does not specifically name another person who requires access, it should be taken that the author of the ticket is requesting access for themselves.1a. If the ticket is not requesting access on behalf of someone else, verify that the user who opened the ticket is the TRU/Security officer in HSDB.1b. If the ticket is requesting access on behalf of someone who is not the TRU/Security Officer, inform the person that they need the TRU/Security Officer to sign off on the ticket granting permission for the API key, then verify that the person who signed off is the TRU/Security Officer by checking the HSDB.\nTo check the TRU/SO on the HSDB, go to the HSDB Account portal and enter the Cloud ID for the account name. Then click on the padlock icon for the account to see further details:\n\n\nYou should scroll down and see the TRU and SO details for the account:\n\n\nIf the person requesting access wishes to have access to multiple accounts, they will need relevant BISO approval for the LOB(s). Please consult the BISO listed in the aforementioned link or this additional list of BISOs.","generating-the-api-key#Generating the API Key":"","step-1--create-a-user#Step 1 >> Create a user":"Create a user account in Elastic Dashboard for the person requesting API access. If the user who submitted the ticket already has an account, ensure that they have the correct role associated with their account (1b).1a. If the user does not have an account created, create one for them. On the home dashboard, click on the dropdown on the top left corner, and then scroll to the bottom and select 'Stack Management'. Then click on 'Users' in the management bar on the left side of the page.1b. If the user already has an account, go to 'Stack Management' then 'Users' to see the page containing the user accounts. Find the user account either in page or in the search bar.","step-2--create-the-api-key-for-the-user#Step 2 >> Create the API Key for the user.":"On the home dashboard, click on the dropdown on the top left corner, and then scroll to the bottom and select 'Dev Tools'.\n\n\nprintf '<id>:<api_key>' | base64\n\nThis should give some value, denominated as <BASE64_API_KEY>. This is the value that is used in the api request call. An example for the call, can be shown as\n\n\ncurl --location --request GET 'https://34.91.162.168:9200/*inspec*/_search?q=\"mcdb.cloud_id\":\"<CLOUD_ID>\"&q=status:failed&size=10' --header 'kbn-xsrf: true'  --header 'Content-Type: application/json' --header 'Authorization: ApiKey <BASE64_API_KEY>' --data-raw '{\"sort\": { \"timestamp\": \"desc\" }}'\nwhere <CLOUD_ID> is the id of the cloud account the query is being called for.\nIf an ssl error is encountered with the curl command, rerun the curl request with the -k flag enabled.\n\nAn important note here is that anyone who is using the API will need to know what fields they are trying to query, in this request the query is \"q=status:failed\". If a user attempts to export the dashboard as a whole, you will only get visualizations for the dashboard, instead of useful data.","step-3--securely-share-the-api-key-for-the-user#Step 3 >> Securely share the API Key for the user.":"To send the Elastic password or API key securely you should use SAP PassVault to create an ad-hoc ephemeral link that can be shared only with the authorized user in the ticket.\nLog on to SAP PassVault (if you have not logged on before you will be prompted to create a new password).\nYou should be part of the MCS-Minerva UGMT Group. If you are not, please request access via the UGMT.\n\n\nOnce you are in the MCS-Minerva group, go to the PassVault password creation tab.\n\n\nComplete and save the following details in the including the Username, any ticket reference and the API key file:\n\n\nOnce the password has been created, go to the PasswordVault password search tab and find the credentials you just created. Click on the ellipsis to the far right of the credentials you just created, and click Edit:\n\n\nClick on Ad hoc tab under the the Decrypt/ changing Authorization section (bottom right of screen) and then click the Create new button:\n\n\nSet the Validity duration to 48 hours and enter the user's staff number under Read Authorization and add them. Save the changes:\n\n\nOnce this is completed successfully, you will be presented with an ephemeral link that will expire in 48 hours from it's creation that only the user can access. Copy the link:\n\n\nPaste the link in to an encrypted e-mail to the user, and clearly state that the link is ephemeral and will expire in 48 hours.","step-4--rotating-and-deleting-api-keys#Step 4 >> Rotating and deleting API keys.":"Rotating a key\n\n\nAfter 90 days an API key is expired, and a new API key must be created. Expired API keys cannot be renewed or reactivated. To rotate/request a new API key, the user must submit another ticket with a request for an API key, and the process from the beginning is restarted.\n\n\nDeleting a key\n\n\nOn the home dashboard, click on the dropdown on the top left corner, and then scroll to the bottom and select 'Stack Management'. Then click on 'Users' in the management bar on the left side of the page.\n\n\nSelect the keys you wish to delete, and then click \"Delete API Keys\"\n\n\nConfirm that you want to delete the selected key. Once a key is deleted, it is permanently gone, and a new key must be created if desired."}},"/internal/operations/incident_management/pager_duty":{"title":"Pager Duty","data":{"":"Find all Pager Duty and Incident Management related documention here!"}},"/internal/operations/incident_management/pager_duty/major_incident_management":{"title":"Major Incident Management","data":{"":"SGS Global Security Operations (also known as security incident response or SGSIR) and the Multi Cloud SecDevOps have agreed to partner together and collaborate jointly on security incident response (SIR) in hyperscalers. The two operations teams will work SIR tickets together as One Team.This allows HS SecDevOps to bring their cloud expertise to bear in security incidents, as well as our ability to access cloud accounts for investigation and provide context using the metadata in our Multi Cloud Database. It also helps streamline processes between the two teams and collaborate on automation procedures based on threat detection from the central Splunk SIEM environment.","incident-definitions#Incident Definitions":"","rating#Rating":"P1 – Highest possible category. This is an immediate threat to SAP as a whole and needs to be taken care of ASAP. A P1 is a threat to SAP itself and can cause serious damage to SAP as a company. P1 incidents could be leaked credentials, cracked accounts with customer or confidential data or breaches which allow access to the internal network of SAP.\nP2 – Serious problem which needs a fast response to prevent further damage to LoBs or Customer infrastructure. P2 incidents can be cryptominers, spammers or other malicious code which runs on a compromised Instance. P2 incidents are a serious problem but don’t require to ring someone out of bed.\nP3 – Priority 3 incidents are indicents like denial of service or distributed denial of service attacks, or abused SAP ressources with no real “breach” of security. Priority 3 incidents are a threat if they are not taken care of, but no immediate threat to SAP or the LoB.\nP4 – ‘Random’ attacks like ssh or RDP bruteforce attempts, port scans and other ‘preperation’ attacks.","priority#Priority":"Priority\tImpact\tDefinition\tInitial Response Time\tP1\tBusiness-Critical Service Down\tA service failure. The business is at risk. Critical functions are unavailable within a revenue-impacting, production service.\t15 Mins.\tP2\tProduction Service Severely Impaired / Non-Production Service Down\tA partial service failure or significant degradation. Customer is able to access some, but not all business resources. Non-production instance of the service is unavailable. Bug creating significant impact to existing service.\t1 Hour\tP3\tService Partially Impaired\tMinor service impact. Non-critical functions behaving abnormally. Customer is able to access almost all business resources. Time-sensitive requirement or development question.\t4 Hours\tP4\tService Usable\tQuestion about features or development. Requesting access or feature enhancement. Customer is able to access all business resources.\t8 Hours","resources-available#Resources Available":"No.\tResource\tWebsite\tR1\tHyperscaler Account Database\thttps://db.multicloud.int.sap/admin/\tR2\tPrisma\thttps://prisma.tools.sap\tR3\tSAP Employee database\thttps://people.wdf.sap.corp\tR4\tSGS Incident Emergency Management\thttps://portal.wdf.sap.corp/go/incident-reporting","notifications#Notifications":"Manual notification via email (DL_5FA91E5D1DFE00027E55AD91@global.corp.sap) from Slack.\nAutomated Information from the hyperscaler to the given email address (Root account email with AWS accounts, service email with Google and Azure)\nAdditional notifications may come in from SGS IR, SGS GSO (Global Security Operations)(via AWS Guard Duty, splunk, etc.), Attack Surface Reduction (ASR)","on-call-duty-manager#On-Call: Duty Manager":"","pager-duty#Pager Duty":"Being on-call means that you are able to be contacted at any time in order to investigate and fix issues that may arise for the system you are responsible for. For example, if you are on-call for your service at PagerDuty, should any alarms be triggered for that service, you will receive a \"page\" (an alert on your mobile device, email, phone call, or SMS, etc.) giving you details on what is broken and how to fix it. You will be expected to take whatever actions are necessary in order to resolve the issue and return your service to a normal state. The current 24/5 duty manager has to answer, analyze and rate incoming reports. After the weekend, it's the duty of the next person in scheduled to go through email and look for possible open alarms.","incident-categorization#Incident Categorization":"","external-incident-response#External Incident Response":"This encompasses discovered incidents on LoBs systems. The on-call manager is accountable for contacting the responsible security contact(s), informing SGS IR and/or SGS GSO, and delegating tasks to the rest of the team to see the incident resolution.","internal-incident-response#Internal Incident Response":"These incidents are internal insofar as the team is concerned. These are incidents, vulnerabilities or otherwise on our own systems for which the on-call manager must assign themselves or someone within the team to address the incident itself.","actions-for-security-incidents#Actions for Security Incidents:":"Rate the Incident into a P1 to P4 incident\nLookup the account in question, note down all relevant information:\nAccount owner\nTechnical Responsible Person\n(If available) Security contact\nHyperscaler\nLine of Business\nYour rating and why\nAffected Ressources (Account/Landscape/Instance)\nIf available:\n\n\nFurther steps according to the the rating\nAnswer to the incoming Alarm email, so the Colleagues know the status.","coverage#Coverage":"On-call coverage currently rotates in 5-6 hour shifts across EU, US East and US West. This will keep all on-call personel only responsible for during the work day outages. The on call coverage has been localized to each of the timezones below.","edinburgh-london-utc0000#Edinburgh, London (UTC++00:00)":"UK\tUS East\tUS West\t09:00-14:00\t14:00-20:00\t20:00-01:00","eastern-time-us--canadautc-500#Eastern Time (US & Canada)(UTC-5:00)":"UK\tUS East\tUS West\t04:00-09:00\t09:00-15:00\t15:00-20:00","pacific-time-us--canadautc-800#Pacific Time (US & Canada)(UTC-8:00)":"UK\tUS East\tUS West\t01:00-06:00\t06:00-12:00\t12:00-17:00"}},"/internal/operations/incident_management/pager_duty/workflow_and_rota":{"title":"Pager Duty Workflow and Rota","data":{"":""}},"/internal/operations/orca":{"title":"Orca","data":{"":"Find all Orca-related documentation here!"}},"/internal/operations/orca/Orca_Access_Requests_Process":{"title":"Orca Access Requests Process","data":{"":"You may get tickets from people requesting access to Orca. Please ask them for the business reason for their request. Once you have a reason, please send them one of the following links, depending upon their business reason.Orca Viewer (with no API access) Role -This role can do the following (except for access API):\nView/List only.\nAssets - View asset information.\nCloud account - View cloud account information.\nDomain - View Domains information.\nJira - View tickets.\nCompliance - View checks foundation benchmark standards for AWS, GCP, and Azure.\nCVE's - View Common Vulnerabilities and Exposures.\nSonar actions - Query and Update Rules.\nRead-only access to all shift-left related resources\n\nOrca Splunk & Integration User Role -This role can do the following:\nAccess Integrations (Splunk, Jira, PagerDuty, AzureDevOps, etc.)Orca Admin Role -This role is mainly for the L3 SecDevOps team.Orca Viewer (with API access) Role -\nThis role is the same as the Orca Viewer Role (listed above) plus has API Access.Orca SecDevOps Developer Role -This role is mainly for the L3 SecDevOps team.For reference, you can check out this link Orca User Roles and Permissions. You'll need to first log into Orca SAP Instance > click on the Single Sign On tab > sign in > click the link above."}},"/internal/operations/orca/orca_ops_runbook":{"title":"Orca Ops Runbook","data":{"":"Find all Orca-related Ops Runbook documentation here!"}},"/internal/operations/orca/orca_ops_runbook/alert_playbook":{"title":"Alert Playbook","data":{"":"Find all Orca-related Alert Playbook documentation here!"}},"/internal/operations/orca/orca_ops_runbook/alert_playbook/issues_and_remediations":{"title":"Orca Ops Runbook - Issues and Remediations","data":{"":"This documentation will cover orca known issues and their remediations","orca-access-permissions#Orca Access Permissions":"Orca permissions are managed through Hyperscaler database (HSDB) and automatically updated based on your roles within accounts and groups on HSDB.\nPlease refer to “How Can I Get Access to Orca?” section of our Sharepoint page","how-to-use-orca-apis#How to use Orca APIs":"Please login to the orca portal and navigate to here"}},"/internal/operations/orca/orca_ops_runbook/alert_playbook/alert_and_troubleshooting":{"title":"Orca Ops Runbook - Alert and Troubleshooting","data":{"":"This documentation will cover orca alerts and how to troubleshoot them","alert-types#Alert Types":"Patching Alerts - These alerts are associated with system updates and modifications, and should be covered by the Orca team.\nAlerts that Require Exception - These alerts can be categorized into two distinct types:\n\n\nSelf-Service Tags-Based Exception\nAccounts that Require Exceptions on Specific Controls","self-service-tags-based-exception#Self-Service Tags-Based Exception":"Self-service tags-based exceptions can be directly applied by Lines of Business (LoBs) to cloud assets with approved tags as per SGS Wiki. The implemented system already has the capability to exclude assets where the approved tag matches the SGS wiki.\nIf a ticket from LoBs is received where an issue has been raised that the self-service tag is already in place but an alert was still triggered, verification must include the following:\nCheck the account status: If offline, most cases will relate to the old alerts.\nConfirm the alert discovery and last seen time: If older than 2 days, most likely it's an old alert.\nVerify the applied tag at the cloud asset in the hyperscaler account using a cloud admin account, ensuring it's the correct tag and validate it with SGS wiki approved tags. Check in the SAE/Orca repository that the control has tag exclusion applied. If any changes are needed, create a PR for it.","accounts-that-require-exceptions-on-specific-controls#Accounts that Require Exceptions on Specific Controls":"Troubleshooting should include checking at least the following information:\nDetermine the account status: If offline, most cases it's an old alert.\nVerify when the alert was discovered and last seen: If it's older than 2 days, it is likely an old alert.\nFor High severity control, check the provided SEMP ticket to determine if it has already expired or had its validity extended recently. For Medium severity control, verify the validity of the exception in Exception CSV. Communicate any required changes with the exception processor.\nNote: For exceptions already expired, direct LoBs to raise a new exception ticket for both High and Medium severity controls.\nIf all above conditions are correct, look up in Orca -> Automations with Ticket-ID under the Description field to see the current rule. Discuss any required changes with the exception processor."}},"/internal/operations/orca/orca_ops_runbook/common_tasks":{"title":"Common Tasks","data":{"":"Find all Orca-related common tasks info documentation here!"}},"/internal/operations/orca/orca_ops_runbook/common_tasks/maintenance":{"title":"Orca Ops Runbook - Patching and Maintenance","data":{"":"This documentation will cover any rotational tasks for Patching and Maintenance\nPatching on some cloud accounts we have after Orca migration.The Patch Management process can be refer here\nEvery 2 weeks we need to make sure that detective control alerts are being looked at by Orca for infra they run on in SAP.\nMake sure any training accounts we have are checked at least once a month for assets and are cleaned up.\nDependabot alerts"}},"/internal/operations/orca/orca_ops_runbook/common_tasks/sso_and_cam_configuration":{"title":"Orca Ops Runbook - SSO and CAM Configuration","data":{"":"This documentation will cover the setup of Orca's SSO application and integration with CAM.","sso-setup-and-configuration#SSO Setup and Configuration":"The steps outlined in the Orca documentation for Azure SSO configuration were followed to setup SSO on Orca. This documentation can be found here.As a result of the setup, we have created an enterprise application on Azure Active Directory with the name \"SAP SE: Orca Security SSO\" and the object ID 58e0ac77-029c-4987-a340-2d3497203389. This enterprise application can be found in the SAP SE tenant.","integration-with-cam#Integration with CAM":"When creating a CAM profile, there is an option to sync the CAM profile to Azure Active Directory. As long as this is done, a group will be created in Azure with the same name as the CAM profile that's been created. Users who have been approved as members of a CAM profile will be automatically added to the corresponding Azure AD group. The sync between CAM and Azure AD can sometimes take up to an hour.\nSome other options in CAM profile creation request:\n- Synchronize to GCP: Select this if using CAM profile for GCP project.\n- Profile Auto-Approve?: No if CAM profile requires approval.\nWhen a CAM profile is created, it is not automatically included in the SSO application for Orca. To do this, one of the owners of the Azure SSO application should add the corresponding Azure AD group to the SSO application. As team members and administrators fluctuate, I will leave it up to the reader of the documentation to check who is an owner of the enterprise app. You can either do this by viewing the enterprise application yourself if you have permissions, or asking other members of the team.Below, you can see the CAM profiles added to the SSO application:\nAfter this initial setup, move on to the creation of the permissions group in Orca found below.","creating-permission-groups#Creating Permission Groups":"Once a CAM profile and corresponding Azure AD group are created, a permission group must be created in Orca so that the users for your specific group can be assigned group-level permissions. To do this, follow Orca's documentation here","troubleshooting-steps#Troubleshooting Steps":"If there are any issues with a user's SSO group-level permissions ask yourself the following questions and investigate:\nHas the user requested the CAM profile? - if not ask them to request the CAM profile\nHas the user been approved for the CAM profile? - if not ask them to wait for approval\nHas the user waited up to an hour for the CAM->Azure AD sync to occur? - if not ask them to wait and check back in\nIs the user included in the Azure AD group? (check azure AD) - if not there is an issue with the CAM->Azure AD sync, given that the sync-period was taken into consideration\nIs the issue related to CAM permissions, or permissions that would be granted by the permissions automation? If it may be linked to the permissions automation, check if there have been any recent errors in the hourly run of the automation that may affect your case.\nIf all of the above options have been exhausted, open a ticket with Orca support as it is likely that there is an issue on the Orca side"}},"/internal/operations/orca/orca_ops_runbook/common_tasks/test_tenant_administration":{"title":"Orca Ops Runbook - Test Tenant Administration","data":{"":"This documentation will cover information about the Orca test tenant","accessing-the-test-tenant#Accessing the Test Tenant":"","pre-requesites-for-logging-in#Pre-requesites for logging in:":"No active Orca production sessions can be logged in on the browser that you plan to log into the test tenant on. If you'd like to access the production tenant and the test tenant at the same time, you must access them in different browser sessions. I most often have one open in a regular session and the other open in an incognito session.\nYou must be granted access to the test tenant by one of the admins. Reach out to others on the team to find out who has admin access to create a user for you.","logging-in#Logging in":"The test tenant uses the same hostname as the production tenant, so to log in you will navigate to https://eu.sap.app.orcasecurity.io/.\nInstead of using SSO to login, navigate to the email/password login.\nLogin with the test user that has been granted to you. This will be your SAP email, with an added +test tag. IE first.last+test@sap.com. The +test tag addition is what allows orca to redirect requests for this user to the test tenant.\nYou should now be logged into the test tenant.","adding-users-to-the-test-tenant#Adding Users to the Test Tenant":"Before adding a user, its best to send them an email with the login instructions, IE:\n\nHello User,\nYou’ll momentarily receive an invite with a registration link for the Orca test tenant. When you register, please be sure to register with your sap email, with a tag “+test” appended to it. For instance, first.last+test@sap.com. This is how Orca redirects users to the test tenant.\nTo login, go to the following url: https://eu.sap.app.orcasecurity.io/\nThis URL is the same as our production instance, except the test tenant is logged into via username and password rather than SSO. Use your email with “+test” appended to it, and you will be logged into the test tenant.\n\n\nNavigate to the Users screen in Orca\nClick on Invite User\nAdd the user email that you'd like to invite to the test tenant. This should be their regular SAP email, with a +test tag added to it. IE first.last+test@sap.com.\nSelect the role/permissions scope that you want the user to have.\nSelect the option to send an invitation email (optional) - for named users, it's best to send them an invite link. If you are creating a testing user for internal team use (IE testing-user1+test@sap.com), do not send an invite email, but rather generate an invitation link and create the user using that link, since the test user would not have an email inbox."}},"/internal/operations/orca/orca_ops_runbook/common_tasks/service_continuity":{"title":"Orca Ops Runbook - Orca Service Continuity","data":{"":"This documentation will cover any rotational tasks for service continuity (e.g. PAT rotation, key rotation)In Secret Manager, check the following to check the current version of\nthe keys in place:sap-mcsec-orca-operations project > Security > Secret Manager\n> orca-alert-publisher-devsap-mcsec-orca-operations project > Security > Secret Manager\n> orca-alert-publisher-prodsap-mcsec-orca-operations project > Security > Secret Manager\n> onboarding-verificationsap-mcsec-orca-operations project > Security > Secret Manager\n> onboarding-verification2sap-mcsec-orca-operations project > Security > Secret Manager\n> orca-security-servicesap-mcsec-orca-operations project > Security > Secret Manager\n> orca-security-service-testingLet's update the keys to the new version.For example, sap-mcsec-orca-operations project > Security >\nSecret Manager > orca-security-service-testing > click on Versions >\nNew Version > paste text into secret value box and click on Add\nNew Version\n\nIn Secret Manager, check the following to check the current version of the keys in place:sap-mcsec-orca-operations project > Security > Secret Manager\n> azurecn-cdc-list-accountssap-mcsec-orca-operations project > Security > Secret Manager\n> azurecn-sap-list-accountsFor example, sap-mcsec-orca-operations project > Security >\nSecret Manager > azurecn-cdc-list-accounts > click on Versions >\nNew Version > paste text into secret value box and click on Add\nNew VersionWe need to create 2 new secrets for SAP & CDC Orca Apps, then store new secret in secret manager under GCP project \"sap-mcsec-orca-operations\"\nFor CDC:\nsvc-orca-cdc\n\n\nFor SAP:\nsvc-orca-sap\n\n\n\nGCP Secrets created for Azure_CN accounts:\nazurecn-cdc-list-accounts\nazurecn-sap-list-accounts\n\nAfter new secrets are created, please create meeting invite as reminder for 15 days before new expiry date.\nTerraform changes or deployment is not required since we are using 'latest' version in the service."}},"/internal/operations/orca/orca_ops_runbook/disaster_recovery_plan":{"title":"Disaster Recovery Plan","data":{"":"Find all Orca-related recovery plan documentation here!"}},"/internal/operations/exceptions/preventive_exceptions/azure_preventive_exception":{"title":"Azure Preventive Controls Exceptions","data":{"":"This document outlines the steps for applying exceptions to Azure preventive controls.","pre-requisites#Pre-requisites":"Cloud Admin Account\nPolicy Contributor role in Azure AD Privileged Identity Management (PIM)","login-to-azure-portal#Login to Azure portal":"Login to https://portal.azure.com/ with your cloud admin account","to-locate-the-subscription#To locate the subscription":"Go to Subscriptions\nChange all active filters (Subscriptions/My role/Status etc.) to show all (e.g. Subscriptions == all)\nIn the Search for any field search bar, search by subscription name or subscription ID\n\nNote: You may have to switch between tenants in order to locate the subscription","to-switch-between-tenants#To switch between tenants":"Click on user profile menu (top right-hand corner) -> Switch directory\nClick on Switch button to switch between tenants","elevate-privileges#Elevate privileges":"Once the subscription has been located, follow these steps to activate Policy Contributor role in Privileged Identity Management (PIM)\nGo to Azure AD Privileged Identity Management -> My roles -> Azure resources\nUnder Action field, click on Activate for the Policy Contributor role\nEnter ticket system type in Ticket system field (e.g. https://securityjira.wdf.sap.corp)\nEnter SEP number(s) in Ticket number field (e.g. SEP-1234, SEP-1235)\nEnter reason for activation in Reason field (e.g. Add exceptions)\nClick on Activate\n\nOnce approved, proceed to apply exception for the subscription.","apply-exception-for-subscription#Apply exception for subscription":"In the subscription, go to Policies -> Assignments -> (Assignment name) -> Edit\nUnder Exclusions, click on ... button to launch scope selector\nClick on Please choose a Subscription field to expand dropdown list, enter subscription name, check the box next to subscription name to select it, then click on Add to Selected Scope\nConfirm that the selected subscription has been added to the list SELECTED SCOPE, then click on Save, Review + save, Save\nOnce policy assignment has been updated successfully, you should see a notification stating Updating policy assignment succeeded appear in the corner of the window"}},"/internal/operations/orca/orca_ops_runbook/disaster_recovery_plan/recovery_plan":{"title":"Orca Ops Runbook - Disaster Recovery Plan","data":{"":"This documentation will cover Disaster Recovery Plans and procedure.","orca-automation-rebuildrecovery-plan#Orca Automation Rebuild/Recovery Plan":"We have terraform for all services which can be used as redeployment","orca-data-pipelines#Orca Data Pipelines":"Backups:\nBackups are performed on a daily basis, active standby for databases and full active redundancy for compute.\nRetention:\nOrca Team retain raw scan data for 30 days and the final scan reports (the data presented in the dashboard with Orca's inputs) for up to 12 months.\nBCP and DR:\nOrca Team perform an annual DRP test."}},"/internal/operations":{"title":"Operations","data":{"":""}},"/internal/operations/orca/orca_ops_runbook/integration_services":{"title":"Integration Services","data":{"":"Here are maintained runbooks for Orca integrations and services."}},"/internal/operations/orca/orca_ops_runbook/integration_services/splunk_integration":{"title":"Orca Ops Runbook - Orca Splunk Integration","data":{"":"This documentation covers support steps to provide the Orca Integration to\nSplunk for LoBs.","orca-integration-for-splunk#Orca Integration for Splunk":"Below are the steps required by both LoB and Operation Engineer to create the\nSplunk integration and automation:\nLoB creates a\nticket\nto request Splunk integration\nLoB requests Multicloud_SecDevOps_OrcaSplunkUser CAM\nprofile\nOnce CAM profile is approved, LoB creates the splunk integration in Orca as\ndescribed in the\ndocumentation.Note: sign in to Orca here via SSO\nand selecting Documentation from the Question Circle Icon on the top right of\nthe dashboard before the documentation can be seenPrerequisite: the LoB needs to understand the type of Splunk integration\nrequirements and create necessary setups in advance\nLoB creates the Splunk integration in Orca and communicates the\nname to Engineer\nLoB communiates the Query for the automation to the Engineer\nEngineer creates Orca Automation using Query and Splunk Integration from\nLoB. Then name of the Automation must be in the format\n<LoB_acronym>-Splunk-<scope>-automation (please note the use of - to\nseparate the diferent parts of the name), where scope is one optional and\ncan be decided by the LoB if needed.  e.g. ASR-Splunk-automation,\nGCS-Splunk-alerts-automation\nLoB to confirm that the data is flowing to Splunk.Note: some time might\nbe required for the data to be visible in Splunk, in some cases this has been\nquick, while it took more than a day for others. Please reach out if you\nstill do not see the data in Splunk after 2 days"}},"/internal/operations/orca/orca_ops_runbook/service_build_information/Orca_Account_Offboarding":{"title":"Orca Ops Runbook - Orca Account Offboarding","data":{"":"This documentation will cover the process of offboarding an account from Orca.\nThe account offboarding service aims to remove accounts from ORCA once the same accounts are marked as deleted in HSDB. It consists of two different GCP functions:\n\nAlert Dismiss Offline\nAccount Offboard","alert-dismiss-offline#Alert Dismiss Offline":"Retrieve all deleted accounts in HSDB\n\n\nIn this cloud function, we begin by retrieving all the deleted accounts from HSDB.\n\n\nCompile deleted accounts into a list\n\n\nSubsequently, we compile a list of all the accounts currently present in ORCA. If we find accounts marked as deleted in HSDB that also exist in ORCA, we create a list of those accounts that need to be deleted within ORCA.\n\n\nTransitioning Deleted Accounts to offline in Orca\n\n\nWe first take these accounts offline and dismiss any associated alerts for each one.\n\n\nUpload account object to GCP Storage bucket\n\n\nAdditionally, we create an object with the account ID and upload it to the GCP storage bucket, which is configured with a lifecycle rule to delete objects after 90 days.\n\n\nAccount Sanity Check\n\n\nLastly, we conduct a daily sanity check to identify if any accounts marked for deletion still have alerts present in ORCA.\n\n\nSlack Notification\n\n\nIf any such accounts are identified, we compile a list of problematic accounts and send notifications to #hs_orca_offboarder slack channel.","account-offboard#Account Offboard":"GCP Buckets with deletion accounts\n\n\nFiles uploaded by the preceding service are stored in the GCP storage bucket named \"orca-account-offboarder.\"\n\n\nObject Lifecycle rule reaches 90 days get deleted\n\n\nThis bucket has object lifecycle enabled, and it follows a policy to automatically delete objects 90 days after their creation date.\n\n\nCloud event pickup\n\n\nWhen an object is deleted, a cloud event triggers the associated cloud function.\n\n\nAccount Delete Function\n\n\nThis function receives metadata related to the deleted object and utilizes the ORCA delete API to remove the account from ORCA.","conclusion#Conclusion":"The account offboarder service plays a crucial role in automating the account deletion process within ORCA. Its two GCP functions, \"Alert Dismiss Offline\" and the \"Account Offboard,\" work in tandem to ensure the seamless removal of accounts and dismissal of relevant alerts.\n\nAdditionally, to enhance transparency and communication, all notifications generated during the account offboarding process are sent to #hs_orca_offboarder slack channel. This ensures that relevant team members stay informed about the status of account deletions and any potential issues, enabling a smoother and more efficient account management workflow."}},"/internal/operations/orca/orca_ops_runbook/integration_services/shiftleft_service":{"title":"Orca Ops Runbook - Orca ShiftLeft service","data":{"":"This documentation covers support steps to provide the Orca ShiftLeft service to\nLoBs.","orca-shiftleft#Orca ShiftLeft":"Orca provides ShiftLeft service for 3 different areas:\nContainers images (CI)\nInfrastructure as Code (IaC)\nCode security (CS)\n\nMore details in the official Orca ShiftLeft\ndocumentation.Below are the steps required by both LoB and Operation Engineer to enable the\nLoB to use the ShiftLeft service:\nLoB creates a\nticket\nto request ShiftLeft service enablement\nLoB requests ShiftLeft CAM profile\nLoB communicates project-key to Engineer as per\ndocumentationNote: sign in to Orca here via SSO\nand selecting Documentation from the Question Circle Icon on the top right of\nthe dashboard before the documentation can be seen\nLoB creates ShiftLeft one or more ShiftLeft policies and communicates the\nname(s) to Engineer\nEngineer creates ShiftLeft project using project-key and policies from\nLoB. Then name of the project must be in the format\n<LoB_acronym-scope_1-scope_2> (please note the use of - to separate the 3\ndiferent parts of the name), where scope_1 is one of [CI, IaC, CS], and\nscope_2 can be decided by the LoB (in case they need additional scoping),\ne.g. isbn-CI-best_practices\nEngineer provides the project name to LoB\nLoB creates an API token as per\ndocumentation"}},"/internal/operations/orca/orca_ops_runbook/service_build_information":{"title":"Service Build Information","data":{"":"Find all Orca-related service build info documentation here!"}},"/internal/operations/orca/orca_ops_runbook/service_build_information/Orca_Account_Onboarding":{"title":"Orca Ops Runbook - Orca Account Onboarding","data":{"":"This documentation will cover the process to onboard new organizations to Orca.","gcp-onboarding#GCP Onboarding":"To onboard a new GCP organization to Orca, the following steps will need to be taken by admins in the respective organization:\nA new project is created within your organization. Ideally, the project will follow the naming convention sap-mcsec-orca-operations-<context>. For example, a project was created in the SAP development organization with the following name: sap-mcsec-orca-operations-dev.\nCreate a service account with the name Orca Security Service and description Account used for onboarding and scanning all accounts in <context about your organization> organization on the Orca Security tool.\nGive the Project IAM Admin role to the HS SecDevOps admin that you are working with.\nEnable the cloudresourcemanager API for the project.\nCreate an org-level role titled orca-side-scanner-role with the following permissions:\ncompute.snapshots.setLabels\ncompute.disks.createSnapshot\ncompute.snapshots.create\ncompute.snapshots.delete\nstorage.buckets.getIamPolicy\nstorage.objects.get\ncompute.snapshots.setIamPolicy\ncompute.snapshots.useReadOnly\nserviceusage.services.enable\nservicemanagement.services.bind\nresourcemanager.folders.list\n\nAssign the orca-side-scanner-role at the organization level to the service account created in step 2.\nAssign the following additional roles at the organization level to the serviec account created in step 2:\nViewer\nStorage Object Viewer\nSecurity Reviewer\nCloud KMS CryptoKey Encrypter/Decrypter\n\n\nThe remaining steps will be carried out by a member of the HS SecDevOps team:\nCreate a service account key for the service account previously created.\nUpload this service account key to Orca in the GCP Multiple account onboarding screen.\nNote: Account discovery and onboarding can take up to 24 hours. Orca onboards accounts after an internal verification process - after this process the accounts will show up in batches."}},"/internal/operations/orca/orca_ops_runbook/service_build_information/Orca_Automation":{"title":"Orca Ops Runbook - Orca Automation","data":{"":"This documentation covers Orca automation used by team","orca-exceptions-via-automation#Orca Exceptions via Automation":"","overview#Overview":"The \"Orca Exceptions via Automation\" project automates the management of exceptions in the Orca tool. It introduces three cloud functions that work together to streamline the exception as automation cycle. Each function serves a specific purpose and operates independently, allowing for a decoupled and efficient workflow.","requirements#Requirements":"The main breakdown of the creation of this service can be found in this tracker. # noqa: E999\nTo use this service, the following requirements must be met:\nData passed to the Create Automation function must include the following fields:\n{\n\"type\": \"new/update\",\n\"mcdb_groups\": [\"<mcdb_group1>\", ...],\n\"is_group_exception\": \"<is_group_exception>\",\n\"account_ids\": [\"<account_id1>\", ...],\n\"alert_names\": [\"<alert_name1>\", ...],\n\"expiry_date\": \"<expiry_date>\"\n}\nPlease ensure that the data provided adheres to this structure, include \"orca_automation_rule_id\": \"xxxxxx\" if type=update.","architecture#Architecture":"The project architecture consists of the following components:Create Automation: This function receives requests via GCP Pub/Sub to create or update exceptions. It examines the payload and determines whether it is a new exception or an update to existing exception. For new exception, the function performs validation on various fields, creates an automation rule in Orca, and inserts a corresponding record in the Cloud Spanner database. In the case of an update exception, the function directly updates the record in the Cloud Spanner database using the Orca automation rule as the primary key.Update Automation: The Update Automation function receives the payload directly from the change stream configured at the Cloud Spanner side. It detects \"update\" modifications and applies the necessary changes to the Orca automation rules, either by updating existing rules or deleting them.Expiry Status Checker: Triggered on a weekly basis by Cloud Scheduler, this function reads all valid exceptions from the Cloud Spanner database. It checks their expiry dates and updates the status to false if the expiry date has passed.Slack Integration (Notification Channel): The Slack integration acts as a real-time notification channel for the cloud functions. It delivers immediate alerts and updates for any exceptions-related events, including errors or modifications. Whenever such events occur, relevant messages are sent to designated Slack channels, enabling timely awareness.","conclusion#Conclusion":"The \"Orca Exceptions via Automation\" project provides a comprehensive solution for automating the Orca automation cycle. By leveraging cloud functions and GCP services, it enables seamless exception management, improves efficiency, and reduces manual intervention."}},"/internal/operations/orca/orca_ops_runbook/service_build_information/Orca_HSDB_Integration":{"title":"Orca Ops Runbook - Orca to HSDB Integration","data":{"":"This documentation covers the build of Orca to HSDB integration.","orca-alerts-to-hsdb-integration#Orca Alerts to HSDB Integration":"HSDB contains cloud accounts information and provides API and portal access to its database.\nOrca alerts are sent from Orca automation rules via GCP Pub/Sub to HSDB for access by LOBs.The following diagram shows the general flow of the Orca alerts to HSDB.\n\nAs part of Orca custom controls deployment, the Orca automation rules are created or updated where applicable.\nThe Orca automation rules notify/publish the alerts to GCP Pub/Sub.\nHSDB subscribes to the GCP Pub/Sub with subscription \"orca-alert-subscription-{environment}\" to get the alerts and adds to its database.\nLOBs can query the HSDB via API to get the alerts.","orca-automation-rules-for-alert-notification#Orca Automation Rules for Alert Notification":"Automation rules have been setup in Orca as part of Orca custom controls deployment.\nThese automation rules track custom controls and notify/publish their alert updates to GCP Pub/Sub, provided by the Alert Egress Service.\nThere is 1 automation rule per cloud provider, with the naming convention as \"{cloud_provider}-automation\".\nWhen Orca custom controls are deployed for a cloud provider, the automation rule for that cloud provider is created if it does not exist. If the automation rule already exists, its list of custom controls gets updated where applicable (such as adding/removing controls).\nWhen there are updates to the alerts of these custom controls, the corresponding automation rule gets triggered and publishes the alert updates to GCP Pub/Sub.","alert-egress-service#Alert Egress Service":"This Alert Egress Service is implemented in GCP project \"sap-mcsec-orca-operations\".\nThis service provides the GCP Pub/Sub topic with the naming convention as \"orca-alert-target-topic-{environment}\".\nIt receives custom control alert updates from Orca automation rules.\nIt allows HSDB to subscribe and get the alert updates.","troubleshooting#Troubleshooting":"If alerts are not updated in HSDB, some troubleshooting steps (not in sequence) to try as follows:\nCheck GCP Pub/Sub subscription queue if it's subscribing alerts or the number of messages doesn't change, if it doesn't change it might be possible something is broken at HSDB end and need to contact HSE team to investigate.\nTo verify new alerts pushed to HSDB, check GCP Pub/Sub topic if new alerts were pushed to topic or not.\nIf alert_type is not updated to automation rule in Orca, therefore leads to not ingesting the alerts to HSDB, check GCP cloud build logs if there was any issue with deployment.\nIf Orca or HSDB alert schema is changed, check if alerts need to be re-synced from Orca.\nIf alert is not available in Orca, alert will not be updated in HSDB. Remove it from HSDB if required.\nManually trigger an alert from Orca and monitor its journey through the entire pipeline (Orca automation rules, GCP Pub/Sub, HSDB). This can help pinpoint where the issue might be occurring."}},"/internal/operations/orca/orca_ops_runbook/service_build_information/Orca_Pipelines":{"title":"Orca Ops Runbook - Orca Pipelines","data":{"":"This documentation covers all pipelines that are used for any Orca OpsOrca Pipelines and service accounts are used test, build and deploy ORCA related code. It makes use of Github Actions, Github Service Accounts, GCP Code Build and Trigger.","prerequisites#Prerequisites":"Github Status Check Before Merging : Mainly there are three type of rules for PR check STRICT, LOOSE & DISABLED. This logic is used to enforce checking for PR.Creating and managing build triggers - This provides a basic idea on how code build and triggers works.","orca-pipelines#ORCA Pipelines":"Orca Pipelines are running on GCP Code Build and make uses of GCP triggers to run pipelines for verify ORCA config, build ORCA and run python flake.There are 2 GCP projects that runs pipelines\nGCP Project Pipelines\tDescription\tsap-mcsec-orca-operations\tPipelines to Verify Custom Orca Controls, Apply controls for Prod and Test tenant, Deploy Services to Prod\tsap-mcsec-dependencies\tPipelines for Flake8 - PEP8 check pipeline for ORCA services and Python builder for dev-utils","pipelines-in-use-for-orca-#Pipelines in use for ORCA :":"orca/administration/controls/deploy.yaml - deploy hyperscaler controls to orca project","pipelines-in-use-for-orca-services#Pipelines in use for ORCA services:":"orca/services/account_fetching/deploy/deploy.yaml - deploy account fetching service to orca project\norca/services/account_onboarding/deploy/deploy.yaml - deploy account onboarder service to orca project\norca/services/alert_egress/deploy/deploy.yaml - deploy alert egress infrastructure to orca project\norca/services/data_extractor/deploy/deploy.yaml - deploy data extractor to orca project\norca/services/orca_mcdb_automation/deploy/deploy.yaml - deploy orca HSDB automation to orca project\norca/services/orca_exceptions/deploy/deploy.yaml - deploy the orca exceptions","pipelines-in-use-for-flake8#Pipelines in use for Flake8":"orca/python-lint.yaml - to check for PEP8 format when PR is created","pipelines-for-python-utilies#Pipelines for Python Utilies":"cicd/py/build_packages.yaml - build and reploy python-utilities","orca-services-account-for-github#ORCA Services Account for GITHUB":"There are 2 service accounts in Github to send webhooks from github to GCP trigger.Services accounts can be managed using Github Service User Management.\nService Account\tDescription\tmc-secdevops-serviceuser\tUsed for ORCA repo\tmcsec-cloud-compliance-serviceuser\tUsed for mcsec-cloud-compliance","orca-services-accounts-for-gcp-code-build#ORCA Services Accounts for GCP Code Build.":"Each trigger in respective GCP Trigger has services accounts that is having role Cloud Build Service Account."}},"/internal/operations/orca/orca_ops_runbook/service_build_information/Orca_Repository":{"title":"Orca Ops Runbook - Orca Repository","data":{"":"This documentations Covers all parts of Orca repo and their functions","orca-automation#Orca Automation":"This repository houses all automations, code, and documentation for orca. The controls are targetted to be complete by mid-April with a swarm to occur the first 2 weeks of April."}},"/internal/operations/orca/orca_ops_runbook/service_level_agreement/ops_level_agreement":{"title":"Orca Ops Runbook - Service Level Agreement","data":{"":"The agreement of the service or operations levels you need to maintain, either as part of an internal goal/KPI or by a formal contract with customers. Service Level Agreements are used for Ops to external parties. Operations Level Agreements are used internally.","mcsae-ops-level-agreement#MCSAE Ops Level agreement":"Please refer to the link here","sgs-policies-covered-by-orca#SGS policies covered by Orca":"AWS SGS Hardening Policy\nGCP SGS Hardening Policy\nAzure SGS Hardening Policy\nAlicloud SGS Hardening Policy"}},"/internal/operations/orca/orca_ops_runbook/service_build_information/Orca_Permissions_Automation":{"title":"Orca Ops Runbook - Orca Permissions Automation","data":{"":"This documentation covers everything you need to know about the Orca Permissions Automation Solution","background#Background":"Cloud accounts are constantly being created and deleted within SAP. Along with changes in cloud accounts, we have personnel and organizational changes. All of these updates to accounts and our organization require permission changes in Orca. To do this manually would be a lot of work, as previously experienced with managing Prisma, and would likely become outdated very quickly. We've implemented an automated process based on self-service attributes in HSDB to handle organizational changes, personnel changes, and new cloud accounts.","implementation#Implementation":"This solution has been implemented in python and runs on Google Cloud Functions - for more information about the specific services in use, and how they are triggered, see the architecture section.This solution pulls information about Hyperscaler Groups and Hyperscaler Accounts from HSDB.To map Hyperscaler Groups to accounts in Orca, the business unit feature in Orca was leveraged. There is a one to one relationship between Hyperscaler Groups and Orca business units. Each run of this solution pulls the accounts for every Hyperscaler Group and child group, and updates the business unit in Orca that coincides with the Hyperscaler Group. A business unit in Orca contains all of the accounts of a group and the accounts of all of its descendent groups. For every business unit in Orca, there is also a permissions group created. This permissions group gets viewer access on the related business unit, and the users in this permissions group are added if they are listed as a Reader, Administrator, Power User, or Editor on the group in HSDB.To map Hyperscaler Account roles to permissions in Orca, the solution pulls information about every user in Orca. Then, the solution pulls the list of accounts from HSDB for which the user is listed as a Technical Responsible User, Security Officer, or Additional User. Then, Viewer access is granted to the user on the list of accounts.The user's permissions are a summation of the business units that they are permitted to view, and the individual accounts that they are permitted to view.","architecture#Architecture":"Retriever - Pulls all groups and group tree (for parent-child relationships) from HSDB to get all current groups and lists blobs in cloud storage to get groups that have already been applied to Orca. Pulls all Orca users and writes to a metadata file. Pulls all cloud account ids belonging to an HSDB group and writes them to a metadata file. Both metadata files will be instantiated as python dictionaries for easy lookups in microservices which need to read the data. After looking up this data, the service sends each HSDB group uuid and each orca user uuid in a pubsub message to topics that trigger the group data collector and user data collector service respectively.Data Collector - Both the group data collector and user data collector follow the same pattern. The service reads data from the metadata files and reads data from the state file for given uuid. There will be a state file for each HSDB group which contains data such as group name, group admins, accounts under the group, etc., and a state file for each orca user with the user's uuid, and a list of accounts that they should have access to in Orca. The diff calculator will rebuild the state object based on the metadata files pulled from HSDB, and then check if the newly built state object matches the existing state file's object. If it matches, the service will exit and do nothing. If it is different, it will write the updated object out to the state file. If a group is new, it will create a new statefile. If a group no longer exists in HSDB, it will mark the state file for deletion.Applier - Both the group data applier and user data applier follow the same pattern. The service is triggered by create/update operations on each statefile, this service applies the HSDB group data, or user permissions to Orca. The group applier will create or update the orca business unit with the given data. It will then create or update a permissions group in orca which gives viewer access to the business unit. It will add the HSDB group admin/other group users to this permissions group. Finally, if a group/business unit is newly created the applier will update the metadata of the statefile to contain 2 tags, one of which is the uuid of the orca business unit, and the other is the uuid of the group. Similarly, the user applier will push permissions for individual users to Orca based on the statefile compiled by the data collector. A user's permissions will be a summation of their permissions from the groups and their individual permissions","contributing#Contributing":"All source code for this solution can be found in the mce/devsecops-orca repository. Code changes merged to the main branch are automatically deployed using Google Cloud Build. All services required for this solution can be found in the same repository in the deploy folder. Unit tests for the solution are currently a work in progress.","local-testing#Local Testing":"Local testing of the functions can be done using functions-framework. This python package allows you to run both HTTP and event-based functions locally.","solution-faq-for-support-tickets#Solution FAQ for Support Tickets":"Below, you'll find a few canned responses that can be used to answer incoming tickets inquiring about Orca.How do I get access to Orca?\nAccess permissions on Orca are provisioned automatically based on attributes from Hyperscaler Database (HSDB). For initial access to Orca, you will need to request the following automatically approved Cloud Access Manager (CAM) profile: https://spc.ondemand.com/sap/bc/webdynpro/a1sspc/cam_wd_central?item=request&profile=Multicloud_Orca_User. This profile does not provide any permissions in Orca, but only provides the ability to sign into Orca via SSO. You will have to periodically re-request this profile to have continued access to Orca.\n\nUsers with any of the following roles on an account in HSDB will be given Viewer access to that account in Orca: Technical Responsible User, Security Officer, and Additional Users\n\nUsers with any of the following roles on a group in HSDB will be given Viewer access to the accounts under that group in Orca: Administrator, Power User, Editor, and Reader\n\nOnce a user has signed into Orca, their permissions will be automatically provisioned and updated based on the Hyperscaler Account roles and Hyperscaler Group roles mentioned above. Permission updates may take up to 2 hours from your initial sign-in. Permissions will be regularly updated based on the Hyperscaler roles.\nCan you please add me to a specific LOB or account in Orca?\nAccess permissions on Orca are provisioned automatically based on attributes from Hyperscaler Database (HSDB) and are therefore self-managed by lines of business within SAP. For initial access to Orca, you will need to request the following automatically approved Cloud Access Manager (CAM) profile: https://spc.ondemand.com/sap/bc/webdynpro/a1sspc/cam_wd_central?item=request&profile=Multicloud_Orca_User. This profile does not provide any permissions in Orca, but only provides the ability to sign into Orca via SSO. You will have to periodically re-request this profile to have continued access to Orca.\n\nTo gain permissions to view an account or LOB in Orca, please reach out to the Technical Responsible User of the account, or the Administrator of the group which you need access to.\n\nUsers with any of the following roles on an account in HSDB will be given Viewer access to that account in Orca: Technical Responsible User, Security Officer, and Additional Users\n\nUsers with any of the following roles on a group in HSDB will be given Viewer access to the accounts under that group in Orca: Administrator, Power User, Editor, and Reader\n\nOnce a user has signed into Orca, their permissions will be automatically provisioned and updated based on the Hyperscaler Account roles and Hyperscaler Group roles mentioned above. Permission updates may take up to 2 hours from your initial sign-in. Permissions will be regularly updated based on the Hyperscaler roles.\n\nFor access to specific groups, please use the Hyperscaler Groups Application to request reader access to the group or groups which you'd like to have access to.\nWhere can I find more information about Orca?\nMore information can be found on our Orca FAQ workzone page here\nI've requested the required CAM profile, but can't login\nThere are two cases that we've been running into. Once CAM profiles are approved, they can take some time to sync to Active Directory. Please wait an hour and try to login again. If you are still unable to login, it is likely because you do not have any permissions in Hyperscaler Database which would warrant access to Orca. Please see the \"How Can I Get Access to Orca?\" section of our Orca FAQ for more information."}},"/internal/operations/orca/orca_ops_runbook/service_overview":{"title":"Service Overview","data":{"":"Find all Orca-related service overview documentation here!"}},"/internal/operations/orca/orca_ops_runbook/service_overview/Service_Overview":{"title":"Orca Ops Runbook - Service Overview","data":{"":"This is the Orca Ops Runbook (aka Admin guide).","orca-service-overview#Orca Service Overview":"Orca is a SaaS-based, Cloud-Native Application Protection Platform (CNAPP) that scans SAP cloud estates for security risks/compromises and assets. It uses an innovative non-intrusive solution for cloud scanning known as “side-scanning”, which involves taking snapshots of running instances using the cloud provider's native cloud API. This means that cloud assets do not need anything installed for Orca to scan them.","what-is-orca-project#What is Orca project?":"The Orca project involves using Orca to support resource and software asset management, including virtual machines and containers. It is also customised with security compliance scanning controls based on SGS (SAP Global Security) Hardening guidelines to provide centralised reporting on the security compliance posture of SAP cloud accounts.The scope for Orca is all public cloud accounts in SAP. It does not provide coverage for Converged Cloud.","orca-queries-or-issues#Orca Queries or Issues":"For Orca information and FAQ, please refer to Orca Sharepoint: Orca SharepointFor any queries or issues related to Orca, please raise a support ticket: Orca Support Ticket"}},"/internal/operations/orca/orca_ops_runbook/software_deployment_workflow":{"title":"Software Deployment Workflow","data":{"":"Find all Orca-related how to deploy the software documentation here!"}},"/internal/operations/orca/orca_ops_runbook/software_deployment_workflow/orca_control_development_process":{"title":"Orca Ops Runbook - Orca Control Development Process","data":{"":"This documentation will cover the process to build Orca Control for SGS new requirement.","orca-control-development-process#Orca Control Development Process":"To build new control requirement by SGS, the following steps will need to be taken :\nGet the requirements from SGS\nRun relevant Tests\nWrite custom queries which provide us with compliance data for assets within our landscape and tested out with sonar query.\nGet SGS validations and approvals\nWrite the Orca controls using Control Definition Format.\ncontrol name\ncontrol description\nsgs_wiki_link\nquery\nscore\nallow_orca_score_adjustment: false\nenabled: true\ncategory: Best practices\nminerva_control_id: null\n\n\nPR validations and approvals\nControl deployed to Orca Production"}},"/internal/operations/orca/Useful_Orca_Links":{"title":"Useful Orca Links","data":{"":"Below is a list of useful links to Orca-related topics and FAQs.Login to Orca first, select Support -> Documentation (allow pop-ups) before accessing these links.Where/How Does Orca Store My Data?What Data Leaves My Environment?How Does Orca Handle Sensitive Data? (PII and Other Sensitive Information)Onboarding AWS account using TerraformOrca Security - Official Terraform ModulesHow Can I Remove or Delete a Cloud Account from Orca?Off-boarding Orca from Your Cloud AccountHow Often Does Orca Scan?Orca Security Pricing GuideHow To Resolve Common CloudTrail Onboarding IssuesBenefits of Onboarding Kubernetes ClustersOnboarding a Self Managed Kubernetes ClusterList of supported inventory resourcesFile Integrity Monitoring (FIM) OverviewHow Do I Enable File Integrity Monitoring (FIM)?List of Asset Categories and Asset TypesList of Supported Cloud Provider ServicesList of Supported Inventory ResourcesAsset Security LogsBusiness Units in Orca - How to Use the Business Units Feature on the PlatformCloud Risk EncyclopediaHow Does Alert Prioritization/Scoring Work?List of Alert Categories and Alert TypesOrca's Vulnerability Scanning SourcesWhat Operating Systems Does Orca Support?Orca Malware ScanningHow Orca's Cloud Security Solution Detects Weak PasswordsHow Do I Use Filters with Alerts?How Do I Dismiss an Alert?How Does Alert Snoozing Work?Saving and Sharing Customized Views of the Alerts PageOrca's Attack PathsAlert AutomationsCompliance Frameworks Available in OrcaCustom Compliance FrameworksIntegrationsOrca REST API ReferenceUsing Orca API with PostmanRisks DashboardOrca Security Score: An In-depth ExplanationView Reports and Export Data From Orca"}},"/internal/operations/prisma_ops/PrismaKeyRotationDocumentation":{"title":"Prisma Key Rotation Documentation","data":{"":"In GCP > change project to\nsap-devsecops.\n\nNavigate to Secret Manager > look for prisma_onboarder_key.In another tab, in GCP > go to Cloud Functions > click on\nprismacloud_api_updater > click on edit > click on dropdown for\nRuntime, Build and Connection Settings to check prisma_key_version and\nprisma_key_id.\n\nIn another tab, in GCP > go to Cloud Functions > click on\nprismacloud_api_updater > click on Edit > click on dropdown\nfor Runtime, Build and Connection Settings to check\nprisma_key_version and prisma_key_id.Once you've checked the above -- go back to the main Cloud Function\nscreen and click on onboard_account > click on Edit >\nclick on dropdown for Runtime, Build and Connection Settings to check\nprisma_key_version and\nprisma_key_id.\n\n\n\nIn the tab with the Secret Manager open > click on\nprisma_onboarder_key > click on New Version > paste key that\nyou've created in Prisma (in Prisma >\nSettings > Access Keys > Add Access Key and set\nexpiration date*) > click on Add New Version.*If you look at the \"prismacloud_api_updater\" code base, you'll see\nthat aside from deleting expired keys, it also makes sure that the max\nexpiration length is ~90 days so as long as an expiration date is set,\nthe key manager will always make sure that all the access keys are\nwithin compliance.\n\n\n\nBack in the Cloud Functions tab, for the onboard_account function\n> update the prisma_key_version and change the prisma_key_id (which\nis from Prisma; in\nPrisma >\nSettings > Access Keys > The new key you've created > ID)\n> Click on Next > Click on Deploy.\n\n\n\nWait for the onboard_account function to deploy successfully (or not).\n\nOnce successfully deployed, let's move on and update the\nprismacloud_api_updater with the same info.Go to Cloud Functions > click on prismacloud_api_updater >\nclick on Edit > click on dropdown for Runtime, Build and\nConnection Settings > update prisma_key_version and prisma_key_id\nwith the same values you updated the onboard_account function with >\nClick on Next > Click on Deploy.\n\n\n\n\n\nGo to Prisma >\nSettings > Access Keys > Check to see if the expired\nkey(s) are gone from the UI."}},"/internal/operations/prisma_ops":{"title":"Prisma Ops","data":{"":"Find all Prisma Ops related documention here!"}},"/internal/operations/prisma_ops/account_creation_aws":{"title":"Runbook: Cloud Account Creation - AWS","data":{"":"","document-control#Document Control":"The document control section describes the revision history and summary of changes made in the document. It will serve as the version control\nfor the contents of the document.","revision-history#Revision History":"Revision Number Revision Date Summary of Change Changed By\n1.0 04.28.2020 Initial Draft Hyperscaler DevSecOps\n\nOperational Procedures\nSTEP 1 >> Select Settings > Cloud Accounts and click + Add New.\nSTEP 2 >> Select Desired Public Cloud.\nSTEP 3 >> Enter Cloud Account Name > Select Monitor + Next\nSTEP 4 >> Enter External ID > Role ARN + Click Next.\nSTEP 5 >> Search for Required Group > Check Mark the Box + Next\nSTEP 6 >> Final Step is to verify connection + Done","operational-procedures#Operational Procedures":"","step-1--select-settings--cloud-accounts-and-click--add-new#STEP 1 >> Select Settings > Cloud Accounts and click + Add New.":"","step-2--select-desired-public-cloud#STEP 2 >> Select Desired Public Cloud.":"","step-3--enter-cloud-account-name--select-monitor--next#STEP 3 >> Enter Cloud Account Name > Select Monitor + Next":"","step-4--enter-external-id--role-arn--click-next#STEP 4 >> Enter External ID > Role ARN + Click Next.":"","step-5--search-for-required-group--check-mark-the-box--next#STEP 5 >> Search for Required Group > Check Mark the Box + Next":"","step-6--final-step-is-to-verify-connection--done#STEP 6 >> Final Step is to verify connection + Done":"Depending on what services that are enabled you will get different green Check Marks.","troubleshooting#Troubleshooting":"","tbd#TBD":"","about-this-document#About This Document":"This document identifies and details the process to be followed when adding a new user to Prisma Cloud. This document outlines the process for Tier 1 support to complete ticket requests for access.","approval#Approval":"Status: Pending Approval\nReviewed By:\nValidated By:\nApproved By:","document-conventions#Document Conventions":"","abbreviations#Abbreviations":"Abbreviation Definition\nLOB Line of Business","document-history#Document History":"Comment Name Date\nInitial Draft"}},"/internal/operations/prisma_ops/admin_and_processes":{"title":"Administration & Processes","data":{"":"","administration--processes#Administration & Processes":"Administration\nOnboard Cloud Account onto Prisma Monitoring\nOnboard SAP Cloud Account Owner/User onto Prisma Monitoring\nUser Roles and Permission Groups\n\n\nUser Single Sign-On","-administration## Administration":"Prisma Cloud platform is currently administered and managed by SAP Multi-Cloud DevSecOps Team.With respect to current implementations and platform limitations, the following is managed:","-onboard-cloud-account-onto-prisma-monitoring## Onboard Cloud Account onto Prisma Monitoring":"Onboard SAP AWS Public Cloud Accounts\n\n\nIf the cloud account is part of the SAP Multi-Cloud Organization (Part of GCS), it'll be onboard Prisma as part of security services.\nIn case you're unable to find your account in Prisma, please request access through Multi-Cloud Servicedesk by providing the following details:\nCloud Account ID, Name and Type\nCloud Account Owner\nCloud Account associated SAP Team, Line of Business and Board Area\n\n\nIf the cloud accounts are outside SAP Multi-Cloud then you can onboard Prisma via putting in a cloud account onboard request through Multi-Cloud Servicedesk. In order to onboard cloud accounts, the following details are required:\nFor AWS, you’ll have to deploy the necessary AWS role (link below) in your AWS accounts outside HS Organization as done with Evident.IO and provide the following details in a csv or json file:\nName of the cloud account\nRole ARN\nRole External ID\nTeam Name/ Line of Business and Board Area *\n\nCross-Account Prisma Role Template link: https://s3.amazonaws.com/redlock-public/cft/rl-read-only.template\n\n\nOnboard SAP's Azure Cloud Accounts & SAP GCP Cloud Accounts\n\nIf the cloud account is part of the SAP Multi-Cloud Organization (Part of GCS), it'll be onboard Prisma as part of security services.\nIf the cloud accounts are outside SAP Multi-Cloud then you can onboard Prisma via putting in a cloud account onboard request through Multi-Cloud Servicedesk. In order to onboard cloud accounts, please follow the steps described by Prisma Platform using the links below:\nFor Azure: https://docs.paloaltonetworks.com/prisma/prisma-cloud/prisma-cloud-admin/connect-your-cloud-platform-to-prisma-cloud/onboard-your-azure-account#id51ddadea-1bfb-4571-8430-91a1f54673d2\nFor GCP: https://docs.paloaltonetworks.com/prisma/prisma-cloud/prisma-cloud-admin/connect-your-cloud-platform-to-prisma-cloud/onboard-your-gcp-account#id9083908f-b803-4b6d-9ec2-3783cff2180f\nCurrently, Prisma doesn’t support very flexible access control. You cannot give single person access over accounts in 2 different teams. Prisma platform team is expecting to deliver this feature by March 2020. We can then create account segregation by teams and with scoped privileged access eventually explore some remediation capabilities of the tool in a year. For now, we suggest keeping everything under a single team and giving everyone authorized on your team access to it.\n\nOnboard SAP Cloud Account Owner/User onto Prisma Monitoring\nUser Roles and Permission Groups\nUsers are assigned a Role-based on their requirements for visibility to accounts. The process for users is as follows:Each Cloud Account is assigned to an Account Group. For larger visibility, one of the Account Groups here (AG1) includes every account within the LOB. Then, a Role is created that represents the scope of visibility. This visibility is limited based on the Cloud Account within the Account Group that is assigned to the Role. Each Role inherits a Permissions Group. For each user, the correct Role is assigned to them based on their need for visibility of Cloud Accounts. Only one Role may be assigned to each user. By this process, any combination of permissions and visibility may be granted to a user.All users are provisioned as needed. When an account is on-boarded, our process validates if the \"Technical Owner\" is already in the Prisma system. Each account is on-boarded to a \"Team\" Account Group as described below in Account Concepts.\" Each technical owner serves to become the \"authority\" in approving users to be added to an Account Group Role as described above. Any number of users may be added with any level of visibility so long as the visibility can be defined around accounts.User Single Sign-On\nAs Cloud Accounts are on-boarded, the associated Technical Owners get provisioned an account within Prisma. Prisma does not support \"Just In Time\" user accounts (currently out of scope for our project). Each user is pre-provisioned and assigned the appropriate Role. When the user attempts to sign-in, they are redirected to SAP IDP to authenticate and then returned to the Prisma portal.Locally created user accounts are provisioned on an as-needed basis only.After you have requested user access through the Multi-Cloud Servicedesk and the provided the necessary details, you can login to PrismaCloud via SSO using the following URL:http://prisma.tools.sap"}},"/internal/operations/prisma_ops/azure_delete_subscription":{"title":"Runbook: Deleting an Azure Subscription","data":{"":"","document-control#Document Control":"The document control section describes the revision history and summary of changes made in the document. It will serve as the version control\nfor the contents of the document.","revision-history#Revision History":"Revision Number Revision Date Summary of Change Changed By\n1.0 05.04.2020 Initial Draft Hyperscaler DevSecOps","table-of-contents#Table of Contents":"Runbook: Deleting an Azure Subscription\nDocument Control\nRevision History\nTable of Contents\nOperational Procedures: Deleting an Azure Subscription\nSTEP 1 >> Select Overview > click + Cancel Subscription\nSTEP 2 >> Select Verify Your Subscription Name > Paste Subscription Name >\nSTEP 3 >> Reason for Cancellation Dropdown > Select Reasoning\nSTEP 4 >> Enter a Brief Reason > click + Cancel Subscription\nSTEP 5 >> Verify Subscription Cancellation > Click Notifications\n\n\nTroubleshooting\nTBD\n\n\nAbout This Document\nApproval\nDocument Conventions\nAbbreviations\nDocument History","operational-procedures-deleting-an-azure-subscription#Operational Procedures: Deleting an Azure Subscription":"","step-1--select-overview--click--cancel-subscription#STEP 1 >> Select Overview > click + Cancel Subscription":"","step-2--select-verify-your-subscription-name--paste-subscription-name-#STEP 2 >> Select Verify Your Subscription Name > Paste Subscription Name >":"","step-3--reason-for-cancellation-dropdown--select-reasoning#STEP 3 >> Reason for Cancellation Dropdown > Select Reasoning":"","step-4--enter-a-brief-reason--click--cancel-subscription#STEP 4 >> Enter a Brief Reason > click + Cancel Subscription":"","step-5--verify-subscription-cancellation--click-notifications#STEP 5 >> Verify Subscription Cancellation > Click Notifications":"","troubleshooting#Troubleshooting":"","tbd#TBD":"","about-this-document#About This Document":"This document identifies and details the process to be followed when adding a new user to Prisma Cloud. This document outlines the process for Tier 1 support to complete ticket requests for access.","approval#Approval":"Status: Pending Approval\nReviewed By:\nValidated By:\nApproved By:","document-conventions#Document Conventions":"","abbreviations#Abbreviations":"Abbreviation Definition\nLOB Line of Business","document-history#Document History":"Comment Name Date\nInitial Draft"}},"/internal/operations/prisma_ops/adding_users_to_multiple_groups":{"title":"Runbook: Adding Users to Multiple Groups","data":{"":"","document-control#Document Control":"The document control section describes the revision history and summary of changes made in the document. It will serve as the version control\nfor the contents of the document.","revision-history#Revision History":"Revision Number Revision Date Summary of Change Changed By\n1.0 05.04.2020 Initial Draft MultiCloud DevSecOps","table-of-contents#Table of Contents":"Runbook: Adding Users to Multiple Groups\nDocument Control\nRevision History\nOperational Procedures: Adding Users to Multiple Groups\nSTEP 1 >> Select Settings > Users and click + Add New\nSTEP 2 >> Enter First Name, Last Name, Email > Assign Roles Dropdown and click Search Bar\nSTEP 3 >> Click on Your Name\nSTEP 4 >> Click on your active Group Membership\nSTEP 5 >> Select the group you want to become your active group.\n\n\nTroubleshooting\nTBD\n\n\nAbout This Document\nApproval\nDocument Conventions\nAbbreviations\nDocument History","operational-procedures-adding-users-to-multiple-groups#Operational Procedures: Adding Users to Multiple Groups":"","step-1--select-settings--users-and-click--add-new#STEP 1 >> Select Settings > Users and click + Add New":"","step-2--enter-first-name-last-name-email--assign-roles-dropdown-and-click-search-bar#STEP 2 >> Enter First Name, Last Name, Email > Assign Roles Dropdown and click Search Bar":"","step-3--click-on-your-name#STEP 3 >> Click on Your Name":"","step-4--click-on-your-active-group-membership#STEP 4 >> Click on your active Group Membership":"","step-5--select-the-group-you-want-to-become-your-active-group#STEP 5 >> Select the group you want to become your active group.":"","troubleshooting#Troubleshooting":"","tbd#TBD":"","about-this-document#About This Document":"This document identifies and details the process to be followed when adding a new user to Prisma Cloud. This document outlines the process for Tier 1 support to complete ticket requests for access.","approval#Approval":"Status: Pending Approval\nReviewed By:\nValidated By:\nApproved By:","document-conventions#Document Conventions":"","abbreviations#Abbreviations":"Abbreviation Definition\nLOB Line of Business","document-history#Document History":"Comment Name Date\nInitial Draft"}},"/internal/operations/prisma_ops/deleting_accounts":{"title":"Runbook: Deleting Cloud Accounts, Subscriptions, and Projects","data":{"":"","document-control#Document Control":"The document control section describes the revision history and summary of changes made in the document. It will serve as the version control\nfor the contents of the document.","revision-history#Revision History":"Revision Number Revision Date Summary of Change Changed By\n1.0 05.04.2020 Initial Draft Hyperscaler DevSecOps","table-of-contents#Table of Contents":"Operational Procedures: Amazon Web Services\nSTEP 1 >> Select Settings > Cloud Accounts and click + Search Box\nSTEP 2 >> Select Settings > Search Box + Account Number\nSTEP 3 >> Verify the Status is a RED X.\nSTEP 4 >> Hover over the Garbage Can and Click Delete\nSTEP 5 >> Click Confirm\n\n\nOperational Procedures: Microsoft Azure\nSTEP 1 >> Select Settings > Cloud Accounts and click + Search Box\nSTEP 2 >> Select Settings > Search Box + Subscription ID\nSTEP 3 >> Verify the Status is a RED X.\nSTEP 4 >> Click the Garbage Can and click Delete\nSTEP 5 >> and Click Confirm\n\n\nOperational Procedures: Google Cloud Platform -- No Action Required","operational-procedures-amazon-web-services#Operational Procedures: Amazon Web Services":"","step-1--select-settings--cloud-accounts-and-click--search-box#STEP 1 >> Select Settings > Cloud Accounts and click + Search Box":"In the upper right you will find the search box after clicking Modern Table (beta)","step-2--select-settings--search-box--account-number#STEP 2 >> Select Settings > Search Box + Account Number":"","step-3--verify-the-status-is-a-red-x#STEP 3 >> Verify the Status is a RED X.":"If the Status is any other color please refer the ticket back to L1 SAP for AWS deletion.","step-4--hover-over-the-garbage-can-and-click-delete#STEP 4 >> Hover over the Garbage Can and Click Delete":"","step-5--click-confirm#STEP 5 >> Click Confirm":"","operational-procedures-microsoft-azure#Operational Procedures: Microsoft Azure":"","step-1--select-settings--cloud-accounts-and-click--search-box-1#STEP 1 >> Select Settings > Cloud Accounts and click + Search Box":"In the upper right you will find the search box after clicking Modern Table (beta)","step-2--select-settings--search-box--subscription-id#STEP 2 >> Select Settings > Search Box + Subscription ID":"","step-3--verify-the-status-is-a-red-x-1#STEP 3 >> Verify the Status is a RED X.":"If the Status is any other color please refer the ticket back to L1 SAP for AWS deletion.","step-4--click-the-garbage-can-and-click-delete#STEP 4 >> Click the Garbage Can and click Delete":"","step-5--and-click-confirm#STEP 5 >> and Click Confirm":"","operational-procedures-google-cloud-platform----no-action-required#Operational Procedures: Google Cloud Platform -- No Action Required":"","troubleshooting#Troubleshooting":"","tbd#TBD":"","about-this-document#About This Document":"This document identifies and details the process to be followed when adding a new user to Prisma Cloud. This document outlines the process for Tier 1 support to complete ticket requests for access.","approval#Approval":"Status: Pending Approval\nReviewed By:\nValidated By:\nApproved By:","document-conventions#Document Conventions":"","abbreviations#Abbreviations":"Abbreviation Definition\nLOB Line of Business","document-history#Document History":"Comment Name Date\nInitial Draft"}},"/internal/operations/prisma_ops/email_notifications_of_alerts":{"title":"Runbook: Email Alerts","data":{"":"","document-control#Document Control":"The document control section describes the revision history and summary of changes made in the document. It will serve as the version control\nfor the contents of the document.","revision-history#Revision History":"Revision Number Revision Date Summary of Change Changed By\n1.0 05.04.2020 Initial Draft MultiCloud DevSecOps","table-of-contents#Table of Contents":"","operational-procedures-deleting-an-azure-subscription#Operational Procedures: Deleting an Azure Subscription":"","step-1--select-alerts--select-alert-rules--click--add-new#STEP 1 >> Select Alerts > Select Alert Rules > click + Add New":"","step-2--enter-alert-rule-name--enter-alert-description--click--next#STEP 2 >> Enter Alert Rule Name > Enter Alert Description > Click + Next":"For the description add any relevent information you would like.","step-3--0-account-groups-dropdown--select-account-groups#STEP 3 >> 0 Account Groups Dropdown > Select Account Groups":"","step-4--click--check-box-with-your-desired-group--click--next#STEP 4 >> Click + Check Box with your desired group > click + Next":"","step-5--verify-subscription-cancellation--click-notifications#STEP 5 >> Verify Subscription Cancellation > Click Notifications":"","step-5--select-the-filter-dropdown-icon--click-the-desired-severity-level#STEP 5 >> Select the Filter Dropdown Icon > Click the desired Severity Level":"","step-6--scroll-to-the-very-bottom-of-the-page--click-the-page-dropdown--select-100page#STEP 6 >> Scroll to the very bottom of the page > Click the Page Dropdown > Select 100/Page":"","step-7--at-the-top-of-the-page--click-the-policy-name-checkbox#STEP 7 >> At the Top of the Page > Click the Policy Name Checkbox":"This will select all the policies on this page. It will NOT select all the alerts.Filter on the Severity level of alerts you want to have emailed to you.Choose any other filter by the same dropdown method.","troubleshooting#Troubleshooting":"","tbd#TBD":"","about-this-document#About This Document":"This document identifies and details the process to be followed when adding a new user to Prisma Cloud. This document outlines the process for Tier 1 support to complete ticket requests for access.","approval#Approval":"Status: Pending Approval\nReviewed By:\nValidated By:\nApproved By:","document-conventions#Document Conventions":"","abbreviations#Abbreviations":"Abbreviation Definition\nLOB Line of Business","document-history#Document History":"Comment Name Date\nInitial Draft"}},"/internal/operations/prisma_ops/dismissing_alerts":{"title":"Runbook: Alert Dismissal and Policy Snooze/Dismissal","data":{"":"","document-control#Document Control":"The document control section describes the revision history and summary of changes made in the document. It will serve as the version control\nfor the contents of the document.","revision-history#Revision History":"Revision Number Revision Date Summary of Change Changed By\n1.0 05.04.2020 Initial Draft Hyperscaler DevSecOps\n\nPolicy\nRequired Information\nAlert Types\nFalse Alert > Suspicious Login\nFalse Policy Trigger > Open TCP Ports and Services\n\n\nOperational Procedures: Dismissing an Incident Alert\nSTEP 1 >> Select Settings > Alert ID and click + Search Box\nSTEP 2 >> Enter your Alert ID > and click the Policy Name\nSTEP 3 >>\n\n\nOperational Procedures: Dismissing a Policy Alert\nSTEP 1 >> Select Settings > Alert ID and click + Search Box","policy#Policy":"No alerts or policies shall be dismissed without prior authorization from SGS.\nRefer requests to these instructions: SGS Exception Request DocumentationSnooze vs Dismissal","required-information#Required Information":"Alert ID: P-xxxxxxxxCloud Account Number:\nAWS 12-digit Numeric Account Number: xxxxxxxxxxxx\nAzure 32 AlphaNumeric Subscription ID: xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\nGCP 12-digit Numeric Project Number: xxxxxxxxxxxx\nAccount Name:Account Owner:Account Security Contact:Requester Name:Line of Business:Example Alert Number:","precheck-authorization-conditions#Precheck Authorization Conditions":"Verify it is a Service Desk Request inside the Jira Ticket.Click on the link inside below the requestNote the exception period for the alert.Ticket MUST be approved by SGS","alert-types#Alert Types":"","false-alert--suspicious-login#False Alert > Suspicious Login":"* Perform an [Arin Lookup](https://arin.net) on the IP addresses in Question.\n- Verify the IPs are in line with the request.\n*","false-policy-trigger--open-tcp-ports-and-services#False Policy Trigger > Open TCP Ports and Services":"","operational-procedures-dismissing-an-incident-alert#Operational Procedures: Dismissing an Incident Alert":"","step-1--select-settings--alert-id-and-click--search-box#STEP 1 >> Select Settings > Alert ID and click + Search Box":"","step-2--enter-your-alert-id--and-click-the-policy-name#STEP 2 >> Enter your Alert ID > and click the Policy Name":"","step-3-#STEP 3 >>":"","operational-procedures-dismissing-a-policy-alert#Operational Procedures: Dismissing a Policy Alert":"","step-1--select-settings--alert-id-and-click--search-box-1#STEP 1 >> Select Settings > Alert ID and click + Search Box":"","troubleshooting#Troubleshooting":"","tbd#TBD":"","about-this-document#About This Document":"This document identifies and details the process to be followed when adding a new user to Prisma Cloud. This document outlines the process for Tier 1 support to complete ticket requests for access.","approval#Approval":"Status: Pending Approval\nReviewed By:\nValidated By:\nApproved By:","document-conventions#Document Conventions":"","abbreviations#Abbreviations":"Abbreviation Definition\nLOB Line of Business","document-history#Document History":"Comment Name Date\nInitial Draft"}},"/internal/operations/prisma_ops/new_cloud_account":{"title":"Onboard Cloud Account onto Prisma Monitoring","data":{"":"","-onboard-cloud-account-onto-prisma-monitoring## Onboard Cloud Account onto Prisma Monitoring":"Onboard SAP AWS Public Cloud Accounts\n\n\nIf the cloud account is part of the SAP Multi-Cloud Organization (Part of GCS), it'll be onboard Prisma as part of security services.\nIn case you're unable to find your account in Prisma, please request access through Multi-Cloud Servicedesk by providing the following details:\nCloud Account ID, Name and Type\nCloud Account Owner\nCloud Account associated SAP Team, Line of Business and Board Area\n\n\nIf the cloud accounts are outside SAP Multi-Cloud then you can onboard Prisma via putting in a cloud account onboard request through Multi-Cloud Servicedesk. In order to onboard cloud accounts, the following details are required:\nFor AWS, you’ll have to deploy the necessary AWS role (link below) in your AWS accounts outside MC Organization as done with Evident.IO and provide the following details in a csv or json file:\nName of the cloud account\nRole ARN\nRole External ID\nTeam Name/ Line of Business and Board Area *\n\nCross-Account Prisma Role Template link: https://s3.amazonaws.com/redlock-public/cft/rl-read-only.template\n\n\nOnboard SAP's Azure Cloud Accounts & SAP GCP Cloud Accounts\n\nIf the cloud account is part of the SAP Multi-Cloud Organization (Part of GCS), it'll be onboard Prisma as part of security services.\nIf the cloud accounts are outside SAP Multi-Cloud then you can onboard Prisma via putting in a cloud account onboard request through Multi-Cloud Servicedesk. In order to onboard cloud accounts, please follow the steps described by Prisma Platform using the links below:\nFor Azure: https://docs.paloaltonetworks.com/prisma/prisma-cloud/prisma-cloud-admin/connect-your-cloud-platform-to-prisma-cloud/onboard-your-azure-account#id51ddadea-1bfb-4571-8430-91a1f54673d2\nFor GCP: https://docs.paloaltonetworks.com/prisma/prisma-cloud/prisma-cloud-admin/connect-your-cloud-platform-to-prisma-cloud/onboard-your-gcp-account#id9083908f-b803-4b6d-9ec2-3783cff2180f"}},"/internal/operations/prisma_ops/policies_and_exception_handling":{"title":"Policies and Exception Handling","data":{"":"Find all policy and exception handling related documention here!"}},"/internal/operations/prisma_ops/new_user_account":{"title":"Runbook: Adding New Users to Prisma Cloud","data":{"":"","document-control#Document Control":"The document control section describes the revision history and summary of changes made in the document. It will serve as the version control\nfor the contents of the document.","revision-history#Revision History":"Revision Number Revision Date Summary of Change Changed By\n1.0 04.28.2020 Initial Draft Hyperscaler DevSecOps\n\nSTEP 1 >> Select Settings > Users and click + Add New.\nSTEP 2 >> Enter First Name, Last Name, and Email of the user.\nSTEP 3 >> Assign a Role to the user.\nSTEP 4 >> Specify a Time Zone for the user and click Save.\nSTEP 5 >> Decide whether to enable API Access.\nSTEP 6 >> After you add a user, you can edit or delete the user.","operational-procedures#Operational Procedures":"","step-1--select-settings--users-and-click--add-new#STEP 1 >> Select Settings > Users and click + Add New.":"","step-2--enter-first-name-last-name-and-email-of-the-user#STEP 2 >> Enter First Name, Last Name, and Email of the user.":"","step-3--assign-a-role-to-the-user#STEP 3 >> Assign a Role to the user.":"Prisma Cloud Administrator Roles can be System Admin, Account Admin, Account Read Only, and Cloud Provisioning Admin.\nEach Line of Business (LOB) has a unique role assignment.\nDefault setting should be LOB Read-Only.\nAlert Creation requires the LOB Power-User Role.","step-4--specify-a-time-zone-for-the-user-and-click-save#STEP 4 >> Specify a Time Zone for the user and click Save.":"Lookup user in the User Directory\nSet User Time to the indicated timezone.","step-5--decide-whether-to-enable-api-access#STEP 5 >> Decide whether to enable API Access.":"To add API rights search for the user and click on their email address.\nOnce the user is opened just check mark the box that says API access.","step-6--after-you-add-a-user-you-can-edit-or-delete-the-user#STEP 6 >> After you add a user, you can edit or delete the user.":"To edit the details of an user, click the record and change any details.\nTo disable an user, toggle the Status of the user.","troubleshooting#Troubleshooting":"","tbd#TBD":"","about-this-document#About This Document":"This document identifies and details the process to be followed when adding a new user to Prisma Cloud. This document outlines the process for Tier 1 support to complete ticket requests for access.","approval#Approval":"Status: Pending Approval\nReviewed By:\nValidated By:\nApproved By:","document-conventions#Document Conventions":"","abbreviations#Abbreviations":"Abbreviation Definition\nLOB Line of Business","document-history#Document History":"Comment Name Date\nInitial Draft"}},"/internal/operations/prisma_ops/policies_and_exception_handling/policy_management":{"title":"Policy Management","data":{"":"Status RELEASED\nDocument owner Herre, Thorsten\nDocument substitute Erler, Andre\nVersion^28\nLast Modified 16.04.\n\nPolicy Review & Maintenance\nPolicy Change Request\n2.1. Difference between Policy Exception handling and Policy Change Process\n2.2. Change request for HIGH polices\n2.3. Change request for LOW and MEDIUM policies\nCustom SAP Prisma Policies","1-policy-review--maintenance#1. Policy Review & Maintenance":"The following chapter uses Prisma specific terms like 'Standard', 'Policy' and 'Alert Rule'. Please refer to\nthe Prisma Public Cloud End User Guide for the explanation of these terms in the Definitions section.\n\nPolicy reviews, policy updates and policy maintenance are done by SGS Security Architecture team the\nfollowing way:\n\nPolicies changes or new custom policies will run through different Standards in Prisma:\nSAP Global Security Research Standard\n\nThis Standard will be used for special policy development and research by\nSGS.\nThis Standard has it's own Alert Rule, which only includes the 3 hyperscaler\naccounts owned by SGS Defensive Architecture.\nThe Alerts of this Standard cannot be seen by any LoB, but only the SGS\nDefensive Architecture team.\nThis Standard is not used for the SCD weekly reporting.\nSAP Global Security Test Standard\nThis Standard will be used to test Policy Changes or new Custom Policies\nacross all Hyperscalers.\nAll Policies that are added to this Standard will get the \"[TEST]\" Prefix.\n\ne.g. [TEST] GCP IAM service account with owner/editor privileges.\nThe Test Standard is applied to all SAP hyperscaler accounts.\nTherefore, the new TEST Standard will get its own Alert Rule: SGS Test Alert\nRule.\n\nThis also means that every LoB sees the alerts produced by this\nStandard.\nThese alerts will be marked with the [TEST] Policyprefix and it is\nhighly appreciated to provide feedback to SGS.\nThis Standard will not be included in the SCD weekly reporting.\nAlerts produced by this Standard do not need to be fixed by the LoB.\nSAP Global Security Hyperscaler Productive Standards:\n\nSAP Global Security AWS Standard\nSAP Global Security Azure Standard\nSAP Global Security GCP Standard\n\nThese 3 Standards are the foundation for the alerts in Prisma. Every\nLoB must apply to the policies within these Standards.\nEach Standard has it's own Alert Rule.\nThis Standard consists of a subset of all policies inside the Prisma\ntool. This subset represents the important and necessary policies\nfrom a security point of view including default and custom policies.\nThese 3 Standards must be used for the SCD weekly reporting.\nAlerts of this Standard must be fixed by the LoB.\n\nPolicy Changes include:\n\nRelated Documents and Links\n\nno related documents","2#2.":"","4#4.":"Activation or deactivation of policies\nChanges to the query of a policy\nChanges to the severity of a policy\nCreation or deletion of policies\n\nApproach:\n\nPHASE 1: When a new custom policy gets created or an existing policy needs to be\nadjusted, this policy will be added to the SAP Global Security Research Standard.\n\nThe policy will be tested and validated until the alert shows exactly what it is\nsupposed to.\nPHASE 2: If the policy is validated, it will be moved from the Research Standard to the\nTest Standard.\nThe Test Standard ensures that the policy works if applied to all SAP\nhyperscaler accounts.\nA Policy will remain in the Test Standard for 1 month.\nThis month can be used by the LoB to provide feedback to SGS.\nAt the end of the month, the policy can be moved back to the Research\nStandard, if further adjustments are necessary.\nPHASE 3: After being validated in the Test Standard, the policy can be moved to the\nrespective productive Standard within the next Prisma Update Cycle.\n\nThe Prisma Update Cycle is 1 month.\nThis includes updates from SGS side or any requested policy changes by the LoB.\nIncomming requests by the LoB are firstly reviewed by SGS, and need to be\nconfirmed to be processed further.\nAll updates are collected in a special JIRA issue in the \"Prisma - Hyperscaler Security\n(REDROLL)\" project.\nThis JIRA issue is named as follows: \"Prisma Policy Changes [Month]\"\nAll Prisma policy changes are documented as comments in this issue.\nAfter 1 month, the issue will be closed and a new issue will be created for the\nnext month.\nIn the third week, the Hyperscaler team is informed about the changes, to\nenable communication of changes to the LoBs.\nAfter the communication of the changes has happened, the policies are updated.\n## 2. Policy Change Request\n\nPrisma policies are in general aligned with the SAP Security standards and security procedure wikis for\nthe various Hyperscalers, which fits most corporate or cloud use cases within SAP. The Global Security\nTeam ensures that the policies work as intended and reflect our security rules and criticality ratings. In\ncase the per default delivered PRISMA policies by the vendor Palo Alto Network do not meet these\nrequirements, we may customize, extend, change the rating or even deactivate the policy in the tool.\nIn case the SAP LoB’s or Cloud units working with Prisma detect that the policies do not work as\nintended or need to be adjusted to reflect additional SAP business cases to prevent broad false positives,\nwe have defined a change request process to allow such requests targeting either Global Security or\nbeing forwarded to the vendor Palo Alto for general fixing.\nSAP global Security has for that reason a dedicated SecJira PRISMA Project created to collect these\nrequests (jira issues). The vendor Palo Alto Network has also access to this Jira to work on those feature\nrequests or policy change requests.\n\n### 2.1. Difference between Policy Exception handling and Policy\n\n### Change Process\n\nIt is important the realize which process needs to be used for which kind of Prisma policy issue or\nrequest. The following definition will give guidance to the teams when to trigger the Exception handling\nprocess outlined in the corresponding wiki page and when to use the overall policy change request\nprocess as defined in this document.\n\nThe Prisma Exception Handling Process is used if:\n\nYou don’t want to change the actual Prisma Policy/Check because it works as intended.\nYou want instead to exclude or ignore it in the Prisma alerting only for your accounts / account\ngroup.\nYou think the policy and corresponding security configuration (therefore Security Procedure Wiki\nrequirement) is not applicable for your business\nYou do not want to request an policy exception for the whole SAP Company.\nThe Prisma Policy Change Request is triggered if:\n\nYou think the Prisma Policy is not working properly; does provide wrong results;\nYou need to change the policy for the whole company to reflect certain SAP specific conditions\nand exclusions\nYou need a completely new check/policy that is not existing yet\nYou want to delete the whole policy for the company SAP or at least change the policy rating.\n\n### 2.2. Change request for HIGH polices\n\nIn case the affected Prisma policy is rated HIGH in the Prisma tool, the following steps need to be\napplied:\n\nUse the feedback button on this wiki page to request the Prisma related policy change via email.\nPlease be specific which Prisma policy needs to be changed / adjusted.\nThe request will be assigned to SAP Global Security CAB (Change Approval Board) to decide if\nthis change will be implemented and does not violate corporate polices or generates additional\nrisks for the company. SAP Global Security may modify the change request to address this risk.\nIf the change is approved, the change request is assigned to the Prisma vendor to modify or\ncreate a prisma policy\nIf the change is not approved, feedback & reasoning is given to the requestor.\n### 2.3. Change request for LOW and MEDIUM policies\n\nIn case the affected Prisma policy is rated MEDIUM or LOW in the Prisma tool, the following steps need\nto be applied:\n\nUse the feedback button on this wiki page to request the Prisma related policy change via email.\nPlease be specific which Prisma policy needs to be changed / adjusted.\nIf the change related to change in policy rating: The request will be assigned to SAP Global\nSecurity CAB (Change Approval Board) to decide and implement.\nIf the change related to change in policy behavior: The request will be assigned to Prisma\nvendor directly for implementation.\n## 3. Custom SAP Prisma Policies\n\nSAP Global Security may change the default prisma policies/checks provided by the vendor Palo Alto\nNetworks if the check does not reflect the settings outlined and required by the SAP IT security standard,\nthe SAP Hyperscaler reference architecture or the various Hyperscaler specific security procedure. For\nexample the default password policy checks provided by Palo Alto Network do not reflect the password\ncomplexity or validity settings of the SAP corporate password policy as defined in the SAP Global\nSecurity Policy and standards. In such cases the default policies by Palo Alto are copied and modified by\nSAP Global Security team. The name of the modified policy is changed by appending \"SAP:\" in front of\nthe policy title. Therefore all checks in Prisma with the syntax \"SAP:<policy name>\" were modified by\nSAP Global security.\n\nIf a Prisma user or LoB/Cloud team thinks that those modifications are either incomplete, incorrect\naccording to the SAP global policies or ineffective to achieve the desired goals, the team may open a\nchange request as desribed in chapter 2 of this wiki page.\n\n\n\nThis is a offline tool, your data stays locally and is not send to any server!\nFeedback & Bug Reports"}},"/internal/operations/prisma_ops/policies_and_exception_handling/exception_handling_process":{"title":"Exception Handling Process","data":{"":"Status RELEASED\n\nDocument owner Herre, Thorsten\n\nDocument substitute Erler, Andre\n\nVersion 6\n\nLast Modified 01.12.2021","table-of-contents#Table of Contents":"Introduction\n1.1 Definitions\n1.2 How to use the Exception Process\n\n\nExceptions for preventive & detective hyperscaler security controls\n2.1 How to request an Exception\n2.2 SGS Review (risk & impact assessment)\n2.3 Self-tagging Exceptions for a subset of controls\n2.3.1 Affected HIGH controls\n2.3.2 Affected preventive controls (Secure-by-default controls)\n\n\n2.3.3 How to implement the tags\n2.3.3.1 AWS\n2.3.3.2 MS Azure\n2.3.3.3 GCP\n2.3.3.4 AliCloud","1-introduction#1. Introduction":"","11-definitions#1.1 Definitions":"SAP uses so called \"Detective Controls\" to monitor the security configuration of ALL SAP Hyperscaler Accounts (e.g. Aws, GCP, Microsoft Azure & Alibaba Cloud) towards the SAP Hyperscaler Security Reference Architecture and configuration standards. These security configuration requirements are defined by SAP Global Security and published in the Security Policy Framework / Wiki.The detective controls are divided by their ratings (HIGH, MEDIUM, LOW), which reflect the SAP Global Security mandatory baseline and additional requirements for critical cloud business:\nHIGH rated control = MUST / Baseline requirement in Security Wiki\nMEDIUM rated control = SHOULD requirement in Security Wiki\nLOW rated control = optional requirements in the Security wiki or even not mentioned.\n\nFurthermore to ensure a Secure-by-Default baseline throughout ALL SAP Hyperscaler Accounts, a policy-as-code deployment (Secure-by-Default controls) will be enforced centrally by the Hyperscaler team. Target is, to get all SAP Hyperscaler Accounts on the same security level / denominator, independent which hyperscaler platform is used (AWS, GCP, MS Azure & Alibaba Cloud). These controls are mapped to their corresponding HIGH detective controls and act as a \"preventive control\" for SAP hyperscaler accounts.In general, Secure-by-Default Controls also known as preventive controls MUST be fulfilled by each LoB, independent if the hyperscaler scenario is considered as dev, test, \"crash & burn\" or as a productive hyperscaler scenario. The overall target with the enforcement of the Secure-by-Default Controls is, to setup a secure baseline across all SAP cloud environments.There might be critical technical or business reasons to delay the enablement of a Secure-by-Default control in a Cloud Area. In such limited cases there is the possibility to request an exception.This wiki defines HOW non-compliance / findings are managed between:\nthe Hyperscaler Account Owners (e.g. SAP LoBs & CBGs),\nthe SAP Hyperscaler (HSO) Team and SAP Global Security (SGS).","12-how-to-use-the-exception-process#1.2 How to use the Exception Process":"It is not required to open a exception ticket for every alert, but tickets should be opened for groups of alerts. As an example, a ticket is opened for an exception on the following detective control: \" SAP: 1.10.13 - AWS S3 buckets are accessible to public\". The exception request then covers all the alerts regarding public S3 buckets for the account or account group. This also implies that exceptions are granted on account level or account group level.In addition to that, it is necessary to request exceptions for HIGH (detective & preventive) controls as well as MEDIUM (detective) controls. Regardless of the control severity, all HIGH and MEDIUM controls SHOULD be fixed in the first place, and exception SHOULD only be requested, if it's the only remaining option.\nHIGH are reflected by MUST requirements in the SAP Security Policy Framework and are mandatory requirements to implement. In the SGS Review (2.2), these controls refer in most cases to a HIGH risk.\nMEDIUM are reflected by SHOULD requirements in the SAP Security Policy Framework. These state a requirement which under certain circumstances may not be implementable, but it is required though that if a requirement is not implemented, a valid reason for this exists and implications are understood. In the SGS Review (2.2), these controls refer in most cases to a MEDIUM risk.\nLOW controls are reflected by OPTIONAL requirements and MUST NOT be submitted via exception Process. LoB can decide by their own if they want to comply to this requirement or not.","2-exceptions-for-preventive--detective-hyperscaler-security-controls#2. Exceptions for preventive & detective hyperscaler security controls":"","21-how-to-request-an-exception#2.1 How to request an Exception":"The exception MUST be submitted in the SAP Global Security(SGS) -Security Exception Management Process (SEMP).Exception Request{: .btn .btn-purple .mx-auto }Make sure that every parameter mentioned in chapter 2.2 is filled out appropriate. Especially the REASON for the exception MUST be described in a comprehensive & understandable way, to finally make the evaluation of the exception faster and effective.A detailed description of the process steps and a workflow chart, as well as an explanation for the roles & responsibilities and guidance on how to request the exception can be found on the SEMP Wiki page.SGS SEMP{: .btn .btn-blue .v-align-middle }Emergency Request: Disruption of productive business\nIn case an exception for a detective or preventive control is needed AND the SGS Topic Owner and his/her substitutes are not available AND major business disruption could be caused, the Hyperscaler Ops team is empowered to grant an exception for 7 days without the acknowledgement from SGS. The LoB may then raise an Incident directly on the Multi Cloud Service Now for implementation of the exception:GCS SNOW{: .btn .m-auto }Furthermore, if the expiration of an exception disrupts productive cloud business, the HSO team can extend the validity of the exception for four weeks. In this period the issue has to be fixed.","22-sgs-review-risk--impact-assessment#2.2 SGS Review (risk & impact assessment)":"SGS DA Team functions as the SGS Topic Manager and is responsible for providing the SGS Review of the exception request. With the result, the LoB together with the Business Unit Security Officer are asked to proceed in the Business Unit Review.For the Exception Acknowledgement the Response Plan MUST be documented and approved by the Business Unit Security Officer.To facilitate the SGS Review, the following pre-requisites needs to be added to the Jira exception request:\nDetective/Preventive control name:\n< specifiy control name >\nHyperscaler Account name:\n< specify Hyperscaler account/project/subscription name >\nL1 Unit & Affected LoB: < specify L1 Unit & LoB >\nREASON FOR EXCEPTION:\n< specify: e.g. false positive or not applicable because... >\n\nOnly with the filled out parameters, the SGS Review can be processed by SGS DAD team. SGS DA decides based on the provided reason, what kind of criticality the Impact level will have.IMPORTANT: The result of the SGS Review does not mean, that an exception is approved. The SGS DAD team DOES NOT approve an exception overall!Detailed information can be found here: Exception Handling ProcessThe Exeption handling Jira items will be processed by SGS DA team. Jira items will be processed/evaluated IRT = 3 days (IRT = Initial Reaction Time).Exceptions are implemented by the SAP Hyperscaler team for the respective Account/Project/Subscription after the Impact Assessment result is provided by SGS DA team.","23-self-tagging-exceptions-for-a-subset-of-controls#2.3 Self-tagging Exceptions for a subset of controls":"","231-affected-high-controls#2.3.1 Affected HIGH controls":"For some special HIGH controls, the LoB is empowered to implement the exception themselves, after the LoB went through the exception process.The following controls have this feature enabled:\nAmazon Web Services\nSAP: 1.40.01.05 - AWS Security Group does not restrict traffic to Telnet port from internet\nSAP: 1.40.01.04 - AWS Security Group does not restrict traffic to File sharing ports from internet\nSAP: 1.40.01.03 - AWS Security Group does not restrict traffic to Infrastructure ports from internet\nSAP: 1.40.01.02 - AWS Security Group does not restrict traffic to Admin ports from internet\nSAP: 1.40.01.01 - AWS Security Group does not restrict traffic to DB ports from internet\nSAP: 1.10.13 - AWS S3 buckets are accessible to public\nSAP: 1.10.13 - AWS S3 Bucket has Global GET Permissions enabled via bucket policy\nSAP: 1.60.06 - AWS Amazon Machine Image (AMI) is publicly accessible\nSAP: 1.50.01 - AWS EBS volumes are not encrypted\nSAP: 1.50.02 - AWS S3 buckets do not have server side encryption\n\n\nGoogle Cloud Platform\nSAP: 1.40.01.05 - GCP Firewall rule with Inbound traffic to Telnet port from the internet\nSAP: 1.40.01.04 - GCP Firewall rule with Inbound traffic to blocklisted File sharing ports from internet\nSAP: 1.40.01.03 - GCP Firewall rule with Inbound traffic to blocklisted Infrastructure ports from internet\nSAP: 1.40.01.02 - GCP Firewall rule with Inbound traffic to blocklisted Admin ports from internet\nSAP: 1.40.01.01 - GCP Firewall rule with Inbound traffic to blocklisted DB ports from internet\nSAP: 1.40.07 - GCP Storage buckets are publicly accessible to all users\n\n\nMicrosoft Azure\nSAP: 1.40.01.05 - MS Azure Network Security Group does not restrict traffic to Telnet port from internet\nSAP: 1.40.01.03 - MS Azure Network Security Group does not restrict traffic to Infrastructure ports from internet\nSAP: 1.40.01.02 - MS Azure Network Security Group does not restrict traffic to Admin ports from internet\nSAP: 1.40.01.01 - MS Azure Network Security Group does not restrict traffic to DB ports from internet\nSAP: 1.40.01.04 - MS Azure Network Security Group does not restrict File sharing ports from internet\nSAP: 1.40.05 - Azure storage accounts has blob container(s) with public access\nSAP: 1.40.05 - Storage Accounts without Secure transfer activated\n\n\nAlibaba Cloud\nSAP: Alibaba Cloud Security Group allow internet traffic to blocklisted Telnet port\nSAP: Alibaba Cloud Security Group allow internet traffic to blocklisted database ports\nSAP: Alibaba Cloud Security Group allow internet traffic to blocklisted admin ports\nSAP: Alibaba Cloud Security Group allow internet traffic to blocklisted infrastructure ports\nSAP: Alibaba Cloud Security Group allow internet traffic to blocklisted file sharing ports","232-affected-preventive-controls-secure-by-default-controls#2.3.2 Affected preventive controls (Secure-by-default controls)":"The following preventive controls can also be excepted by implementing the tags:\nSecurity groups shall not contain Internet ingress 0.0.0.0/0 rules for blacklisted ports.\nAPI Logging central storage location must not be public accessible (AWS CloudTrail, Azure Activity Logs, GCP Cloud Admin logs).\nNon-encrypted buckets are per default not allowed (only for AWS and Azure).\nPublic buckets are per default not allowed (newly created buckets).","233-how-to-implement-the-tags#2.3.3 How to implement the tags":"SAP Global Security is monitoring the use of this label and will randomly review the exceptions done this way, by confirming the LoBs/CBGs use case / business reason for the exceptions.","2331-aws#2.3.3.1 AWS":"The exception must be implemented by tagging the affected resources. Resources with this tag will be excluded from the alerts automatically. The tag has to be implemented as follows:\nFor tags mentioned for the following security group polices, the key will be: \"sec-by-def-network-exception\". The exception will be done on port-level and this will be defined in the value. So the value may be \"SSH\" or \"RDP\" if you want an exception for either one port, or \"SSH, RDP\" if both ports should be excepted.An example may look like as follows: [sec-by-def-network-exception: SSH, RDP]\n\nFurther information on how tags are implemented can be found in chapter 3.2.1 of this wiki-page.\nThe following table shows all accepted values for a security group exception:\nDetective control\tKey\tAccepted values for the tag\tSAP: 1.40.01.05 - AWS Security Group does not restrict traffic to Telnet port from internet\tsec-by-def-network-exception\tTelnet\tSAP: 1.40.01.01 - AWS Security Group does not restrict traffic to DB ports from internet\tsec-by-def-network-exception\tPostgreSQL, MySQL, MSSQL, OracleSQL, MongoDB\tSAP: 1.40.01.02 - AWS Security Group does not restrict traffic to Admin ports from internet\tsec-by-def-network-exception\tSSH, RDP, VNC, RPC, RSH\tSAP: 1.40.01.03 - AWS Security Group does not restrict traffic to Infrastructure ports from internet\tsec-by-def-network-exception\tDNS, HTTP, POP3, SMTP, DHCP, SNMP\tSAP: 1.40.01.04 - AWS Security Group does not restrict traffic to File sharing ports from internet\tsec-by-def-network-exception\tNetBIOS, SMB, FTP, TFTP\nAdditional information can be found here: Amazon Web Services Hardening Procedure 1.40.01\nOther special HIGH policies:\nFor \"SAP: 1.10.13 - AWS S3 buckets are accessible to public\" & \"SAP: 1.10.13 - AWS S3 Bucket has Global GET Permissions enabled via bucket policy\" the key is the following: \"sec-by-def-public-storage-exception\" and the value is \"enabled\".\nFor \"SAP: 1.50.01 - AWS EBS volumes are not encrypted\" the key is the following: \"sec-by-def-ebs-encryption-exception\" and the value is \"enabled\".\nFor \" SAP: 1.60.06 - AWS Amazon Machine Image (AMI) is publicly accessible \" the key is the following \"sec-by-def-public-image-exception\" and the value is \"enabled\".\nFor \"SAP \"SAP: 1.50.02 - AWS S3 buckets do not have server side encryption\" the key is the following: \"sec-by-def-encrypt-storage-exception\" and the value is \"enabled\".","2332-ms-azure#2.3.3.2 MS Azure":"The exception must be implemented by tagging the affected resources. Resources with this tag will be excluded from the alerts automatically. The tag has to be implemented as follows:\nFor tags mentioned for the following security group polices, the key will be: \"sec-by-def-network-exception\". The exception will be done on port-level and this will be defined in the value. So the value may be \"SSH\" or \"RDP\" if you want an exception for either one port, or \"SSH, RDP\" if both ports should be excepted.\nAn example may look like as follows: [sec-by-def-network-exception: SSH, RDP]\n![Azure_tagging](/assets/docs-images/exception_handling_process/Azure_tagging.png)\n\nFurther information on how tags are implemented can be found in chapter 3.2.1 of this wiki-page.\nThe following table shows all accepted values for a security group exception:\n\n| Detective control | Key | Accepted values for the tag |\n| --- | --- | --- |\n| SAP: 1.40.01.05 - MS Azure Network Security Group does not restrict traffic to Telnet port from internet | sec-by-def-network-exception | Telnet |\n| SAP: 1.40.01.01 - MS Azure Network Security Group does not restrict traffic to DB ports from internet | sec-by-def-network-exception | PostgreSQL, MySQL, MSSQL, OracleSQL, MongoDB |\n| SAP: 1.40.01.02 - MS Azure Network Security Group does not restrict traffic to Admin ports from internet | sec-by-def-network-exception | SSH, RDP, VNC, RPC, RSH |\n| SAP: 1.40.01.03 - MS Azure Network Security Group does not restrict traffic to Infrastructure ports from internet | sec-by-def-network-exception | DNS, HTTP, POP3, SMTP, DHCP, SNMP |\n| SAP: 1.40.01.04 - MS Azure Network Security Group does not restrict traffic to File-sharing ports from internet | sec-by-def-network-exception | NetBIOS, SMB, FTP, TFTP |\n\n*Secure-by-default tag structure*\n- For the Azure policies, it is crucial that the values are configured exactly in the order as described in the table above, e.g.:\n- Key: **sec-by-def-network-exception**, value: '**SSH, VNC, RSH'**\n- If the value will be set to **VNC, RSH, SSH** the policy will **NOT** accept this tag for the exception.Additional information can be found here: [Microsoft Azure Hardening Procedure 1.40.01](<https://wiki.wdf.sap.corp/wiki/display/itsec/Microsoft+Azure+-+Hardening#MicrosoftAzureHardening-1.40Networking%E2%80%93configuringsecureaspectsofNetworkSecurityGroups(NSG)>)\n\nFor the other special HIGH policies:\n\"SAP: 1.40.05 - Azure storage accounts has blob container(s) with public access\" the key is the following: \"sec-by-def-public-storage-exception\" and the value is \"enabled\".\n\"SAP: 1.40.05 - Storage Accounts without Secure transfer activated\" the key is the following: \"sec-by-def-encrypt-storage-exception\" and the value is \"enabled\".","2333-gcp#2.3.3.3 GCP":"The exception must be implemented by tagging the affected resources. Resources with this tag will be excluded from the alerts automatically. The label has to be implemented as follows:\nFor tags mentioned for the following firewall rule polices in GCP, the following labels will be used as it is not possible to use key-value pairs:\nDetective control\tLabel\tSAP: 1.40.01.05 - GCP Firewall rule with Inbound traffic from the internet to telnet port on the blocklist\tsec-by-def-telnet-port-exception\tSAP: 1.40.01.01 - GCP Firewall rule with Inbound traffic from the internet to DB ports on the blocklist\tsec-by-def-database-port-exception\tSAP: 1.40.01.02 - GCP Firewall rule with Inbound traffic from the internet to admin ports on the blocklist\tsec-by-def-admin-port-exception\tSAP: 1.40.01.03 - GCP Firewall rule with Inbound traffic from the internet to Infrastructure ports on the blocklist\tsec-by-def-infrastructure-port-exception\tSAP: 1.40.01.04 - GCP Firewall rule with Inbound traffic from the internet to file sharing ports on the blocklist\tsec-by-def-fileshare-port-exception\nIt is important to notice that exceptions in GCP are not per port but per Policy and each policy covers multiple ports. Information on how to tag firewall rules in GCP can be found in chapter 3.2.3. An example may look like this:\n\n\nFor the other special HIGH policy:\n\"SAP: 1.40.07 - GCP Storage buckets are publicly accessible to all users\" the key is the following: \"sec-by-def-public-storage-exception\" and the value is \"enabled\".","2334-alicloud#2.3.3.4 AliCloud":"For tags mentioned the in following security group polices, the key will be: \"sec-by-def-network-exception\". The exception will be done on port-level and the port is defined in the value. So the value may be \"SSH\" or \"RDP\" if you want an exception for either one port, or \"SSH, RDP\" if both ports should be excepted.An example may look like as follows: [sec-by-def-network-exception: SSH, RDP]\nGo to the ECS Service from AliCloud and move to Security Groups. Select the affected Security Group, toggle the box and click on \"Edit Tags\":\n\n\n\nEnter the tag and the value and click on \"Confirm\". The result may look like the following example:\n\n\n\n\nThis table shows all accepted values for a security group exception:\nDetective control\tKey\tAccepted values for the tag\tSAP: Alibaba Cloud Security Group allow internet traffic to blocklisted Telnet port\tsec-by-def-network-exception\tTelnet\tSAP: Alibaba Cloud Security Group allow internet traffic to blocklisted database ports\tsec-by-def-network-exception\tPostgreSQL, MySQL, MSSQL, OracleSQL, MongoDB\tSAP: Alibaba Cloud Security Group allow internet traffic to blocklisted admin ports\tsec-by-def-network-exception\tSSH, RDP, VNC, RPC, RSH\tSAP: Alibaba Cloud Security Group allow internet traffic to blocklisted infrastructure ports\tsec-by-def-network-exception\tDNS, HTTP, POP3, SMTP, DHCP, SNMP\tSAP: Alibaba Cloud Security Group allow internet traffic to blocklisted file sharing ports\tsec-by-def-network-exception\tNetBIOS, SMB, FTP, TFTP"}},"/internal/operations/prisma_ops/policies_and_exception_handling/prisma_public_cloud_enduser_guild":{"title":"Prisma Public Cloud End User Guide","data":{"":"Status RELEASED\n\nDocument owner Herre, Thorsten\n\nDocument substitute Schippers, Markus\n\nVersion^6\n\nLast Modified 27.04.","table-of-contents#Table of contents":"Definitions\n1.0 Onboarding new account & user request\n2.0 How to Login\n3.0 How to navigate through Prisma Public Cloud\n3.1 Dashboard: Central overview of scan results and resources\n3.2 Investigate: Create your own security queries\n3.3 Policies: Overview of all implemented policies\n3.4 Compliance: Show implemented security reports\n3.5 Alerts: Overview of all security findings\n4.0 How you are supposed to use the tool\n5.0 Prisma Public Cloud FAQ","definitions#Definitions":"Query\n\nThe Query Language of Prisma Public Cloud is a powerful and flexible tool that helps you gain\nsecurity and operational insights about your deployments in public cloud environments. You can use\nRQL to perform configuration checks on resources deployed on different cloud platforms and to gain\nvisibility and insights into user and network events. (for further information see: https://docs.\npaloaltonetworks.com/redlock/redlock-rql-reference/rql-reference/rql)\n\nPolicy\n\nThe Policy is the actual security check. It is based on a specific query and contains additional\ninformation such as remediation steps and a short description. Prisma differentiates between different\npolicy types:\n\nConfig\nAudit Event\nAnomaly\n\nThe exception handling process and the policy management process do apply for all policy types.\n\nFurther information on anomaly policies can be found here, as their query can not be inspected inside\nthe Prisma tool:\nhttps://docs.paloaltonetworks.com/prisma/prisma-cloud/prisma-cloud-admin/prisma-cloud-policies\n/anomaly-policies\n\n(Compliance) Standard\n\nA (Complaince) Standard represents a set of policies.\n\nRelated Documents and Links\n\nAlibaba Cloud Hardening\nProcedure\nAmazon Web Services\nHardening Procedure\nGoogle Cloud Platform\nHardening Procedure\nMicrosoft Azure Hardening\nProcedure\n\nAlert Rule\n\nThe Alert Rule defines which Policies or Standard is assigned to which hyperscaler accounts. Based\non this assignment the alerts are generated in the Prisma tool.\n\nResource\n\nResource is used equivalent to asset in Prisma Public Cloud. In the hyperscaler context, a resource\nincludes VMs, storage buckets, loadbalancers, ...","10-onboarding-new-account--user-request#1.0 Onboarding new account & user request":"Your AWS, GCP or MS Azure account/project/subscription is per default monitored by Prisma Public\nCloud. To get access to the data, request an user for Prisma Public Cloud via the Hyperscaler JIRA (https:/\n/jira.multicloud.int.sap/plugins/servlet/desk/portal/2).","20-how-to-login#2.0 How to Login":"","30-how-to-navigate-through-prisma-public-cloud#3.0 How to navigate through Prisma Public Cloud":"After you logged in to Prisma Public Cloud you will find this navigation bar in the top-left corner of your\nscreen. This bar is used to navigate through the different features of the tool.The 3 most important ones for you are circled in red:\nThe Dashboard section is used to get an overview of your hyperscaler resources and their\nstatus\nWith the Investigate tab specific queries can be build to search for detailed events\nIn the Alerts section, all of the unresolved alerts are shown\nFor further information, refer to the dedicated sections below.","31-dashboard-central-overview-of-scan-results-and-resources#3.1 Dashboard: Central overview of scan results and resources":"The Dashboard screen is used to get an overview of the resources of your hyperscaler accounts and\ntheir status (Pass or Fail). Additionally the resources are structured into several categories.\nIf you click on the number in the 'TOTAL' or 'PASS' column, you will be redirected to the Investig\nate screen. This screen lists all resource names based on a specified query.\nIf you click on the number in the 'FAIL' column, you will be redirected to the Alerts screen. This\nscreen lists the names of the policies which are violated.\nThe sidebar on the left side can be fully customized wit individual filters. You can save these\nfilters if you need to use them again later.","32-investigate-create-your-own-security-queries#3.2 Investigate: Create your own security queries":"After you have requested a user and the onboarding is finished, you can login to PrismaCloud\nvia SSO using the following URL:\n\nhttp://prisma.tools.sap\nThe Investigate feature can be used to build queries, which search for specific configurations ofhyperscaler resources.\nEvery policy is based on such a query.\nYou can use this section to build individual queries for your use case.\nWith the button on the right, you can download your results in a csv file","33-policies-overview-of-all-implemented-policies#3.3 Policies: Overview of all implemented policies":"The Policies screen shows all policies which are implemented in Prisma Public Cloud.\nThis feature is helpful, if you want to have any further details regarding a policy.\nJust click on a policy, and you will see the query behind that policy and some\nremediation steps for fixing.\nYour hyperscaler accounts will be scanned based on a subset of policies. This subset is defined\nby SAP Global Security.","34-compliance-show-implemented-security-reports#3.4 Compliance: Show implemented security reports":"The Compliance feature provides different compliance standards as ISO 27001 or CIS benchmarks, and\nrates your accounts based on the implemented policies.\nA compliance standard is a set of policies.\nThese compliance standards are not complete. What is displayed is only a estimation of Prisma\nPublic Cloud.\nAs on the Dashboard you see the 'TOTAL' assets divided in 'PASS' and 'FAIL'.\nThe sidebar on the left side can be fully customized wit individual filters. You can save these\nfilters if you need to use them again later.\nThe SAP standards implemented by SAP Global Security are the most important ones, because\nthese are the\nThese standards are based on the SAP Security Procedures: SAP Security Policy\nFramework","35-alerts-overview-of-all-security-findings#3.5 Alerts: Overview of all security findings":"","1#1.":"","2#2.":"","3#3.":"a.\nb.\nc.\n\n4.\n\na.\n\ni.\nii.\niii.\nb.\n\n5.","1-1#1.":"The Alerts section is the most important section for you. The alerts for your hyperscaler accounts are\nlisted here. They are rated LOW, MEDIUM or HIGH based on the alert rules defined by SAP GlobalSecurity\nThe severity is set based on the SAP Security Procedures (see: SAP Security Policy Framework)\nThe list of alerts provides an overview of which policies are violated and how severe the\nviolation is.\nThe sidebar on the left side can be fully customized wit individual filters. You can save these\nfilters if you need to use them again later.\nBy clicking on an entry of the list the following screen will show up:\n\nOn this page, a short description of the policy and the remediation steps are is provided\nYou can see a list on the bottom, which shows the affected resources, their related hyperscaler\naccounts and regions\nBased on this information, you should be able to fix the alert\nIn case you need further information please refer to the SAP Security Procedures for\nAWS, GCP and Azure (see: SAP Security Policy Framework) or to the official Prisma\nPublic Cloud documentation (see: https://docs.paloaltonetworks.com/redlock)","40-how-you-are-supposed-to-use-the-tool#4.0 How you are supposed to use the tool":"The purpose of Prisma Public Cloud is to monitor your hyperscaler accounts and reveal security relevant\nmisconfiguration. A typical practice would be:\nLogin to Prisma Public Cloud\nNavigate to the Alert section\nSort the list based on the severity\nHigh rated alerts should be fixed within 30 days.\nMedium rated alerts should be fixed within 90 days.\nLow rated alerts are an optional recommendation by SAP Global Security with no\npredefined fixing timeline.\nTo start the remediation of an alert just click on it, and a new screen shows up (see\nchapter 3.5, second screenshot)\nNow you should see additional information regarding the alert (description, remediation\nsteps, resources)\nThe description provides some more information around the policy\nThe remediation steps describe the actions to be done, to mitigate the alert\nIn the list on the bottom of the page, all affected resources are displayed\nThe easiest way to mitigate an alert is to follow the remediation steps provided by\nPrisma Public Cloud\nRepeat step 4 for all High and Medium findings\nRepeat this procedure at least once every 2 weeks.For the initial assessment please use the \"All Time\" filter in the sidebar.","50-prisma-public-cloud-faq#5.0 Prisma Public Cloud FAQ":"Do I have to pay for Prisma Public Cloud?\n\nFor any further information, please refer to the education video: https://video.sap.com/media/t\n/1_ep2jazyl","1-2#1.":"a.\n\n\n\n\na.","3-1#3.":"a.\n\na.\na.\n\n\nNo, Prisma Public Cloud is paid for by the SAP Global Security and the SAP Hyperscaler\nOperations team.\nHow is the data transmitted to Prisma Public Cloud & what is the impact on my\nresources?\nPrisma Public Cloud is a SaaS solution, which pulls the configuration metadata from\nthe API. This means aswell, that Prisma Public Cloud does not consume any resources\nin your hyperscaler accounts.\nHow often is the data pulled?\nThe data gets pulled multiple times per day. The requests are hereby distributed in a\nway that the limitations of the hyperscaler API are not reached.\nIs it possible to implement my own policies and reports?\nYes it is possible, but policy configuration is managed by SAP Global Security based\non the SAP Security Framework (SAP Security Policy Framework)\nDo I have to consider any data protection requirements?\nA Master Data Protection Agreement (MDPA) is signed by SAP Global Security.\nThis is a offline tool, your data stays locally and is not send to any server!\nFeedback & Bug Reports"}},"/internal/operations/prisma_ops/prisma_api_key_usage":{"title":"Prisma API Key Usage","data":{"":"","document-control#Document Control":"The document control section describes the revision history and summary of changes made in the document. It will serve as the version control\nfor the contents of the document.","revision-history#Revision History":"Revision Number Revision Date Summary of Change Changed By\n1.0 05.04.2020 Initial Draft Hyperscaler DevSecOps","table-of-contents#Table of Contents":"Access Keys are a secure way to enable programmatic access to the Prisma Cloud API.\nBy default, only the System Admin has API access and can enable API access for other administrators.\nIf you have API access, you can create up to two access keys.\nFurther documentation can be found on the Prisma API Documentation Page\n\nRemember:\nKeys should be set to expire within 90 days (Maximum)\nKeys set to expire in a timeframe greater than 90 days will be modified by the System Admin to expire within 90 days of its created date.\nOnce keys expire, you cannot reactivate an expired access key.","operational-procedures#Operational Procedures":"","step-1--select-settings--access-keys-and-click--add-new#STEP 1 >> Select Settings > Access Keys and click + Add New.":"","step-2--enter-name--check-key-expiry--set-expiration-date-and-click--create#STEP 2 >> Enter Name > Check Key Expiry > Set Expiration Date and click + Create.":"Name:  Line of Business_API Shortname\nExample:\nMC_DevOps_Automation\nSGS_Vulnerablity_MGMT\n60 Day Max Lifetime of the API Key","step-1--select-settings--access-keys-and-click--add-new-1#STEP 1 >> Select Settings > Access Keys and click + Add New.":"Use your existing key to generate the next API key.\n<insert Code here>"}},"/internal/operations/orca/orca_ops_runbook/service_level_agreement":{"title":"Service Level Agreement","data":{"":"Find all Orca-related service level agreement here!"}},"/internal/operations/prisma_ops/account_group_membership_change":{"title":"Runbook: Adding a Group to an Accounts/Subscription/Project","data":{"":"","document-control#Document Control":"The document control section describes the revision history and summary of changes made in the document. It will serve as the version control\nfor the contents of the document.","revision-history#Revision History":"Revision Number Revision Date Summary of Change Changed By\n1.0 05.04.2020 Initial Draft GCS SRRC Hyperscaler Security team","table-of-contents#Table of Contents":"","operational-procedures-adding-users-to-multiple-groups#Operational Procedures: Adding Users to Multiple Groups":"","step-1--select-settings--users-and-click--add-new#STEP 1 >> Select Settings > Users and click + Add New":""}},"/internal/operations/prisma_ops/prisma_cloud_overview":{"title":"Prisma Cloud Overview","data":{"":"Prisma Cloud is a Configuration Compliance, and security monitoring tool for use in AWS, Azure, GCP, and soon AliCloud. Configuration Compliance means that it is constantly monitoring your public cloud account for \"best practice\" and \"most secure\" configurations of your services and instances. Prisma Cloud is NOT a vulnerability scanner. It is not aware of things like patch level of your instances. However, it does know whether you have created an S3 bucket with public access, or if IAM users are not using MFA. These alerts are classified into \"High Risk\", \"Medium Risk\" and \"Low Risk\"."}},"/internal/operations/prisma_ops/new_alert_exception":{"title":"Alert Suppression Procedure","data":{"":"","alert-suppression-procedure#Alert Suppression Procedure":"Step 1 - Open an ITDirect ticket for Alert Exception Approval\nStep 2 - Make a ticket for alert suppression on the Multi-Cloud JIRA","step-1#Step 1":"Open an ITDirect ticket for Alert Exception Approval\nUse the following link to create the ticket: Alert Exception Ticket\nUse Category ID: SRAS_SEC_APPROVAL | Category Description: Security Architecture Advisory by Global Security | Summary: Please include ‘Exception Request’ along with information on the alert and assets in scope. E.g.: ‘Exception Request for AWS S3 Bucket has Global GET Permissions enabled via bucket policy’\nPlease include the following in the description: Alert Signature, Description of the alert, AWS Account ID(s), and Affected assets.\nAlso, Kindly include the business case for the exception, including -Business/technical reason for vulnerability cannot be fixed\nFactors already in place that reduce the vulnerability’s risk\nLong-term plan for addressing the risk of the vulnerability\nUpon receiving the exception approval from SGS, please follow Step 2","step-2#Step 2":"Make a ticket for alert suppression on the Multi-Cloud JIRA\nUse the following link to create the ticket: Alert Supression Ticket\nSelect the ‘PrismaCloud' request type and then the ‘Customization Request’ option\nLine of business should correspond to the service that the alert suppression is being requested for\nPlease include in the description - The ITDirect ticket number along with the approval and the details (Alert Signature, Description of the alert, AWS Account ID(s), and Affected assets)\nPlease submit the ticket.Multi-Cloud Team will update you once the exception is in place.\n\nFor particular high policies, LOBs are empowered to grant exceptions and suppress the alerts on their own (meaning the service owner or security officer). Please see the following link for more details:https://wiki.wdf.sap.corp/wiki/display/itsecurity/Prisma+Cloud+Exception+Handling+Process#:~:text=for%20Storage%20%26%20Network-,2.4.1%20Affected%20HIGH%20policies,-For%20some%20specialFor low and medium policies, LOBs are empowered grant exceptions and to suppress the alerts on their own (meaning the service owner or security officer). Please see the following link for more details:https://wiki.wdf.sap.corp/wiki/display/itsecurity/Prisma+Cloud+Exception+Handling+Process#PrismaCloudExceptionHandlingProcess-3.ExceptionsforLOW-andMEDIUM-ratedPrismaPolicies"}},"/internal/operations/prisma_ops/runbook_template":{"title":"Runbook: Adding/Changing Routes to a Router","data":{"":"","document-control#Document Control":"The document control section describes the revision history and summary of changes made in the document. It will serve as the version control\nfor the contents of the document.","revision-history#Revision History":"Revision Number Revision Date Summary of Change Changed By\n1.0 15 Oct 2019 Initial Draft Aaron McConnell","application-description#Application Description":"Occasionally, tickets will appear on the queue where a LoB cannot achieve communications because an on-prem router is not configured to\nforward their traffic properly. This can be observed in ticket SPC ticket 1106200161 (and in derived ticket MCO-20473). This ticket was originally\nrelated to a PCI setup, but a subsequent comment in the ticket makes an extra request: \"Add route for the new prefix 10.238.209.0/24 to rt-ext-\nrot1-01 and rt-ext-wdf51-02\". This request is outside of the operational remit of the DevNetOps team.","operational-procedures#Operational Procedures":"1: Examine the request for keywords which indicate that routers to be changed are on-prem. The requestor may use the words \"on-prem\" or \"on\npremise\", or refer to the geographical location of the router, e.g., Walldorf. Conversely, the requestor may specifically state the cloud project ID\nor cloud platform in which a cloud-side change should be made. In this case, a route may be have to be added (or changed) by the DevNetOps\nteam on the cloud-side.\n2: If the request is for a change to an on-prem router then:\n2.1: If there is a SPC ticket:\nAdd a comment to both the Jira ticket and to the SPC ticket: \"This is an on-prem request. Assigning the ticket to the\ncorrect queue \"ITI NETWORK IDC L2 - Network - Internal Data Center Level 2 Support\".\nAssign the SPC ticket to the \"ITI NETWORK IDC L2 - Network - Internal Data Center Level 2 Support\" queue.\nResolve the Jira ticket with type \"Declined\". Add a resolution comment of:\"This is an on-prem request. SPC ticket has been\nassigned to the correct queue \"ITI NETWORK IDC L2 - Network - Internal Data Center Level 2 Support\".2.2 If there is no SPC ticket:Add a comment to the Jira ticket: \"This is an on-prem request. Please raise a SPC ticket with \"ITI NETWORK IDC L2 - Network -\nInternal Data Center Level 2 Support\".\nResolve the Jira ticket with type \"Declined\". Add a resolution comment of: \"This is an on-prem request. Requestor has been advised\nto open a SPC ticket with \"ITI NETWORK IDC L2 - Network - Internal Data Center Level 2 Support\".\n3: If the ticket is for a cloud-side change then escalate the ticket to L3.\n4: If the request is not clear, i.e., you are not sure where the ticket should be directed to, then escalate the ticket to L3.","troubleshooting#Troubleshooting":"","tbd#TBD":"","about-this-document#About This Document":"This document identifies and details the process to be followed when attempting to resolve \"Adding/changing routes to a router\" tickets. This\ndocument aims to help the Ops engineer to classify the ticket and assign it to the correct queue.","approval#Approval":"Status: Pending Approval\nReviewed By: Colm McKenna\nValidated By:\nApproved By:","document-conventions#Document Conventions":"","abbreviations#Abbreviations":"Abbreviation Definition\nPCI Public Cloud Interconnect\nSPC (SAP) Service Provider Cockpit","document-history#Document History":"Comment Name Date\nInitial Draft Aaron McConnell 10/15/"}},"/internal/operations/service_now/Service-Now-Introduction":{"title":"Service Now Introduction","data":{"":"This guide outlines salient Service Now (aka SNOW) information specific to the GCS HSEO team. More general information about Service Now can be found under the GCS Cloud Core Enablement Sharepoint.The guide covers the following areas:\nCreating a Service Now account.\nManaging Service Now notifications.\nHow to forward tickets to other teams.","creating-a-service-now-account#Creating a Service Now account":"To create a Service Now account, follow the steps in the guide for creating Individual access requests to ServiceNow found in User Instruction on How to Request Authorization to ServiceNow Instances.\nOnce you have completed this, you will need to add the following Service Team groups to your account:GCS Response Operations Teams\nGCS L3 SecurityTo add the Service Team groups, go to ITSM and follow the steps in Request and Approval of access.\nIf you cannot find the aforementioned groups or you get an error message when trying to add the group, raise an IT ticket to get access through the Unified Ticketing System.","managing-service-now-notifications#Managing Service Now notifications":"To manage Service Now notifications, go to ITSM and follow the steps in Internal Mail Notifications guide. The steps are on pages 18/19 and the guide also describes the different notifications.","forwarding-tickets-to-other-teams#Forwarding tickets to other teams":"Tickets may be allocated to the team that need to be forwarded to other teams. The process for doing this is to identify the correct team to send it to and change the assignment group. An overview of the roles of the GCS team roles can be found GCS Components Service Offerings.Once you have found the correct team to forward the ticket to, from the Details tab of the ticket, click on Assignment group to change the appropriate team:"}},"/internal/operations/service_now":{"title":"ServiceNow","data":{"":"Find all ServiceNow related documention here!"}},"/internal/operations/sprint_operations_tracking":{"title":"Sprint Operations Tracking","data":{"":"Reporting on team operations progress for each sprint generally consists of three components:\nData Gathering - Writing Jira queries and exporting each team members work and progress into .csv\nAnalysis - Reviewing each ticket done for specific metrics (MTTR, Response Time etc.)\nVisualization and Storage - Updating any applicable graphs and making sure monthly / quarterly information is up to date.","data-gathering#Data Gathering":"","jira-queries#Jira Queries":"Jira queries can be written either through the basic filtering options or writing a custom query\nAn example query can be found and/or used below:\nproject = MCO AND resolved >= YEAR-MONTH-DAY AND resolved <= YEAR-MONTH-DAY AND assignee in (\"TEAM MEMBERS E-MAIL\")\nAfter the search filters your results, export the data into a .csv based on (Current fields)","analysis#Analysis":"","ticket-metrics#Ticket Metrics":"Ticket progress can be tracked by the following SLA (service level agreements):\nInitial Response Time - Important for measuring how long a ticket sits in the queue before it is worked on.\nMTTR (Mean Time To Resolution) - Once a ticket is assigned, a measure of how long it takes until it is resolved.\n\nCurrently the process is that for each team member, each ticket is reviewed individually to track both the response time and MTTR. Individual ticket review also helps identify tickets that are being blocked, or stale tickets that need to be updated or closed due to a lack of action (waiting for customer, vendor, etc.)","visualization-and-storage#Visualization and Storage":"","updating-historical-record#Updating historical record":"Currently all DSEC and MCO ticket progress is tracked by team member and uploaded to a monthly updated sharepoint document. This is used by management to track each team members progress and ratio of development to operations work. Currently all data is summarized by month and by quarter."}},"/internal/ARCHIVED/Dev_Workflow/hasi_improvements":{"title":"[ARCHIVE] HaSi improvements","data":{"":"As part of SAP security standards, the Minerva infrastructure was subject to a\nthorough security penetration test (formerly HASI). A number of findings emerged\nand security improvements were recommended. Below is a brief overview of some\ninteresting tasks done to implement the recommended improvements and address the\nHASI findings.Although the recommended security improvements should be considered common\npractice, they are not always implemented and possibly overlooked. We would like\nto share our personal experience so that others may see the benefits and perhaps\nconsider implementing these security improvements in their respective\nlandscapes.","workload-identity#Workload identity":"The Workload Identity allows GKE applications to access GCP services in a secure\nfashion and is the GCP recommended way.The Minerva infrastructure is based on GKE; the GCP managed Kubernetes\nprovision.  GKE services that run in pods use Kubernetes service accounts as\nidentity. On the other hand, GCP uses IAM service accounts to allow access to\nGCP services.  Enabling the Workload Identity allows Kubernetes service accounts\nto act as IAM service accounts to access GCP services. This way, pods that use\nthe configured Kubernetes service accounts automatically authenticate as IAM\nservice accounts to access GCP APIs. When properly configured, this mechanism\nallows a fine grained tuning of the authorization and authentication of GKE\napplications.An important component of the Workload Identity is the GKE Metadata Server,\nwhich maintains important information such as:\nGCP project information\nGCP Nodes attributes\nGCP Service accounts associated to a node\n\nThis information is used by the Workload Identity to understand the specific pod\nrunning in a node association with the IAM identity.For more information please refere to the GCP documentation\nhttps://cloud.google.com/kubernetes-engine/docs/concepts/workload-identityIn order for the Workload Identity to identify a Kubernetes service account, it\nneeds to use the namespace of the service account. We had already separated\ndifferent cluster services into their respective namespaces. Therefore, we made\na configuration decision to create multiple Workload Identities in each cluster\nwith each Workload Identity paired with the respective Kubernetes service\naccount in the service namespace. The result being a Workload Identity\nconfiguration per namespace associated with the different services in that\nparticular namespace.","gcp-service-accounts-and-secrets-configuration#GCP Service accounts and secrets configuration":"As IAM service accounts are given roles to perform certain actions on Cloud\nresources, using the same service account for all Kubernetes workloads is not\nadvisable. We already separated the different service accounts per workload as\ndescribed above. Furthermore, as service accounts need to access secrets to\nperform certain operations, normally the secret-accessor role is given.\nHowever, that role gives access to all secrets in the project which is also not\nadvisable. Therefore, rather than provide such role, we have configured each\nrelevant secret to allow access to the specific service account. The logic is\nbasically reversed where rather than being the service account able to access\nall secrets, the secrets themselves allow access to only the service accounts\nthat need access.","kubernetes-preventive-controls-and-audit#Kubernetes preventive controls and audit":"Preventive controls in Kubernetes stop any uncompliant deployments from being\nadmitted into the cluster thereby earning it the title \"Admission controller\".\nOPA Gatekeeper is the Admission Controller with which the preventive controls\nare implemented.  Gatekeeper is a customisable admission control webhook, that\nintercepts admission requests before they are persisted, enforcing policies that\nare executed by the Open Policy Agent (OPA). Have custom policy needs for your\ncluster? No worries.  Using OPA Gatekeeper, it is possible to write custom\npolicies in rego and familiar kubernetes YAML syntax.  Another interesting\nfeature is the possibility to exclude namespaces from any policy defined.  This\nsimply means that Gatekeeper will enforce any defiined policy on all namespaces\nexcept the excluded namespace. In our case, this was an absloute win.Aside from being an Admission Controller, Gatekeeper provides audit information\non deployments already running in the cluster before the introduction of\nGatekeeper to the cluster. This is a very useful feature to perform compliance\naudits an already running deployments in a cluster.  Configurations made prior\nto the Gatekeeper deployment cannot be stopped by the OPA, an audit of such\nconfigurations against the control policies is generated. Such audit violations\ncan therefore be acted upon by developers without necessarely stopping the\nrunning cluster.PS: We would recommend redefining the scope of the webhook to exclude the\nkube-system namespace in order to avoid breaking system pods and node\nregistration, or unpredictable behaviour of the cluster during upgrades.More details can be found at\nhttps://open-policy-agent.github.io/gatekeeper/website/docs/Currently we have implemented the following contraint policies in our\nlandscapes, with more to be added in the future:\nhost-namespace\npriviledge-escalation\npriviledged-container\nrun-as-user-group","containers-runtime-scan#Containers runtime scan":"This feature allows scanning containers at run-time to detect any suspicious\nactivity based on defined rules. Falco was used to implement this functionality\n(more details on Falco can be fount at https://falco.org/docs/). Falco comes\nwith a set of pre-defined rules that only need to be enabled. CVEs rules are\nalso available. Falco intercepts linux syscalls and generate alerts according to\nthe enabled rules.In our landscapes, the following rules have been enabled, which are based on the\ndriver-loader image configuration at\nhttps://github.com/falcosecurity/charts/tree/master/falco:\nproc-fs\nboot-fs\nlib-modules\nuser-fs\netc-fs\ndriver-fs","intranode-visibility#Intranode visibility":"Intranode visibility uses VPC network for packets sent between Pods. This in\nturn uses firewall rules, routes, flow logs, and packet mirroring configurations\nfor the packets. This allows to:\nflow logs can be seen for traffic between pods\nhave firewall rules created for all traffic among pods, even on the same node\nclone traffic throufh Packet Mirroring for examination\n\nMore details can be found at\nhttps://cloud.google.com/kubernetes-engine/docs/how-to/intranode-visibility"}},"/external/Team_Overview":{"title":"Team Overview","data":{"":"Contents\nSecurity Solution Overview1.1 Orca Deployment1.2 Solution Security1.3 Solution Costs\nPlatform2.1 Side Scanning2.2 Context Aware Security Intelligence2.3 Enablement2.4 Integrations2.5 UI\nHyperscaler Compliance3.1 Hyperscaler Security Compliance Scans3.2 Orca Compliance Scanning3.3 Accessing Previous Scans\nAutomation and Customization4.1 How to get alerts 4.2 API Documentation4.3 API Limits\nImplementation5.1 Orca Ownership5.2 Hyperscalers Security Engineering & Operations (HSEO) Ownership5.3 Hyperscalers Security Engineering & Operations (HSEO) Support\nAccess to Orca6.1 Account Level View Access6.2 Group Level View Access6.3 HSDB Access6.4 Account Maintenance\nControl Details7.1 AWS Compliance Scanning Control Details7.2 Azure Complinace Scanning Control Details7.3 GCP Compliance Scanning Control Details7.4 AliCloud Compliance Scanning Control Details\nControl Validation\nReporting9.1 ISO/SOC Audit Reports9.2 Cloud Security Office Hours9.3 Hyperdash\nExceptions10.1 How to request an exception\nPublic Content","1-security-solution-overview#1. Security Solution Overview":"Originating from the Multicloud Security team established in 2017, the Hyperscalers Security Engineering & Operations (HSEO) provides services that develop, deploy, and run workloads on hyperscalers to all SAP's Lines of Business. From development and testing to production, we are there every step of the way to enhance their journey to public cloud. It is our mission to engineer, deliver products, and solutions to maximize SAP cloud infrastructure agility, security, and cost efficiency across AWS, Azure, GCP, and AliCloud.\n\nThe current back-end, Orca Security, offers a zero-touch approach to cloud security that eliminates the organizational friction and performance impact associated with traditional solutions. Orca SideScanning technology delivers security visibility and coverage across your entire cloud environment, while a context engine combines workload and cloud configuration details to build a unified data model and visual map of all your assets. The Orca agentless approach and robust capability set replaces many of the point solutions previously needed to secure your cloud estate and maintain regulatory compliance now and in the future. Implementation of the tool is done at org level by creating service accounts (AWS & GCP) or in the case of Azure approval of an Enterprise App.\n\nOrca Further Reading","11-orca-deployment#1.1 Orca Deployment":"The immediate use case that drove the initial deployment was to support resource and software Asset Management, specifically on virtual machines and containers. However, the solution also provides vulnerability scans for both. Orca also provides insight into misconfigurations in the landscape that could impact risk severity ratings including Kubernetes clusters, indicators of compromise, known malware, as well as use cases around IP address management and domain name registration.\n\nThe scope for Orca is all public cloud accounts in SAP, similar to the Minerva compliance scans. It does not provide coverage for Converged Cloud.\nSAP cloud accounts were onboarded at the org level with scans disabled. Scans were triggered according to deployment rings in the same fashion that preventative controls were rolled out.","12-solution-security#1.2 Solution Security":"During the evaluation and approval process for the purchase of the tool, this topic was extensively discussed and deemed acceptable even for PCI-DSS certification relevant systems.\n\nThe SAP implementation of Orca has been installed in cloud accounts owned by SAP. Since the deployment is entirely within SAP cloud accounts, data never leaves SAP. The deployment, maintenance and management of the accounts is done by Orca. In this way, SAP consumes Orca as a SaaS solution while having all the data and components in SAP EMEA.\n\nOrcas account access read-only. There is an added capability to create, read, and delete its own Orca tagged snapshots. Orca does not have permissions to make any changes to cloud accounts/landscapes. The solution does not have access to encrypted database files. If the scans detect PII data, it flags it for attention, but masks it in the results. Once onboarded, Orca reads account meta data and creates snapshots in the region of the originating instance of endpoints that are then scanned by Orca's SaaS platform. Once the scan is completed the snapshots are deleted.","13-solution-costs#1.3 Solution Costs":"There are minimal costs that the LoBs are accountable for. While normally a SaaS solution, Orca for SAP is deployed in SAP-owned dedicated cloud accounts operated by the Hyperscalers Security Engineering & Operations (HSEO) . Orca will create snapshots of running compute instances and transfer that to a scanner running in the same cloud region and same provider in the HSEO cloud account. All of the scanning occurs there, with the cloud run costs incurred by the HSEO team.\n\nLicense and operating costs for 2022 are already covered through the SCD2 budget. For 2023, there will be discussions about cross-charging, but the primary targets for those are stakeholder organizations for the resulting data, such as CCIR (SISM), SAM (ALM) and SGS, not the LoBs.","2-platform#2. Platform":"","21-orca-side-scanning#2.1 Orca Side Scanning":"Side scanning is based on the ability to take snapshots of cloud workloads and scan the snapshots.\n\nOrca SideScanning is an approach that addresses the shortcomings of agent-based solutions by collecting data from the workloads’ runtime block storage without requiring agents. Orca then reconstructs the workload file system – OS, applications, and data – in a virtual read-only view, and performs a full risk analysis with zero performance impact on the workloads themselves.\n\nFurther Reading","22-orca-context-aware-security-intelligence#2.2 Orca Context Aware Security Intelligence":"Unlike solutions that simply report on the severity of each siloed security issue, Orca's multi-dimensional approach considers three crucial factors to prioritize risk:\n\nSeverity: How severe is the underlying security issue? For example, what type of threat is it, how likely is it to be exploited, and what is the CVSS score?\nAccessibility: How easy is it for an attacker to access the asset that contains this issue? For example, is the asset public facing, or is there lateral movement risk?\nBusiness impact: How would the business be impacted if this asset was exploited? For example, is this asset critical to the company’s business, does it contain sensitive PII, or is it adjacent to one that does?\n\n\nFurther Reading","23-enablement#2.3 Enablement":"Once you have access to Orca, further documentation can be found in their resource library docs.orcasecurity.io","24-integrations#2.4 Integrations":"The following is a brief summary of some of the possible integrations with Orca. A full list can be found on orcasecurity.io/integrations\n\nOrca API\n\nGraphQL\nOrca API\nSwagger\n\n\nTicketing\n\nAzure DevOps\nJira Cloud\nJira Server (on-prem)\nService Now ITSM\nService Now SIR\n\n\nNotifications\n\nAWS SNS\nAWS SQS\nGCP Pub/Sub\nMicrosoft Teams\nPagerDuty\nSlack\nWebhook\n\n\nSIEM/SOAR\n\nAmazon Security Lake\nAnecdotes\nSplunk\n\n\nStorage\n\nAzure Blob\nGCP Bucket\nS3 Buckets","25-ui#2.5 UI":"A brief introduction on the Orca UI can be found in their documentation: Navigating the Orca User Interface","3-hyperscaler-compliance#3. Hyperscaler Compliance":"Hyperscaler compliance requires coverage via a unified, purpose-built platform. Unfortunately, solutions today that are poorly integrated or agent-based deployments lead to blind spots and significant work for security and compliance teams. Relying on tools that approach compliance on a per-asset basis results in gaps in coverage, increased cybersecurity risk, organizational friction, and failed audits.\n\nThe compliance scanning solution is designed to help report the security compliance posture of all cloud accounts within SAP. It provides a centralized reporting platform as the lowest barrier-to-entry for security compliance scanning with LoBs (Line Of Business). The scanning service has the ability to gather all the LoBs cloud accounts across all the hyperscalers and deliver compliance reports against security controls based on SGS (SAP Global Security) Hardening guidelines. LoBs then have the information to act on those reports results to make their accounts compliant. Additionally, as new accounts are added to the hyperscalers, the scanning service is able to gather for them seamlessly without manual intervention. Additionally, charts and statistical analysis are provided through HS Portal or Hyperdash for overall reporting to high management in SAP. Finally, the solution is integrated with the HS portal which allows access and visibility of the scan results to all LoBs.","31-hyperscaler-security-compliance-scans#3.1 Hyperscaler Security Compliance Scans":"Hyperscaler Security Compliance Scanning controls are mapped one-to-one to SGS security policies for hyperscalers, and the full landscape is currently scanned with Minerva weekly for compliance.","32-orca-compliance-scanning#3.2 Orca Compliance Scanning":"Orca compliance scans are scheduled everyday. Scan duration is largely measured by the duration it takes for Orca to create the snapshot of the target account (approximately 15 minutes). Scans are performed on a daily basis and also based on detected changes in cloud logs. The full snapshot is only performed the first time and subsequent scans are based on detected changes, which reduces overall time. The solution does not have access to encrypted database files. If the scans detect PII data, it flags it for attention, but masks it in the results.","33-accessing-previous-scans#3.3 Accessing Previous Scans":"Previous Minerva scan data can be found in the HS Secure Architecture & Engineering sharepoint.","4-automation-and-customization#4. Automation and Customization":"Orca uses a simple, yet expressive query language that offers core capabilities including advanced querying and automation. Its capabilities include:\n\nQuery data to filter or search for assets.\nSearch and investigate security issues.\nMonitoring on compliance and standards violations, and other security issues.\nAbility to define asset/issue groups and assign them to IT and DevOps teams for remediation.\nTicketing and reporting automation.\n\n\nFurther Reading","41-how-to-get-alerts#4.1 How to get alerts":"For general alert UI information, when you select Alerts from the Orca menu on the left pane, the Alerts page opens showing you a range of useful information about alerts.\nOn the Alerts page, you can:\n\nView cumulative information on alerts\nConfigure alert views using filters, sorting, and grouping\nManage the configured views\nDrill down to an individual alert and view its properties\n\n\nFurther Reading","to-list-the-custom-controls-against-which-the-hyperscaler-account-is-getting-scanned-for#To list the custom controls against which the hyperscaler account is getting scanned for":"Login into Orca portal -> Navigate to the menu -> Settings -> Modules -> Alerts Settings -> All the controls under \"Custom Alerts\" tab are SAP specific controls developed. SAP Custom controls","to-get-a-list-of-all-the-alerts-associated-with-the-custom-controls#To get a list of all the alerts associated with the custom controls:":"Option 1: Navigate to \"SGS-Custom-Alerts\" View in \"Alerts\" page\nGo to the \"Alerts\" page in Orca portal\nIn the top-left corner, click on \"Current View\"\nChoose \"SGS-Custom-Alerts\" from the list of available shared views\n\n\nOption 2: Sonar query (Navigate to the menu -> Discovery -> Click on 'Switch to Sonar')\nAll the custom controls have been tagged with \"sap\" and \"sap_hyperscaler\" where hyperscaler value could be 'aws', 'gcp', 'azure' or 'alicloud'\nYou can retrieve alerts for all the custom controls using \"sap\" label or per hyperscaler using \"sap_hyperscaler\" label. E.g. \"sap_gcp\"\nRun following Sonar query: Alert with Labels containing \"sap\" to fetch all the alerts related to all custom controls\nRun following Sonar query: Alert with Labels containing \"sap_hyperscaler\" to fetch all the alerts related to all hyperscaler specific custom controls","42-api-documentation#4.2 API Documentation":"Information on how to connect and use the HSDB API for Orca can be found here.\n\nOrca's API documentation can be found here.","43-api-limits#4.3 API Limits":"There are some known UI export limits. These are the current limitations from UI exports.\n\n\"ui_export_limits\": {\n\"alerts\": 75000,\n\"assets\": 75000,\n\"compliance\": 75000,\n\"sonar\": 75000,\n\"vulnerabilities\": 75000\n}","5-implementation#5. Implementation":"With the additional capacity to generate, read, and delete its own Orca labeled snapshots, Orcas access to accounts is read-only. Orca is not authorized to alter any cloud accounts or landscapes. After being onboarded, Orca reads cloud account meta data and creates snapshots of endpoints in the region of the initial instance, which are then scanned by Orca's SaaS platform. The snapshots are erased after the scan is finished. An average snapshot's lifecycle for scanning is about 15 minutes at a time. Scheduled scans are carried out depending on changes found in cloud logs as well as on a configurable basis (the default is every 12 hours). This enables Orca to capture fleeting assets that it might otherwise overlook.\n\nThe Orca stack will be set up in SAP-owned cloud accounts for the SAP implementation. Orca will handle the deployment, upkeep, and administration of the stack. In this method, SAP may continue to use the Orca stack as SaaS while still having complete control over all data and components in EMEA.","51-orca-ownership#5.1 Orca Ownership":"Orca provides software updates, manages the infrastructure of the solution and all maintenance one would expect within a SaaS solution","52-hyperscalers-security-engineering--operations-hseo-ownership#5.2 Hyperscalers Security Engineering & Operations (HSEO) Ownership":"HSEO controls the cloud accounts' general IAM as well as the scan data, user, and API access for SAP staff and stakeholder teams. The cloud account owner in HSEO is still entirely in charge of ensuring that the cloud accounts follow SAP SGS security regulations and all other operational guidelines.","53-hyperscalers-security-engineering--operations-hseo-support#5.3 Hyperscalers Security Engineering & Operations (HSEO) Support":"To open a ticket with the HSEO team, one can do so through Service Now.\n\nGCS - SRRC - Hyperscaler Security\nServices related to security in public cloud\nGCS - Hyperscaler - Minerva\nServices related to security compliance scanning through Minerva in compliance with SGS security policies and hardening procedures and associated account onboarding, compliance reporting and dashboarding, as well as the Chef InSpec self-scan container.\nGCS - Hyperscaler - Orca\nServices related to security compliance scanning through Orca\nSGS - Create Security Incident\nSubmit a request to investigate a potential cyber security incident/attack. A Security Incident Record will be created to track and document progress.\nWe also have our #sap-hyperscaler-security Slack channel if you want to pop in for questions\n\n\nFurther information on how to get access and/or modify ServiceNow roles can be found in the Harmonized Customer Service Management (HCSM) documents page: User Instruction on How to Request Authorization to ServiceNow Instances","6-access-to-orca#6. Access to Orca":"You can log into Orca via SSO here (Orca Login) after requesting the Multicloud_Orca_User Cloud Access Manager (CAM) profile. More information on requesting initial CAM profile can be found here (Initial CAM Profile).\n\nFor initial access to Orca, you will need to request the following automatically approved CAM profile: Multicloud_Orca_User. This profile does not provide permissions in Orca but enables you to sign into Orca via SSO. You will have to periodically request this profile to have continued access to Orca. Beginning on December 21st, 2022, once a user has signed into Orca, their permissions will be automatically provisioned and updated based on the Hyperscaler Account roles and Hyperscaler Group roles. Prior to this point, all users with only the Multicloud_Orca_User CAM profile will have blank permissions and should not be able to view any account information in Orca.","61-account-level-view-access#6.1 Account Level View Access":"The following Hyperscaler account-level roles will grant a user Viewer access to the respective account in Orca:\n\nTechnical Responsible User, Security Officer, and Additional User. More information about Hyperscaler Accounts can be found here.","62-group-level-view-access#6.2 Group Level View Access":"The following Hyperscaler group-level roles will grant a user Viewer access in Orca to all accounts which fall under the group:\n\nAdministrator, Power User, Editor, and Reader. More information about Hyperscaler Groups can be found here.","63-hsdb-access#6.3 HSDB Access":"A Service Now request can be filed under GCS-Hyperscaler-Onboarding/Offboarding Hyperscaler Employee.","64-account-maintenance#6.4 Account Maintenance":"To have your Hyperscaler Account modified or to have it deleted you can open a Service Now ticket under GCS-Hyperscaler-Manage/Delete Account\n\nDeletions can also be processed in HSDB.\n\nOn the Orca side, automation looks at any account listed as \"DELETED\" in HSDB and will offboard the account 90 days after the initial deletion date. It's worth noting that any associated alerts with the cloud account will have been dismissed as soon as the deletion was processed in HSDB.","7-control-details#7. Control Details":"","71-aws-compliance-scanning-control-details#7.1 AWS Compliance Scanning Control Details":"Further Reading: SAP Global Security (SGS) AWS - Hardening Guidelines","72-azure-compliance-scanning-control-details#7.2 Azure Compliance Scanning Control Details":"Further Reading: SAP Global Security (SGS) Azure - Hardening Guidelines","73-gcp-compliance-scanning-control-details#7.3 GCP Compliance Scanning Control Details":"Further Reading: SAP Global Security (SGS) GCP - Hardening Guidelines","74-alicloud-compliance-scanning-control-details#7.4 AliCloud Compliance Scanning Control Details":"Further Reading: SAP Global Security (SGS) AliCloud - Hardening Guidelines","8-control-validation#8. Control Validation":"The tool custodians (GCS SRRC Hyperscalers Security Engineering & Operations team) followed a rigorous methodology to develop full control parity between Minerva and Orca. All new control development was done in Orca for 6 months prior to the transition. Key controls were tested and validated to identify any large discrepancies in data. When gaps were identified, they were investigated, and either the query was updated or the new findings were documented.\n\nMore information can be found in our Orca vs. Minerva Controls document.","9-reporting#9. Reporting":"","91-isosoc-audit-reports#9.1 ISO/SOC Audit Reports":"A SOC report is an attestation by an independent auditor or Certified Public Accountant (CPA) firm that provides an overview of the compliance controls put in place by your vendors in regard to your outsourced functions.\n\nThe risks of ignoring SOC reporting or ISO compliance can be detrimental to your business. When it comes to finance, data security and overall operations, you need to be able to outsource with confidence. Introducing vendors to your business inherently adds risk and SOC reports are one way of mitigating potential gaps.\n\nTo make things more organized, accessible and secure, we have migrated our Security compliance reports repository to our Team Sharepoint. In order to obtain the Cloud Service Provider security compliance reports, please use the following link: Confidential Documents\n\nA ticket to access this folder can be filed under GCS - SRRC - Hyperscaler Security.","92-cloud-security-office-hours#9.2 Cloud Security Office Hours":"Cloud Security Office Hours is a community of LoBs and SAP Security teams for cloud security related topics. The meetings occur weekly on Tuesdays at 14:00 GMT. The meetings are interactive and action oriented. The goal is to work collaboratively for a more secure SAP using surfaced vulnerabilities information for remediation next steps.\nThe target audience: BISO, security engineers, Security Officers, and Technical Responsible Users\n\nThe Cloud Security Office Hours DL has been made public and the meeting will be refreshed periodically to update the attendee list.\n\nFor your convenience, here's a quick join directly: Join DL Cloud Security Office Hours Distro (External)","93-hyperdash#9.3 Hyperdash":"Project Hyperdash is a single-pane solution that streamlines the management of accounts across all Hyperscalers by consolidating data from multiple sources. It offers a centralized view of cost, compliance, and health of cloud infrastructure, enabling users to cut costs, maintain regulatory adherence, and verify cloud environments. Hyperdash promotes the use of existing SAP cloud management and cost optimization services by allowing users to access appropriate tools. Key features include a unified dashboard, cost management, compliance monitoring, vulnerability assessment, and competitive services. Hyperdash is divided into four main dashboards, each focusing on a specific topic. Two executive dashboards are available for specific users, providing a breakdown of cost by Hyperscalers, business units, and competitive services.\n\nURL: https://pages.github.tools.sap/SAE/hyperdash/","931-hyperscaler-security-office-hours#9.3.1 Hyperscaler Security Office Hours":"Cloud Security Office Hours is a community of LoBs and SAP Security teams for cloud security related topics.\n\nThis page displays the data, used by this community to report on the alerts produced by Orca for compliance to SGS standards.\n\nWhile the graphs show a summary, a full data download can be done by clicking the Data Downloads button at the top.\n\nURL: https://hyperdash.multicloud.int.sap/office-hours","932-sgs-policy-compliance-dashboard#9.3.2 SGS Policy Compliance Dashboard":"The aim of the compliance page is to give you an overview of your alignment to SGS policies affecting hyperscaler resources.\n\nWhile the graphs show a summary, a full data download can be done by clicking the Data Downloads button at the top.\n\nURL: https://hyperdash.multicloud.int.sap/compliance","933-access#9.3.3 Access":"Access controls in Hyperdash are managed through the HSDB, which automatically updates permissions based on your roles within accounts and groups on HSDB.","934-bug-reporting#9.3.4 Bug Reporting":"To report a bug in Hyperdash, you have two options:\n\nGitHub Issue: You can open an issue on the Hyperdash GitHub repository. You can do this by visiting the Hyperdash GitHub repository here and creating a new issue with details about the bug you encountered. This is a formal way to report issues and allows for structured tracking and resolution.\n\nTeams Channel: Additionally, you can also post a message in the dedicated Teams channel for Hyperdash. You can access this channel here. This can be a more informal way to report issues or discuss them with the Hyperdash team and the community.","935-data-downloads-and-backend#9.3.5 Data Downloads and Backend":"With regards to how Hyperdash presents data, Hyperdash is pulling the data directly from the Orca APIs.\n\nThere are a number of services run every day which refresh the data we hold.\nOne of these services specifically reaches out directly to the Orca API endpoint for ‘Custom Rules’ which are the custom SGS policies which have been implemented in Orca.\nEach of these are then added to the asset for which the alert is present.\nFinally, when presenting the data, a query is run for each of the assets in a data lake with an SGS alert on it, then are batched into each level (eg. L4, L5, L6, L7, account, region, asset type) and then added to the report.\n(Additional Info) L4-L7 data can be retrieved from HSDB (https://portal.multicloud.int.sap/accounts)","10-exceptions#10. Exceptions":"Exceptions are applied at the account level and all assets within the account are excluded for the policy in scope.\n\nShould such an exception be granted, the Hyperscalers Security Engineering & Operations (HSEO) will exclude the cloud accounts in scope of that exception from scanning. For Hyperscaler Security Exceptions, the usual security exception process applies, but is not yet required at this time, pending final confirmation from the SGS ASR team for vulnerability management, as well as container and K8s security, for which the evaluation is not yet completed. For vulnerability management with SGS ASR, which is furthest ahead and who we plan to integrate Orca data into the SGS SIEM with immediately once the scans flow, please contact Bill Vink.\n\nExceptions will automatically be applied when switching from Minerva to Orca so exception requests will not need to be re-applied.\n\nSAP uses so called “Detective Controls” to monitor the security configuration of ALL SAP Hyperscaler Accounts (e.g. Aws, GCP, Microsoft Azure & Alibaba Cloud) towards the SAP Hyperscaler Security Reference Architecture and configuration standards. These security configuration requirements are defined by SAP Global Security and published in the Security Policy Framework / Wiki.\n\nThe detective controls are divided by their ratings (HIGH, MEDIUM, LOW), which reflect the SAP Global Security mandatory baseline and additional requirements for critical cloud business:\n\nHIGH rated control = MUST / Baseline requirement in Security Wiki\nMEDIUM rated control = SHOULD requirement in Security Wiki\nLOW rated control = optional requirements in the Security wiki or even not mentioned.\n\n\nFurthermore to ensure a Secure-by-Default baseline throughout ALL SAP Hyperscaler Accounts, a policy-as-code deployment (Secure-by-Default controls) will be enforced centrally by the GCS SRRC Hyperscaler Security team. Target is, to get all SAP Hyperscaler Accounts on the same security level / denominator, independent which hyperscaler platform is used (AWS, GCP, MS Azure & Alibaba Cloud). These controls are mapped to their corresponding HIGH detective controls and act as a “preventive control” for SAP hyperscaler accounts.\n\nIn general, Secure-by-Default Controls also known as preventive controls MUST be fulfilled by each LoB, independent if the hyperscaler scenario is considered as dev, test, “crash & burn” or as a productive hyperscaler scenario. The overall target with the enforcement of the Secure-by-Default Controls is, to setup a secure baseline across all SAP cloud environments.\n\nThere might be critical technical or business reasons to delay the enablement of a Secure-by-Default control in a Cloud Area. In such limited cases there is the possibility to request an exception.\n\nThis wiki defines HOW non-compliance / findings are managed between:\n\nthe Hyperscaler Account Owners (e.g. SAP LoBs & CBGs),\nthe SAP GCS SRRC Hyperscaler Security Team and SAP Global Security (SGS).","101-how-to-request-an-exception#10.1 How to request an exception":"To request an exception for different severity levels in the Orca, you can use the following updated instructions:","1011-high-severity-orca-score-80#10.1.1. High Severity (Orca Score: 8.0):":"For exceptions with a 'High' severity rating, which corresponds to an Orca score of 8.0, please follow these steps:\n\nEnsure that the exception request includes all necessary details:\n\nAccount ids or HSDB group (ex. 1489138207353627 or HSDB group: ‘mcce’)\nAlert Name (ex. 2_01_aws_iam_ssh_key_rotation)\n\n\nSubmit the exception through the SAP Global Security SGS Security Exception Management Process (SEMP).","1012-medium-or-lower-severity-orca-score---50#10.1.2. Medium or Lower Severity (Orca Score < = 5.0):":"For exceptions with a 'Medium' severity rating or any lower severity (Orca score < = 5.0), please use the following process:\n\nEnsure that the exception request includes all necessary details:\n\nAccount ids or HSDB group (ex. 1489138207353627 or HSDB group: ‘mcce’)\nAlert Name (ex. 2_01_aws_iam_ssh_key_rotation)\n\n\nSubmit a Snow Ticket to the GCS Hyperscaler Security Engineering & Operations Team. Ensure approval from either the organization’s BISO or the business unit’s L1, signifying acceptance of the risk at the business unit level. : Snow Ticket","1013-there-are-certain-orca-alerts-that-support-resource-tag-based-exceptions-please-find-more-information-at-how-to-implement-the-tags#10.1.3. There are certain Orca Alerts that support resource tag based exceptions, please find more information at How to implement the tags":"Note: These processes are applicable exclusively for Hyperscaler Custom Alerts on Orca.","11-public-content#11. Public Content":"We are aware there is high demand from our end customers for information on how SAP operates securely in public cloud. Below you can find publicly available material on public cloud security and cloud transformation:\n\nSeven Ways SAP Helps Secure Technology Stacks on Public Clouds\n\n\nSAP Community site:\n\nSecure by default for SAP on public cloud infrastructure\nPreventative controls – using organizational policies to provide guardrails for SAP’s public cloud accounts\nPublic Cloud Infrastructure Compliance Scanning at SAP with Chef InSpec\nSecurity Reporting and Analytics for Public Cloud Policy Compliance in SAP\nHyperscaler Security Compliance Scanning: Compliance-as-Code and Operational Benefits"}},"/internal/ARCHIVED/Dev_Workflow/repositories":{"title":"[ARCHIVE] Repositories","data":{"":"This page provides information on how the repositories for the SecDevOps Team\nare organised.The approach taken is based on the separation of concerns which has been\nimplemented by separating services and specifc areas into different\nrepositories.Also, all repositories are prefixed with devsecops- with the exception of the\ncloud-compliance repository which contains the consumers services and is the\nmost customer facing repository of the lot.","cloud-compliance#cloud-compliance":"This repo is for the consumers, which are the services that scan the different\ncloud providers.The service folder contains the service code for each cloud provider\nseparated into their own folders. The deploy folder contains the helm charts\nused for deploying each hyperscaler consumer. This is because all hyperscalers\nconsumers share the same deploy process.Finally, the cicd folder contains pipelines divided per landscape, i.e. dev,\npreprod and prod. It also contains a scripts folder with scripts used by the\nvarious pipelines. Finally, the templates folder maintains templates which are\nused by the pipeline scripts.","devsecops-dispatcher#devsecops-dispatcher":"This repo is for the dispatchers, which are services that gather individual\ncloud accounts information.Similarly to the cloud-compliance repo, there is the service folder\ncontaining the service code for each cloud provider separated into their own\nfolders and the deploy folder contains the helm charts used for deploying\neach hyperscaler dispatcher. This is because all hyperscalers dispatchers share\nthe same deploy process.","devsecops-mailer#devsecops-mailer":"This repo is for the mailer services, which report scan results to relevant\npeople through emails, as well as stats on undelivered emails to the SecDevOps\nTeam.Each of the mailer service is separated into its own folder that contains a\nservice and deploy folder respectively.The cicd folder contains hadolint docker tests used in the relevant CICD PR\nvalidation pipeline.","devsecops-data-archiver#devsecops-data-archiver":"This repo is for the data-archiver services, which creates elastic data backups\nand stores them into buckets or restore backups into elastic.Each of the data-archiver service is separated into its own folder that contains\na service and deploy folder respectively.The cicd folder contains hadolint docker tests used in the relevant CICD PR\nvalidation pipeline.","devsecops-relay#devsecops-relay":"This repo is for the relay service, which fetch scan results and push them into\nelastic.The repo has service and deploy folders, for the service code and\ndeployment respectively.The cicd folder contains hadolint docker tests used in the relevant CICD PR\nvalidation pipeline.","devsecops-3p-services#devsecops-3p-services":"This repo is for third party services used in the infrastructure, which we\nmodify for our needs. Each of the services have service and deploy folders,\nfor the service code and deployment respectively.Currently the repo hosts Falco for runtime container\nmonitoring for vulnerabilities, and stackdriver-metrics-adapter that\nprovides HPA for various services.","devsecops-infra#devsecops-infra":"This repo is for the infrastructure related code and services. The terraform\nfolder contains terraform files to create GCP infrstructure resources, e.g. GKE\nclusters, secrets, pubsub topics, etc.The helm folder contains services that are not modified, but only deployed,\nspecifically, sap-compliance-reporting, which has charts to deploy ECK.The k8s folder contains native kubernetes deployment files that cannot use\nhelm, which are gatekeeper for admission control, and network-policies\nfor pods ingress/egress rules.Finally, the cicd folder that contains terraform validation scripts for the\nPR validation pipeline.","devsecops-utils#devsecops-utils":"This repo is for utils and scripts used for different scopes within the\nSecDevOps Team.The structure of each util differs depending on the code is organised, but a\nREADME file should always be present.","devsecops-docs#devsecops-docs":"This repo contains the documentation and code to generate the GHpages available\nhereThe docs folder contains the documents that are created and maintained by\nteam members, while the scripts contains the code to automatically generate\nthe GHpages each time a PR is merged.","minerva_public#Minerva_Public":"This repo maintains the Minerva roadmap availablehere"}},"/internal/ARCHIVED/Minerva_docs/compliance_scanning/minerva_dash_requirements/datasets":{"title":"[ARCHIVE] Datasets","data":{"":"There are three immediately critical data sets, but potentially a growing number of data sources in scope for Minerva Dashboarding.The three critical ones in scope for MVP and v1 are the ones below - with the third an aspirational goal at best for MVP, but certainly in scope for v1 release.\nMinerva Public Cloud Infrastructure Policy Compliance Scans\nHSDB Account data, including organizational hierarchy and security attributes\nHS Assets\n\nThe following are out of scope for MVP and v1 release, but are deliverables for the 2nd half of 2022 as part of Secure Cloud Delivery 2.0.\nCerberus (Compliance Drift reporting)\nAdditional Minerva compliance scans\nMinerva Compliance Drift Reporting\nGolden Image compliance scans\nNetwork Security compliance scans\nKubernetes and Container Security compliance scans\n\n\nHS DaaS resource asset inventory data\n\nFinally, at as of yet unpredictable schedule and not particularly part of any SCD2 deliverables:\nFuture HS or SGS data sources\n\nThe list below identifies the specific data sources, rather than the numbered time line above. That is, all Minerva scans are bundled together, rather than in the numbered list above. The links jump directly to the appropriate section, though.","minerva-scan-alerts#Minerva Scan Alerts":"Minerva is the heart of our compliance scans, with the public cloud infrastructure policy compliance scans the most obvious. However, as part of the SCD2 program, we cover compliance scans for four new areas as well. Luckily, we should be able to leverage our existing scan data enrichment and reporting pipeline for these additional use cases.","minerva-public-cloud-infrastructure-policy-compliance-scans#Minerva Public Cloud Infrastructure Policy compliance scans":"The key data set for Minerva Dashboarding and the reason it exists at all - with other data sets providing further enrichment or additional use cases. This data (of course) is stored in our ELK stack, enriched with HSDB account data.The data should allow us to show a current state view segmented by organizational hierarchy, cloud provider, environment type and other security attributes, as well as allow for time-based historical views segmented similarly. Through the link with cloud account IDs as well as resource IDs, we should also be able to combine this data with HSDB and HS Assets/HS DaaS data to further enrich the context of the compliance scans, for instance through alerts/cloud account and alerts/resources ratios that normalize security compliance posture across business units and board areas.","minerva-compliance-drift-reporting#Minerva Compliance Drift Reporting":"Cerberus is the Security Concept Generation Tool and guaranteed-to-be-secure-by-default IaC Generation and Deployment Tool developed out of the HS Network & Security Architecture team, led by Ryan Tolentino. During 2022 Cerberus will see an expansion of features, including integration with the Golden Image service, and become the only way to submit Security Concepts to SGS at all (planned to be expanded eventually to only require SGS interaction for cases requiring exceptions).Landscapes we have security concepts/Cerberus templates for and can be marked in some way - best if deployed through Cerberus itself - should be in compliance upon deployment. But, will they remain that way? At least anecdotally we know that cloud landscapes often bear little relationship to their Security Concepts, which may be several years old - and landscapes may never have looked like their security concepts. At least with Cerberus deployments we will have a known-good starting point.The compliance drift reporting therefore is a scan against the actual deployed landscape and the Cerberus template it supposedly was based on. While a direct detailed report card is best shared directly with account owners and securiyt officers, aggregated results should be viewable in OrcMinervaa Dashboarding, including comparisons between compliance posture of Cerberus deployed landscapes and those outside of it, through scatter plots and other visualizations. The assumption, of course, would be that Cerberus deployed lanscapes - even if drifted from their initial deployment state - are still statistically more in compliance as those continuing to opt for doing things themselves.The goal would be to create pressure on teams not yet on the central Cerberus service and not meeting their compliance targets or falling below the Cerberus-deployed standard, and visualize the difference.","minerva-golden-image-compliance-scans#Minerva Golden Image compliance scans":"Related to the Compliance Drift reporting based on Cerberus, SCD2 commitments require us to provide compliance scans around the Golden Image central services developed out of HS Network & Security Architecture and for which the HS SecDevOps team will provide (light) Operational support.The compliance verification scans will consist of two controls:\nIs the image deployed based on the HS provided hardened Golden Image?\nWhat is the age of the image used?\n\nThe data set from such compliance scans should allow for reporting as an intersection of the two: i.e. Golden Image used or not, and relative age. That will allow for interesting scatterplots and comparative clustering!","minerva-network-security-compliance-scans#Minerva Network Security compliance Scans":"Data for Network Security compliance scans are associated with the deliverables for Workstream 6 in SCD2. This workstream requires the deployment of:\nDDoS Mitigation Service\nCentral DNS Service (initiative separate from SCD2 between GCS & SGS)\nWAF instance Service\n\nThese services will initially be made available electively... but increasingly become mandatory, especially the first two, unless an exception is granted and mitigating controls are in place.The Minerva compliance scanning will scan for whether cloud accounts have the service deployed to verify compliance, and make it visible how adoption is spread through board areas, business units and environment types. Here, too, comparisons between those who have deployed the service and who haven't should be possible.","minerva-kubernetes-and-container-security-compliance-scans#Minerva Kubernetes and Container Security compliance scans":"As part of SCD2 Workstream 8, we are heavily involved in the creation of a security suite of tools for Kubernetes-based landscapes.This security suite eventually will become mandatory and should be pre-deployed in cloud-native or Gardener-provisioned Kubernetes clusters. To ensure that these tools remain deployed and in-use, they are associated with Minerva compliance scanning against the Kubernetes and Container Security policy requirements, but also that Admission Control based on OPA Gatekeeper is deployed and which constraints are active, and that the yet-to-be-selected runtime security tool is in place.As with all our other compliance scans, this data should be enriched through the usual pipeline, allowing for visualization based on organizational hierarchy and security attributes. Note that ideally we also have indicators (whether through Cerberus or Gardener) that the toolset was pre-deployed, or deployed by the cloud account admins themselves.","hsdb-data#HSDB data":"The second critical data source is HSDB. This database holds the association between account IDs and organizatonal hierarchy (driven by cost object), ownership and roles, and security attributes and other metadata kept up to date by the CALM processes.This data already enriches the compliance scan data, but is valuable in its own right in any alerts/cloud account ratio-based data visualizations included in the Dashboard Views. Given that there is such a wide variety of number cloud accounts in use by particular business units, as well as varying preferences and deployment numbers on different providers, these ratios already have proven powerful in reporting - with the board area/company-wide ratio heatmaps already part of Tim McKnight's Monthly Security Report.In the concepts so far, the depth of these visualizations has been limited to the data included in the data exports via the Multi Cloud Creator Accounts data export. This gives 4 layers of organizational hierarchy and cloud providers, but misses two more (practically) usuable organizational levels as well as the security attributes, including environment type. Being able to directly query the HSDB at full data level will be very powerful - including future security attributes or asset management required attributes yet to be defined.As HSDB will host HS Assets data, as well as Minerva aggregate data, it would be ideal to let the database engine prepare the require data for data visualizations, rather than let the data visualization filter and aggregate data as needed, or spend resources on combining datasets. This both to reduce the data transferred and the computational load on the users' browsers.","hs-assets-and-hs-daas-data#HS Assets and HS DaaS data":"Within the context of Minerva Dashboarding, this primarily involves the inclusion of resource asset inventory data in compliance scan reporting.","hs-assets-vs-hs-daas#HS Assets vs. HS DaaS":"The eventual terminology is yet to be determined but for short-hand's sake, HS Assets refers to data collected \"wide and shallow\" by HS Engineering. This provides a running inventory of all cloud objects deployed in each cloud account, with IDs but little metadata. This allows us to count resources by type in each cloud account, as well as rolled up to higher organizational levels, cloud provider, and environment types. This data will be stored within HSDB.HS DaaS (in our context) refers specifically to additional metadata collected from CloudHealth for particularly interesting resource types, i.e. \"narrow and deep\". This includes public IPs, VMs/instance IDs, VPCs/NSGs, load balancers and others, with the scope potentially to be expanded in the future. This data currently is collected into a storage bucket in AWS.For the former there is a clear path to integration within HSDB, for HS DaaS data this is yet to be determined. Attention will focus on HS Assets data until the data becomes available through HS API (whether through direct data integration or via service account call to a remote API, intermediated by HS API).","hs-assets-data#HS Assets data":"HS Assets data specifically refers to the \"wide but shallow\" scans performed by HS Engineering and retrieved directly from the cloud provider APIs. This data will be accessible in the same way as HSDB data.Similarly to cloud accounts in HSDB, resource asset inventory data allows us to normalize the number of alerts across the organization, provider and security attributes against the number of resources deployed - whether in total as proxy for the size of the landscape, or as proportion of (non-)compliant resources as percentage of the total number of a particular resource type. The alerts/cloud account ratio visualizations already included in Dashboard Views could easily be conceived of based on alerts/resources ratios.","extended-hs-assets-and-hs-daas-assets#Extended HS Assets and HS DaaS assets":"HS Assets data specifically refers to the \"narrow and deep\" scans performed by the HS Technology Office and retrieved via the CloudHealth API. This avoid hitting the cloud APIs again, unnecessarily, as CloudHealth has already collected it. Unlike other scans this therefore does not add to the scan load (beyond what CloudHealth is already doing).The data collected from this is in AWS. It is yet to be determined in Q1 how this data will be made accessible via HS API. Without clear timeline (to be decided between HS TO and HS Engineering), it is not yet clear when or how we can use this data for Minerva Dashboards.The additional metadata for key resources such as compute instances, network groups, , load balancers, NAT gateways, disk volumes, storage buckets, as well as managed services eventually should give us increasingly better view on how resources are deployed in SAP's cloud account landscapes, and provide an invaluable tool for security incident response and even operational improvements.","cerberus-compliance-drift-reporting#Cerberus (Compliance Drift Reporting)":"Note: see above for the Minerva scans related to compliance drift reporting)\nCerberus data is required to know what the supposedly \"good\" state is of a particular landscape in a cloud account, and is therefore listed here as a separate data set. It is at the moment not immediately clear what data structures are involved or how this data would be accessed - or at what stage. For instance, the compliance scan itself may well handle this data integration to include pass/fail/skip results in the scans. There may therefore not be a particular need to query the Cerberus data base directly for security analytics in Minerva Dashboarding.","future-hs-or-sgs-data-sources#Future HS or SGS data sources":"It is likely that additional data sources will present themselves, including from tooling we ourselves operate. For instance, the scans from the proposed eBPF-based container runtime security tool will need to be enriched as well with HSDB data in order to become practically useful in the LoB teams.As always, the more we can combine datasets together, the more valuable they become - so the obvious combination of Minerva Kubernetes compliance scans and runtime tool scans is a given. However, we could also consider resource utilization data coming from Plutus/HS DaaS once it becomes available through HS API, as well as through mutual data exchange with SGS. They may be uncomfortable with sharing data in deep detail, but rolled up to a certain organizational level is likely feasible. Should a cloud-native vulnerability scanning tool be deployed out of Workstream 7 as planned, this data set could be used in Minerva Dashboarding. Should that choice be Tanium, we may still be able to either have or get access to that data set.As much of this remains uncertain, none of these additional data sets are explicit SCD2 deliverables - but an eye should be kept on how such data sets can be integrated into Minerva Dashboards as they come online, and prioritize accordingly."}},"/_app":{"title":" App","data":{"":""}}}